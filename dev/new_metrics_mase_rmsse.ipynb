{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/varios/skforecast'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "str(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skforecast.datasets import fetch_dataset\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skforecast.metrics import mean_absolute_scaled_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.3.0\n",
      "skforecast: 0.13.0\n",
      "numpy: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import skforecast\n",
    "import numpy\n",
    "\n",
    "print(f'sklearn: {sklearn.__version__}')\n",
    "print(f'skforecast: {skforecast.__version__}')\n",
    "print(f'numpy: {numpy.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "# ==============================================================================\n",
    "data = fetch_dataset(\n",
    "    name=\"h2o\", raw=True, kwargs_read_csv={\"names\": [\"y\", \"datetime\"], \"header\": 0},\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Data preprocessing\n",
    "# ==============================================================================\n",
    "data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\n",
    "data = data.set_index('datetime')\n",
    "data = data.asfreq('MS')\n",
    "data = data['y']\n",
    "data = data.sort_index()\n",
    "\n",
    "# Train-validation dates\n",
    "# ==============================================================================\n",
    "end_train = '2002-01-01 23:59:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7e057c1f5d4e96a9cd341ac1a4bcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting MASE: 2.344471182248395\n",
      "mae forecast: 0.19922359134545478\n",
      "mae in sample: 0.08497591817460318\n",
      "mase: 2.344471182248395\n"
     ]
    }
   ],
   "source": [
    "# Backtesting forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoreg(\n",
    "                 regressor = RandomForestRegressor(random_state=123),\n",
    "                 lags      = 1 \n",
    "             )\n",
    "\n",
    "metric, predictions = backtesting_forecaster(\n",
    "                          forecaster            = forecaster,\n",
    "                          y                     = data,\n",
    "                          steps                 = 10,\n",
    "                          metric                = 'mean_absolute_scaled_error',\n",
    "                          initial_train_size    = len(data.loc[:end_train]),\n",
    "                          fixed_train_size      = False,\n",
    "                          gap                   = 0,\n",
    "                          allow_incomplete_fold = True,\n",
    "                          refit                 = False,\n",
    "                          n_jobs                = 'auto',\n",
    "                          verbose               = False,\n",
    "                          show_progress         = True  \n",
    "                      )\n",
    "\n",
    "print(f\"Backtesting MASE: {metric}\")\n",
    "\n",
    "# Manual check\n",
    "# ==============================================================================\n",
    "mae_foreast = mean_absolute_error(data.loc[end_train:], predictions)\n",
    "naive_in_sample_foreast = data.loc[:end_train].shift(1).dropna()\n",
    "mae_in_sample = mean_absolute_error(data.loc[naive_in_sample_foreast.index], naive_in_sample_foreast)\n",
    "print(f\"mae forecast: {mae_foreast}\")\n",
    "print(f\"mae in sample: {mae_in_sample}\")\n",
    "print(f\"mase: {mae_foreast/mae_in_sample}\")\n",
    "assert metric == mae_foreast/mae_in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of backtesting process\n",
      "----------------------------------\n",
      "Number of observations used for initial training: 127\n",
      "Number of observations used for backtesting: 77\n",
      "    Number of folds: 8\n",
      "    Number of steps per fold: 10\n",
      "    Number of steps to exclude from the end of each train set before test (gap): 0\n",
      "    Last fold only includes 7 observations.\n",
      "\n",
      "Fold: 0\n",
      "    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n",
      "    Validation: 2002-02-01 00:00:00 -- 2002-11-01 00:00:00  (n=10)\n",
      "Fold: 1\n",
      "    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n",
      "    Validation: 2002-12-01 00:00:00 -- 2003-09-01 00:00:00  (n=10)\n",
      "Fold: 2\n",
      "    Training:   1991-07-01 00:00:00 -- 2002-01-01 00:00:00  (n=127)\n",
      "    Validation: 2003-10-01 00:00:00 -- 2004-07-01 00:00:00  (n=10)\n",
      "Fold: 3\n",
      "    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n",
      "    Validation: 2004-08-01 00:00:00 -- 2005-05-01 00:00:00  (n=10)\n",
      "Fold: 4\n",
      "    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n",
      "    Validation: 2005-06-01 00:00:00 -- 2006-03-01 00:00:00  (n=10)\n",
      "Fold: 5\n",
      "    Training:   1991-07-01 00:00:00 -- 2004-07-01 00:00:00  (n=157)\n",
      "    Validation: 2006-04-01 00:00:00 -- 2007-01-01 00:00:00  (n=10)\n",
      "Fold: 6\n",
      "    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n",
      "    Validation: 2007-02-01 00:00:00 -- 2007-11-01 00:00:00  (n=10)\n",
      "Fold: 7\n",
      "    Training:   1991-07-01 00:00:00 -- 2007-01-01 00:00:00  (n=187)\n",
      "    Validation: 2007-12-01 00:00:00 -- 2008-06-01 00:00:00  (n=7)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832add7b707241f9aebb528915d9f83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting MASE: 2.462755601999461\n",
      "mae forecast: 0.23185816123376632\n",
      "mae in sample: 0.09414582634408604\n",
      "mase: 2.462755601999461\n"
     ]
    }
   ],
   "source": [
    "# Backtesting forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoreg(\n",
    "                 regressor = RandomForestRegressor(random_state=123),\n",
    "                 lags      = 1 \n",
    "             )\n",
    "\n",
    "metric, predictions = backtesting_forecaster(\n",
    "                          forecaster            = forecaster,\n",
    "                          y                     = data,\n",
    "                          steps                 = 10,\n",
    "                          metric                = 'mean_absolute_scaled_error',\n",
    "                          initial_train_size    = len(data.loc[:end_train]),\n",
    "                          fixed_train_size      = False,\n",
    "                          gap                   = 0,\n",
    "                          allow_incomplete_fold = True,\n",
    "                          refit                 = 3,\n",
    "                          n_jobs                = 'auto',\n",
    "                          verbose               = True,\n",
    "                          show_progress         = True  \n",
    "                      )\n",
    "\n",
    "print(f\"Backtesting MASE: {metric}\")\n",
    "\n",
    "# Manual check\n",
    "# ==============================================================================\n",
    "refit_intervals = [\n",
    "    ('1991-07-01 00:00:00', '2002-01-01 00:00:00'),\n",
    "    ('1991-07-01 00:00:00', '2004-07-01 00:00:00'),\n",
    "    ('1991-07-01 00:00:00', '2007-01-01 00:00:00')\n",
    "]\n",
    "\n",
    "mae_foreast = mean_absolute_error(data.loc[end_train:], predictions)\n",
    "y_train = pd.concat([data.loc[start:end] for start, end in refit_intervals])\n",
    "y_train = y_train.loc[~y_train.index.duplicated(keep='first')]\n",
    "naive_in_sample_foreast = y_train.shift(1).dropna()\n",
    "mae_in_sample = mean_absolute_error(y_train.loc[naive_in_sample_foreast.index], naive_in_sample_foreast)\n",
    "print(f\"mae forecast: {mae_foreast}\")\n",
    "print(f\"mae in sample: {mae_in_sample}\")\n",
    "print(f\"mase: {mae_foreast/mae_in_sample}\")\n",
    "assert metric == mae_foreast/mae_in_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_sales\n",
      "-----------\n",
      "Simulated time series for the sales of 3 different items.\n",
      "Simulated data.\n",
      "Shape of the dataset: (1097, 3)\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "# ==============================================================================\n",
    "data = fetch_dataset(name=\"items_sales\")\n",
    "data.head()\n",
    "\n",
    "# Split data into train-val-test\n",
    "# ==============================================================================\n",
    "end_train = '2014-07-15 23:59:00'\n",
    "data_train = data.loc[:end_train, :].copy()\n",
    "data_test  = data.loc[end_train:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b253799654fd4e98b143910ab8be1a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting MASE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>levels</th>\n",
       "      <th>mean_absolute_scaled_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item_1</td>\n",
       "      <td>1.091527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_2</td>\n",
       "      <td>1.563339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item_3</td>\n",
       "      <td>0.959262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   levels  mean_absolute_scaled_error\n",
       "0  item_1                    1.091527\n",
       "1  item_2                    1.563339\n",
       "2  item_3                    0.959262"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mase: [1.09152723 1.56333908 0.95926186]\n"
     ]
    }
   ],
   "source": [
    "# Backtesting forecaster: no aggregation\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                 regressor = Ridge(),\n",
    "                 lags      = 1 \n",
    "             )\n",
    "\n",
    "metric, predictions = backtesting_forecaster_multiseries(\n",
    "                          forecaster            = forecaster,\n",
    "                          series                = data,\n",
    "                          steps                 = 10,\n",
    "                          metric                = 'mean_absolute_scaled_error',\n",
    "                          initial_train_size    = len(data.loc[:end_train]),\n",
    "                          fixed_train_size      = True,\n",
    "                          gap                   = 0,\n",
    "                          allow_incomplete_fold = True,\n",
    "                          refit                 = False,\n",
    "                          n_jobs                = 'auto',\n",
    "                          verbose               = False,\n",
    "                          show_progress         = True  \n",
    "                      )\n",
    "\n",
    "print(\"Backtesting MASE:\")\n",
    "display(metric)\n",
    "\n",
    "# Manual check\n",
    "# ==============================================================================\n",
    "mae_foreast = mean_absolute_error(\n",
    "    data.loc[end_train:], predictions, multioutput=\"raw_values\"\n",
    ")\n",
    "naive_in_sample_foreast = data.loc[:end_train].shift(1).dropna().drop_duplicates()\n",
    "mae_in_sample = mean_absolute_error(\n",
    "    data.loc[naive_in_sample_foreast.index],\n",
    "    naive_in_sample_foreast,\n",
    "    multioutput=\"raw_values\",\n",
    ")\n",
    "mase = mae_foreast / mae_in_sample\n",
    "print(f\"mase: {mase}\")\n",
    "assert (metric['mean_absolute_scaled_error'] == mase).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0739938e362c4534b573dd78bf296355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>levels</th>\n",
       "      <th>mean_absolute_scaled_error</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item_1</td>\n",
       "      <td>1.091527</td>\n",
       "      <td>1.691320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_2</td>\n",
       "      <td>1.563339</td>\n",
       "      <td>3.707672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item_3</td>\n",
       "      <td>0.959262</td>\n",
       "      <td>3.571717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>average</td>\n",
       "      <td>1.204709</td>\n",
       "      <td>2.990236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weighted_average</td>\n",
       "      <td>1.204709</td>\n",
       "      <td>2.990236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pooling</td>\n",
       "      <td>1.173480</td>\n",
       "      <td>2.990236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             levels  mean_absolute_scaled_error  mean_absolute_error\n",
       "0            item_1                    1.091527             1.691320\n",
       "1            item_2                    1.563339             3.707672\n",
       "2            item_3                    0.959262             3.571717\n",
       "3           average                    1.204709             2.990236\n",
       "4  weighted_average                    1.204709             2.990236\n",
       "5           pooling                    1.173480             2.990236"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Backtesting forecaster: aggregation\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                 regressor = Ridge(),\n",
    "                 lags      = 1 \n",
    "             )\n",
    "\n",
    "metric, predictions = backtesting_forecaster_multiseries(\n",
    "                          forecaster            = forecaster,\n",
    "                          series                = data,\n",
    "                          steps                 = 10,\n",
    "                          metric                = ['mean_absolute_scaled_error', 'mean_absolute_error'],\n",
    "                          add_aggregated_metric = True,\n",
    "                          initial_train_size    = len(data.loc[:end_train]),\n",
    "                          fixed_train_size      = True,\n",
    "                          gap                   = 0,\n",
    "                          allow_incomplete_fold = True,\n",
    "                          refit                 = False,\n",
    "                          n_jobs                = 'auto',\n",
    "                          verbose               = False,\n",
    "                          show_progress         = True  \n",
    "                      )\n",
    "display(metric)\n",
    "\n",
    "# Manual check\n",
    "# ==============================================================================\n",
    "mae_foreast = mean_absolute_error(\n",
    "    data.loc[end_train:], predictions, multioutput=\"raw_values\"\n",
    ")\n",
    "naive_in_sample_foreast = data.loc[:end_train].shift(1).dropna().drop_duplicates()\n",
    "mae_in_sample_foreast = mean_absolute_error(\n",
    "    data.loc[naive_in_sample_foreast.index],\n",
    "    naive_in_sample_foreast,\n",
    "    multioutput=\"raw_values\",\n",
    ")\n",
    "mase = mae_foreast/mae_in_sample\n",
    "average_mase = mase.mean()\n",
    "weighted_average_mase = np.average(mase, weights=predictions.notna().sum())\n",
    "average_mae = mae_foreast.mean()\n",
    "weighted_average_mae = np.average(mae_foreast, weights=predictions.notna().sum())\n",
    "\n",
    "pooled_y_true_y_pred = pd.merge(\n",
    "    data.melt(ignore_index=False).reset_index(names='datetime'),\n",
    "    predictions.melt(ignore_index=False).reset_index(names='datetime'),\n",
    "    on=['datetime', 'variable'],\n",
    "    suffixes=('_true', '_pred'),\n",
    "    how='inner'\n",
    ")\n",
    "pooled_mae_foreast = mean_absolute_error(\n",
    "    pooled_y_true_y_pred['value_true'],\n",
    "    pooled_y_true_y_pred['value_pred']\n",
    ")\n",
    "pooled_naive_in_sample_foreast = np.concatenate([np.diff(data.loc[:end_train, col]) for col in data.columns])\n",
    "pooled_mae_in_sample_forecast = np.mean(np.abs(pooled_naive_in_sample_foreast))\n",
    "pooled_mase = pooled_mae_foreast/pooled_mae_in_sample_forecast\n",
    "expected_results = pd.DataFrame({\n",
    "    'levels': ['average', 'weighted_average', 'pooling'],\n",
    "    'mean_absolute_scaled_error': [average_mase, weighted_average_mase, pooled_mase],\n",
    "    'mean_absolute_error': [average_mae, weighted_average_mae, pooled_mae_foreast]\n",
    "})\n",
    "\n",
    "pd.testing.assert_frame_equal(\n",
    "    metric.query('levels in [\"average\", \"weighted_average\", \"pooling\"]').reset_index(drop=True),\n",
    "    expected_results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of backtesting process\n",
      "----------------------------------\n",
      "Number of observations used for initial training: 927\n",
      "Number of observations used for backtesting: 170\n",
      "    Number of folds: 17\n",
      "    Number of steps per fold: 10\n",
      "    Number of steps to exclude from the end of each train set before test (gap): 0\n",
      "\n",
      "Fold: 0\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-07-16 00:00:00 -- 2014-07-25 00:00:00  (n=10)\n",
      "Fold: 1\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-07-26 00:00:00 -- 2014-08-04 00:00:00  (n=10)\n",
      "Fold: 2\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-08-05 00:00:00 -- 2014-08-14 00:00:00  (n=10)\n",
      "Fold: 3\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-08-15 00:00:00 -- 2014-08-24 00:00:00  (n=10)\n",
      "Fold: 4\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-08-25 00:00:00 -- 2014-09-03 00:00:00  (n=10)\n",
      "Fold: 5\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-09-04 00:00:00 -- 2014-09-13 00:00:00  (n=10)\n",
      "Fold: 6\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-09-14 00:00:00 -- 2014-09-23 00:00:00  (n=10)\n",
      "Fold: 7\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-09-24 00:00:00 -- 2014-10-03 00:00:00  (n=10)\n",
      "Fold: 8\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-10-04 00:00:00 -- 2014-10-13 00:00:00  (n=10)\n",
      "Fold: 9\n",
      "    Training:   2012-01-01 00:00:00 -- 2014-07-15 00:00:00  (n=927)\n",
      "    Validation: 2014-10-14 00:00:00 -- 2014-10-23 00:00:00  (n=10)\n",
      "Fold: 10\n",
      "    Training:   2012-04-10 00:00:00 -- 2014-10-23 00:00:00  (n=927)\n",
      "    Validation: 2014-10-24 00:00:00 -- 2014-11-02 00:00:00  (n=10)\n",
      "Fold: 11\n",
      "    Training:   2012-04-10 00:00:00 -- 2014-10-23 00:00:00  (n=927)\n",
      "    Validation: 2014-11-03 00:00:00 -- 2014-11-12 00:00:00  (n=10)\n",
      "Fold: 12\n",
      "    Training:   2012-04-10 00:00:00 -- 2014-10-23 00:00:00  (n=927)\n",
      "    Validation: 2014-11-13 00:00:00 -- 2014-11-22 00:00:00  (n=10)\n",
      "Fold: 13\n",
      "    Training:   2012-04-10 00:00:00 -- 2014-10-23 00:00:00  (n=927)\n",
      "    Validation: 2014-11-23 00:00:00 -- 2014-12-02 00:00:00  (n=10)\n",
      "Fold: 14\n",
      "    Training:   2012-04-10 00:00:00 -- 2014-10-23 00:00:00  (n=927)\n",
      "    Validation: 2014-12-03 00:00:00 -- 2014-12-12 00:00:00  (n=10)\n",
      "Fold: 15\n",
      "    Training:   2012-04-10 00:00:00 -- 2014-10-23 00:00:00  (n=927)\n",
      "    Validation: 2014-12-13 00:00:00 -- 2014-12-22 00:00:00  (n=10)\n",
      "Fold: 16\n",
      "    Training:   2012-04-10 00:00:00 -- 2014-10-23 00:00:00  (n=927)\n",
      "    Validation: 2014-12-23 00:00:00 -- 2015-01-01 00:00:00  (n=10)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2687d032d0c4c23af1c0a17c568d533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting MASE:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>levels</th>\n",
       "      <th>mean_absolute_scaled_error</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>item_1</td>\n",
       "      <td>1.106355</td>\n",
       "      <td>1.690569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>item_2</td>\n",
       "      <td>1.571983</td>\n",
       "      <td>3.670363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>item_3</td>\n",
       "      <td>1.005813</td>\n",
       "      <td>3.715934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   levels  mean_absolute_scaled_error  mean_absolute_error\n",
       "0  item_1                    1.106355             1.690569\n",
       "1  item_2                    1.571983             3.670363\n",
       "2  item_3                    1.005813             3.715934"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae forecast: [1.69056888 3.67036308 3.71593437]\n",
      "mae in sample: [1.52805244 2.33486251 3.69445992]\n",
      "mase: [1.10635527 1.57198253 1.00581261]\n",
      "mase average: 1.2280501371035948\n",
      "mase weighted average: 1.228050137103595\n"
     ]
    }
   ],
   "source": [
    "# Backtesting forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                 regressor = Ridge(),\n",
    "                 lags      = 1 \n",
    "             )\n",
    "\n",
    "metric, predictions = backtesting_forecaster_multiseries(\n",
    "                          forecaster            = forecaster,\n",
    "                          series                = data,\n",
    "                          steps                 = 10,\n",
    "                          metric                = ['mean_absolute_scaled_error', 'mean_absolute_error'],\n",
    "                          initial_train_size    = len(data.loc[:end_train]),\n",
    "                          fixed_train_size      = True,\n",
    "                          gap                   = 0,\n",
    "                          allow_incomplete_fold = True,\n",
    "                          refit                 = 10,\n",
    "                          n_jobs                = 'auto',\n",
    "                          verbose               = True,\n",
    "                          show_progress         = True  \n",
    "                      )\n",
    "\n",
    "print(\"Backtesting MASE:\")\n",
    "display(metric)\n",
    "\n",
    "# Manual check\n",
    "# ==============================================================================\n",
    "# Manual check\n",
    "refit_intervals = [\n",
    "    ('2012-01-01 00:00:00', '2014-07-15 00:00:00'),\n",
    "    ('2012-04-10 00:00:00', '2014-10-23 00:00:00')\n",
    "]\n",
    "\n",
    "mae_forecast = mean_absolute_error(\n",
    "    data.loc[end_train:], predictions, multioutput=\"raw_values\"\n",
    ")\n",
    "data_train = pd.concat([data.loc[start:end, :] for start, end in refit_intervals])\n",
    "data_train = data_train.loc[~data_train.index.duplicated(keep='first'), :]\n",
    "naive_in_sample_foreast = data_train.shift(1).dropna()\n",
    "mae_in_sample = mean_absolute_error(\n",
    "    data.loc[naive_in_sample_foreast.index],\n",
    "    naive_in_sample_foreast,\n",
    "    multioutput=\"raw_values\",\n",
    ")\n",
    "mase = mae_forecast/mae_in_sample\n",
    "mase_average = mase.mean()\n",
    "mase_weighted_average = np.average(mase, weights=predictions.notna().sum())\n",
    "\n",
    "print(f\"mae forecast: {mae_forecast}\")\n",
    "print(f\"mae in sample: {mae_in_sample}\")\n",
    "print(f\"mase: {mase}\")\n",
    "print(f\"mase average: {mase_average}\")\n",
    "print(f\"mase weighted average: {mase_weighted_average}\")\n",
    "assert (metric.loc[metric['levels']=='average', 'mean_absolute_scaled_error'] == mase_average).all()\n",
    "assert (metric.loc[metric['levels']=='weighted_average', 'mean_absolute_scaled_error'] == mase_weighted_average).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items_sales\n",
      "-----------\n",
      "Simulated time series for the sales of 3 different items.\n",
      "Simulated data.\n",
      "Shape of the dataset: (1097, 3)\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "# ==============================================================================\n",
    "data = fetch_dataset(name=\"items_sales\")\n",
    "data.head()\n",
    "\n",
    "# Split data into train-val-test\n",
    "# ==============================================================================\n",
    "end_train = '2014-07-15 23:59:00'\n",
    "end_val = '2014-10-15 23:59:00'\n",
    "data_train = data.loc[:end_train, :].copy()\n",
    "data_val = data.loc[end_train:end_val, :].copy()\n",
    "data_test  = data.loc[end_val:, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 models compared for 3 level(s). Number of iterations: 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa472876263d49eb985cb1c2a4a5a8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lags grid:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fa5492ec6c4a5789506d12ed00446c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params grid:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggregation</th>\n",
       "      <th>lags</th>\n",
       "      <th>lags_label</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>mean_absolute_scaled_error</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'alpha': 0}</td>\n",
       "      <td>3.011987</td>\n",
       "      <td>1.182015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>3.012102</td>\n",
       "      <td>1.182061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0}</td>\n",
       "      <td>3.285467</td>\n",
       "      <td>1.289339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>3.285631</td>\n",
       "      <td>1.289404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>{'alpha': 0}</td>\n",
       "      <td>3.411347</td>\n",
       "      <td>1.338739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>3.411571</td>\n",
       "      <td>1.338827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'alpha': 10000000000}</td>\n",
       "      <td>4.986100</td>\n",
       "      <td>1.956731</td>\n",
       "      <td>10000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 10000000000}</td>\n",
       "      <td>4.988957</td>\n",
       "      <td>1.957852</td>\n",
       "      <td>10000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pooling</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>{'alpha': 10000000000}</td>\n",
       "      <td>4.990710</td>\n",
       "      <td>1.958540</td>\n",
       "      <td>10000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  aggregation       lags lags_label                  params  \\\n",
       "0     pooling  [1, 2, 3]  [1, 2, 3]            {'alpha': 0}   \n",
       "1     pooling  [1, 2, 3]  [1, 2, 3]            {'alpha': 1}   \n",
       "2     pooling     [1, 2]     [1, 2]            {'alpha': 0}   \n",
       "3     pooling     [1, 2]     [1, 2]            {'alpha': 1}   \n",
       "4     pooling        [1]        [1]            {'alpha': 0}   \n",
       "5     pooling        [1]        [1]            {'alpha': 1}   \n",
       "6     pooling  [1, 2, 3]  [1, 2, 3]  {'alpha': 10000000000}   \n",
       "7     pooling     [1, 2]     [1, 2]  {'alpha': 10000000000}   \n",
       "8     pooling        [1]        [1]  {'alpha': 10000000000}   \n",
       "\n",
       "   mean_absolute_error  mean_absolute_scaled_error        alpha  \n",
       "0             3.011987                    1.182015            0  \n",
       "1             3.012102                    1.182061            1  \n",
       "2             3.285467                    1.289339            0  \n",
       "3             3.285631                    1.289404            1  \n",
       "4             3.411347                    1.338739            0  \n",
       "5             3.411571                    1.338827            1  \n",
       "6             4.986100                    1.956731  10000000000  \n",
       "7             4.988957                    1.957852  10000000000  \n",
       "8             4.990710                    1.958540  10000000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                 regressor = Ridge(),\n",
    "                 lags      = 1 \n",
    "             )\n",
    "lags_grid =[1, 2, 3]\n",
    "param_grid = {'alpha': [0, 1, 10000000000]}\n",
    "\n",
    "results = grid_search_forecaster_multiseries(\n",
    "                          forecaster            = forecaster,\n",
    "                          series                = data.loc[:end_val, :],\n",
    "                          lags_grid             = lags_grid,\n",
    "                          param_grid            = param_grid,\n",
    "                          steps                 = 12,\n",
    "                          metric                = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                          aggregate_metric      = 'pooling',\n",
    "                          refit                 = False,\n",
    "                          initial_train_size    = len(data.loc[:end_train]),\n",
    "                          fixed_train_size      = True,\n",
    "                          return_best           = False,\n",
    "                          n_jobs                = 'auto',\n",
    "                          verbose               = False,\n",
    "                          show_progress         = True\n",
    "                      )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd5e4c4a5fd4cbf929b0eb1a106e580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggregation</th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>mean_absolute_scaled_error</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weighted_average</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'alpha': 0.4807958141120149}</td>\n",
       "      <td>2.562235</td>\n",
       "      <td>1.046785</td>\n",
       "      <td>0.480796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weighted_average</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'alpha': 0.667878511469039}</td>\n",
       "      <td>2.562239</td>\n",
       "      <td>1.046787</td>\n",
       "      <td>0.667879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighted_average</td>\n",
       "      <td>[1, 2, 3, 4, 5]</td>\n",
       "      <td>{'alpha': 0.6709608626961889}</td>\n",
       "      <td>2.562239</td>\n",
       "      <td>1.046787</td>\n",
       "      <td>0.670961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_average</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'alpha': 0.2579065805327433}</td>\n",
       "      <td>3.012016</td>\n",
       "      <td>1.243240</td>\n",
       "      <td>0.257907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        aggregation             lags                         params  \\\n",
       "0  weighted_average  [1, 2, 3, 4, 5]  {'alpha': 0.4807958141120149}   \n",
       "1  weighted_average  [1, 2, 3, 4, 5]   {'alpha': 0.667878511469039}   \n",
       "2  weighted_average  [1, 2, 3, 4, 5]  {'alpha': 0.6709608626961889}   \n",
       "3  weighted_average        [1, 2, 3]  {'alpha': 0.2579065805327433}   \n",
       "\n",
       "   mean_absolute_error  mean_absolute_scaled_error     alpha  \n",
       "0             2.562235                    1.046785  0.480796  \n",
       "1             2.562239                    1.046787  0.667879  \n",
       "2             2.562239                    1.046787  0.670961  \n",
       "3             3.012016                    1.243240  0.257907  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                 regressor = Ridge(),\n",
    "                 lags      = 1,\n",
    "                 encoding= 'ordinal'\n",
    "             )\n",
    "\n",
    "def search_space(trial):\n",
    "    search_space  = {\n",
    "        'lags'             : trial.suggest_categorical('lags', [3, 5]),\n",
    "        'alpha'            : trial.suggest_float('alpha', 0.1, 1.0),\n",
    "    }\n",
    "    \n",
    "    return search_space\n",
    "\n",
    "results, best_trial = bayesian_search_forecaster_multiseries(\n",
    "                          forecaster            = forecaster,\n",
    "                          series                = data.loc[:end_val, :],\n",
    "                          search_space          = search_space,\n",
    "                          steps                 = 12,\n",
    "                          metric                = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                          aggregate_metric      = 'weighted_average',\n",
    "                          refit                 = False,\n",
    "                          initial_train_size    = len(data.loc[:end_train]),\n",
    "                          fixed_train_size      = True,\n",
    "                          n_trials              = 10,\n",
    "                          random_state          = 123,\n",
    "                          return_best           = False,\n",
    "                          n_jobs                = 'auto',\n",
    "                          verbose               = False,\n",
    "                          show_progress         = True,\n",
    "                          engine                = 'optuna',\n",
    "                          kwargs_create_study   = {},\n",
    "                          kwargs_study_optimize = {}\n",
    "                      )\n",
    "\n",
    "results.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information of backtesting process\n",
      "----------------------------------\n",
      "Number of observations used for initial training: 30\n",
      "Number of observations used for backtesting: 20\n",
      "    Number of folds: 4\n",
      "    Number of steps per fold: 5\n",
      "    Number of steps to exclude from the end of each train set before test (gap): 3\n",
      "    Last fold only includes 2 observations.\n",
      "\n",
      "Fold: 0\n",
      "    Training:   0 -- 29  (n=30)\n",
      "    Validation: 33 -- 37  (n=5)\n",
      "Fold: 1\n",
      "    Training:   0 -- 29  (n=30)\n",
      "    Validation: 38 -- 42  (n=5)\n",
      "Fold: 2\n",
      "    Training:   0 -- 29  (n=30)\n",
      "    Validation: 43 -- 47  (n=5)\n",
      "Fold: 3\n",
      "    Training:   0 -- 29  (n=30)\n",
      "    Validation: 48 -- 49  (n=2)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c096f0852bc24edbb4e0fecdfe615fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unit test backtesting_forecaster_multiseries\n",
    "# ==============================================================================\n",
    "import re\n",
    "import pytest\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from skforecast.exceptions import IgnoredArgumentWarning\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom\n",
    "from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multivariate\n",
    "\n",
    "# Fixtures\n",
    "from skforecast.model_selection_multiseries.tests.fixtures_model_selection_multiseries import series\n",
    "# THIS_DIR = Path(__file__).parent\n",
    "series_dict = joblib.load('/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/tests/fixture_sample_multi_series.joblib')\n",
    "exog_dict = joblib.load('/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/tests/fixture_sample_multi_series_exog.joblib')\n",
    "end_train = \"2016-07-31 23:59:00\"\n",
    "series_dict_train = {k: v.loc[:end_train,] for k, v in series_dict.items()}\n",
    "exog_dict_train = {k: v.loc[:end_train,] for k, v in exog_dict.items()}\n",
    "series_dict_test = {k: v.loc[end_train:,] for k, v in series_dict.items()}\n",
    "exog_dict_test = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n",
    "series_with_nans = series.copy()\n",
    "#series_with_nans['l2'].iloc[:10] = np.nan\n",
    "#series_with_nans.loc[:10, 'l2'] = np.nan\n",
    "series_with_nans.iloc[:10, series_with_nans.columns.get_loc('l2')] = np.nan\n",
    "\n",
    "def create_predictors(y): # pragma: no cover\n",
    "    \"\"\"\n",
    "    Create first 2 lags of a time series.\n",
    "    \"\"\"\n",
    "    lags = y[-1:-3:-1]\n",
    "\n",
    "    return lags\n",
    "\n",
    "def create_predictors_14(y): # pragma: no cover\n",
    "    \"\"\"\n",
    "    Create first 14 lags of a time series.\n",
    "    \"\"\"\n",
    "    lags = y[-1:-15:-1]\n",
    "\n",
    "    return lags\n",
    "\n",
    "\n",
    "forecaster = ForecasterAutoregMultiSeries(regressor=Ridge(random_state=123), \n",
    "                            lags=2, transformer_series=None,\n",
    "                            encoding='onehot')\n",
    "\n",
    "\n",
    "n_validation = 20\n",
    "steps = 5\n",
    "gap = 3\n",
    "\n",
    "metrics_levels, backtest_predictions = backtesting_forecaster_multiseries(\n",
    "                    forecaster            = forecaster,\n",
    "                    series                = series_with_nans,\n",
    "                    steps                 = steps,\n",
    "                    levels                = 'l1',\n",
    "                    metric                = 'mean_absolute_error',\n",
    "                    initial_train_size    = len(series_with_nans) - n_validation,\n",
    "                    gap                   = gap,\n",
    "                    allow_incomplete_fold = True,\n",
    "                    refit                 = False,\n",
    "                    fixed_train_size      = False,\n",
    "                    exog                  = series_with_nans['l1'].rename('exog_1'),\n",
    "                    interval              = [5, 95],\n",
    "                    n_boot                = 150,\n",
    "                    random_state          = 123,\n",
    "                    in_sample_residuals   = True,\n",
    "                    verbose               = True\n",
    "                )\n",
    "\n",
    "expected_metric = pd.DataFrame({'levels': ['l1'], \n",
    "                                    'mean_absolute_error': [0.11243765852459384]})\n",
    "expected_predictions = pd.DataFrame(\n",
    "    data = np.array([[0.49746847, 0.24620019, 0.74208308],\n",
    "                        [0.46715961, 0.27280913, 0.69897019],\n",
    "                        [0.42170236, 0.21384761, 0.6338432 ],\n",
    "                        [0.47242371, 0.23550111, 0.68264162],\n",
    "                        [0.66450589, 0.4128072 , 0.87549193],\n",
    "                        [0.67311291, 0.42184464, 0.91772752],\n",
    "                        [0.48788629, 0.29353581, 0.71969688],\n",
    "                        [0.55141437, 0.34355962, 0.76355521],\n",
    "                        [0.33369285, 0.09677024, 0.54391075],\n",
    "                        [0.43287852, 0.18117983, 0.64386456],\n",
    "                        [0.46643379, 0.21516552, 0.7110484 ],\n",
    "                        [0.6535986 , 0.45924812, 0.88540919],\n",
    "                        [0.38328273, 0.17542798, 0.59542356],\n",
    "                        [0.49931045, 0.26238785, 0.70952835],\n",
    "                        [0.70119564, 0.44949695, 0.91218168],\n",
    "                        [0.49290276, 0.24163448, 0.73751736],\n",
    "                        [0.54653561, 0.35218513, 0.7783462 ]]),\n",
    "    columns = ['l1', 'l1_lower_bound', 'l1_upper_bound'],\n",
    "    index = pd.RangeIndex(start=33, stop=50, step=1)\n",
    ")\n",
    "                                \n",
    "pd.testing.assert_frame_equal(expected_metric, metrics_levels)\n",
    "pd.testing.assert_frame_equal(expected_predictions, backtest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 models compared for 1 level(s). Number of iterations: 6.\n"
     ]
    }
   ],
   "source": [
    "from skforecast.model_selection_multiseries.model_selection_multiseries import _evaluate_grid_hyperparameters_multiseries\n",
    "import os \n",
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                    regressor = Ridge(random_state=123),\n",
    "                    lags      = 2\n",
    "                )\n",
    "\n",
    "steps = 3\n",
    "n_validation = 12\n",
    "lags_grid = {'lags_1': 2, 'lags_2': 4}\n",
    "param_grid = [{'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
    "output_file = 'test_evaluate_grid_hyperparameters_multiseries_output_file.txt'\n",
    "\n",
    "results = _evaluate_grid_hyperparameters_multiseries(\n",
    "                forecaster         = forecaster,\n",
    "                series             = series,\n",
    "                param_grid         = param_grid,\n",
    "                steps              = steps,\n",
    "                metric             = 'mean_absolute_error',\n",
    "                initial_train_size = len(series) - n_validation,\n",
    "                fixed_train_size   = False,\n",
    "                levels             = 'l1',\n",
    "                exog               = None,\n",
    "                lags_grid          = lags_grid,\n",
    "                refit              = False,\n",
    "                return_best        = False,\n",
    "                verbose            = False,\n",
    "                show_progress      = False,\n",
    "                output_file        = output_file\n",
    "            )\n",
    "results  = results.astype({'aggregation': str, 'lags': str, 'params': str})\n",
    "\n",
    "assert os.path.isfile(output_file)\n",
    "output_file_content = pd.read_csv(output_file, sep='\\t', low_memory=False)\n",
    "output_file_content = output_file_content.sort_values(by='mean_absolute_error').reset_index(drop=True)\n",
    "output_file_content = output_file_content.astype({'aggregation': str, 'lags': str, 'params': str})\n",
    "pd.testing.assert_frame_equal(results, output_file_content)\n",
    "os.remove(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test bayesian_search_forecaster_multiseries\n",
    "# ==============================================================================\n",
    "import re\n",
    "import pytest\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom\n",
    "from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n",
    "from skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import bayesian_search_forecaster_multivariate\n",
    "\n",
    "# Fixtures\n",
    "from skforecast.model_selection_multiseries.tests.fixtures_model_selection_multiseries import series\n",
    "series_dict = joblib.load(\"/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/tests/fixture_sample_multi_series.joblib\")\n",
    "exog_dict = joblib.load(\"/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/tests/fixture_sample_multi_series_exog.joblib\")\n",
    "end_train = \"2016-07-31 23:59:00\"\n",
    "series_dict_train = {k: v.loc[:end_train,] for k, v in series_dict.items()}\n",
    "exog_dict_train = {k: v.loc[:end_train,] for k, v in exog_dict.items()}\n",
    "series_dict_test = {k: v.loc[end_train:,] for k, v in series_dict.items()}\n",
    "exog_dict_test = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 failed with parameters: {'n_estimators': 4, 'max_depth': 3, 'lags': [1, 7, 14]} because of the following error: KeyError(\"['id_1002'] not in index\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py\", line 1852, in _objective\n",
      "    metrics, _ = backtesting_forecaster_multiseries(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py\", line 933, in backtesting_forecaster_multiseries\n",
      "    metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries(\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py\", line 752, in _backtesting_forecaster_multiseries\n",
      "    metrics_levels = _calculate_metrics_multiseries(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py\", line 362, in _calculate_metrics_multiseries\n",
      "    n_predictions_levels = n_predictions_levels.loc[metrics_levels['levels']]\n",
      "                           ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1191, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1420, in _getitem_axis\n",
      "    return self._getitem_iterable(key, axis=axis)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1360, in _getitem_iterable\n",
      "    keyarr, indexer = self._get_listlike_indexer(key, axis)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1558, in _get_listlike_indexer\n",
      "    keyarr, indexer = ax._get_indexer_strict(key, axis_name)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6200, in _get_indexer_strict\n",
      "    self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6252, in _raise_if_missing\n",
      "    raise KeyError(f\"{not_found} not in index\")\n",
      "KeyError: \"['id_1002'] not in index\"\n",
      "Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_1000', 'id_1001', 'id_1003', 'id_1004'], dtype='object')\n",
      "metrics_levels\n",
      "0    id_1000\n",
      "1    id_1001\n",
      "2    id_1002\n",
      "3    id_1003\n",
      "4    id_1004\n",
      "Name: levels, dtype: object\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['id_1002'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m     24\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptuna\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     results_search, best_trial \u001b[38;5;241m=\u001b[39m \u001b[43mbayesian_search_forecaster_multiseries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseries_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexog_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_absolute_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_metric\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maverage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseries_dict_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid_1000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_best\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:1657\u001b[0m, in \u001b[0;36mbayesian_search_forecaster_multiseries\u001b[0;34m(forecaster, series, search_space, steps, metric, initial_train_size, aggregate_metric, fixed_train_size, gap, allow_incomplete_fold, levels, exog, lags_grid, refit, n_trials, random_state, return_best, n_jobs, verbose, show_progress, suppress_warnings, output_file, engine, kwargs_create_study, kwargs_study_optimize)\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggregate_metric \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_aggregate_metrics:\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1653\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`aggregate_metric` must be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_aggregate_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1654\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maggregate_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1655\u001b[0m     )\n\u001b[0;32m-> 1657\u001b[0m results, best_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_bayesian_search_optuna_multiseries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m                          \u001b[49m\u001b[43maggregate_metric\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maggregate_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m                          \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfixed_train_size\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfixed_train_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mallow_incomplete_fold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mallow_incomplete_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mreturn_best\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_best\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m                          \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkwargs_create_study\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkwargs_create_study\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkwargs_study_optimize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkwargs_study_optimize\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results, best_trial\n",
      "File \u001b[0;32m~/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:1912\u001b[0m, in \u001b[0;36m_bayesian_search_optuna_multiseries\u001b[0;34m(forecaster, series, search_space, steps, metric, initial_train_size, aggregate_metric, fixed_train_size, gap, allow_incomplete_fold, levels, exog, refit, n_trials, random_state, return_best, n_jobs, verbose, show_progress, suppress_warnings, output_file, kwargs_create_study, kwargs_study_optimize)\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs_create_study\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1910\u001b[0m     study\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;241m=\u001b[39m TPESampler(seed\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 1912\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_study_optimize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:1852\u001b[0m, in \u001b[0;36m_bayesian_search_optuna_multiseries.<locals>._objective\u001b[0;34m(trial, search_space, forecaster, series, exog, steps, levels, metric, aggregate_metric, initial_train_size, fixed_train_size, gap, allow_incomplete_fold, refit, n_jobs, verbose, suppress_warnings)\u001b[0m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlags\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sample:\n\u001b[1;32m   1850\u001b[0m         forecaster\u001b[38;5;241m.\u001b[39mset_lags(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlags\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 1852\u001b[0m metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbacktesting_forecaster_multiseries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_aggregated_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_train_size\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfixed_train_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_incomplete_fold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mallow_incomplete_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuppress_warnings\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mloc[metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m aggregate_metric, :]\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;66;03m# Store metrics in the variable `metric_values` defined outside _objective.\u001b[39;00m\n",
      "File \u001b[0;32m~/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:933\u001b[0m, in \u001b[0;36mbacktesting_forecaster_multiseries\u001b[0;34m(forecaster, series, steps, metric, initial_train_size, fixed_train_size, gap, allow_incomplete_fold, levels, add_aggregated_metric, exog, refit, interval, n_boot, random_state, in_sample_residuals, n_jobs, verbose, show_progress, suppress_warnings)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    907\u001b[0m         (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`forecaster` must be of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmulti_series_forecasters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    908\u001b[0m          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor all other types of forecasters use the functions available in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe `model_selection` module. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecaster_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    910\u001b[0m     )\n\u001b[1;32m    912\u001b[0m check_backtesting_input(\n\u001b[1;32m    913\u001b[0m     forecaster            \u001b[38;5;241m=\u001b[39m forecaster,\n\u001b[1;32m    914\u001b[0m     steps                 \u001b[38;5;241m=\u001b[39m steps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m     suppress_warnings     \u001b[38;5;241m=\u001b[39m suppress_warnings\n\u001b[1;32m    931\u001b[0m )\n\u001b[0;32m--> 933\u001b[0m metrics_levels, backtest_predictions \u001b[38;5;241m=\u001b[39m \u001b[43m_backtesting_forecaster_multiseries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_aggregated_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_aggregated_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_train_size\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfixed_train_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_incomplete_fold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mallow_incomplete_fold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_sample_residuals\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43min_sample_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuppress_warnings\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_levels, backtest_predictions\n",
      "File \u001b[0;32m~/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:752\u001b[0m, in \u001b[0;36m_backtesting_forecaster_multiseries\u001b[0;34m(forecaster, series, steps, metric, initial_train_size, fixed_train_size, gap, allow_incomplete_fold, levels, add_aggregated_metric, exog, refit, interval, n_boot, random_state, in_sample_residuals, n_jobs, verbose, show_progress, suppress_warnings)\u001b[0m\n\u001b[1;32m    749\u001b[0m         cols \u001b[38;5;241m=\u001b[39m cols \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lower_bound\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_upper_bound\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    750\u001b[0m     backtest_predictions\u001b[38;5;241m.\u001b[39mloc[no_valid_index, cols] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m--> 752\u001b[0m metrics_levels \u001b[38;5;241m=\u001b[39m \u001b[43m_calculate_metrics_multiseries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbacktest_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_index\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspan_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_aggregated_metric\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_aggregated_metric\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m set_skforecast_warnings(suppress_warnings, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_levels, backtest_predictions\n",
      "File \u001b[0;32m~/varios/skforecast/skforecast/model_selection_multiseries/model_selection_multiseries.py:362\u001b[0m, in \u001b[0;36m_calculate_metrics_multiseries\u001b[0;34m(series, predictions, folds, span_index, metrics, levels, add_aggregated_metric)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_levels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics_levels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 362\u001b[0m n_predictions_levels \u001b[38;5;241m=\u001b[39m \u001b[43mn_predictions_levels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetrics_levels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlevels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m  metrics_levels\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    364\u001b[0m     weighted_averages[col] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[1;32m    365\u001b[0m         metrics_levels[col],\n\u001b[1;32m    366\u001b[0m         weights\u001b[38;5;241m=\u001b[39mn_predictions_levels\n\u001b[1;32m    367\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/skforecast_13_py11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['id_1002'] not in index\""
     ]
    }
   ],
   "source": [
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "    regressor=LGBMRegressor(\n",
    "        n_estimators=2, random_state=123, verbose=-1, max_depth=2\n",
    "    ),\n",
    "    lags=14,\n",
    "    encoding='ordinal',\n",
    "    dropna_from_series=False,\n",
    "    transformer_series=StandardScaler(),\n",
    "    transformer_exog=StandardScaler(),\n",
    ")\n",
    "\n",
    "lags_grid = [[5], [1, 7, 14]]\n",
    "\n",
    "def search_space(trial):\n",
    "    search_space = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 5),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"lags\": trial.suggest_categorical(\"lags\", lags_grid),\n",
    "    }\n",
    "\n",
    "    return search_space\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "    \n",
    "    results_search, best_trial = bayesian_search_forecaster_multiseries(\n",
    "        forecaster         = forecaster,\n",
    "        series             = series_dict,\n",
    "        exog               = exog_dict,\n",
    "        search_space       = search_space,\n",
    "        metric             = 'mean_absolute_error',\n",
    "        aggregate_metric   ='average',\n",
    "        initial_train_size = len(series_dict_train['id_1000']),\n",
    "        steps              = 24,\n",
    "        refit              = False,\n",
    "        n_trials           = 3,\n",
    "        return_best        = False,\n",
    "        show_progress      = False,\n",
    "        verbose            = False,\n",
    "        suppress_warnings  = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aggregation</th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weighted_average</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.23598059857016607}</td>\n",
       "      <td>0.208803</td>\n",
       "      <td>0.235981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.398196343012209}</td>\n",
       "      <td>0.208811</td>\n",
       "      <td>0.398196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.4441865222328282}</td>\n",
       "      <td>0.208813</td>\n",
       "      <td>0.444187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.53623586010342}</td>\n",
       "      <td>0.208818</td>\n",
       "      <td>0.536236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.7252189487445193}</td>\n",
       "      <td>0.208827</td>\n",
       "      <td>0.725219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.5558016213920624}</td>\n",
       "      <td>0.209135</td>\n",
       "      <td>0.555802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.6995044937418831}</td>\n",
       "      <td>0.209139</td>\n",
       "      <td>0.699504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.7406154516747153}</td>\n",
       "      <td>0.209140</td>\n",
       "      <td>0.740615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.8509374761370117}</td>\n",
       "      <td>0.209143</td>\n",
       "      <td>0.850937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[weighted_average, weighted_average]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.9809565564007693}</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.980957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            aggregation          lags  \\\n",
       "0                      weighted_average  [1, 2, 3, 4]   \n",
       "1  [weighted_average, weighted_average]  [1, 2, 3, 4]   \n",
       "2  [weighted_average, weighted_average]  [1, 2, 3, 4]   \n",
       "3  [weighted_average, weighted_average]  [1, 2, 3, 4]   \n",
       "4  [weighted_average, weighted_average]  [1, 2, 3, 4]   \n",
       "5  [weighted_average, weighted_average]        [1, 2]   \n",
       "6  [weighted_average, weighted_average]        [1, 2]   \n",
       "7  [weighted_average, weighted_average]        [1, 2]   \n",
       "8  [weighted_average, weighted_average]        [1, 2]   \n",
       "9  [weighted_average, weighted_average]        [1, 2]   \n",
       "\n",
       "                           params  mean_absolute_error     alpha  \n",
       "0  {'alpha': 0.23598059857016607}             0.208803  0.235981  \n",
       "1    {'alpha': 0.398196343012209}             0.208811  0.398196  \n",
       "2   {'alpha': 0.4441865222328282}             0.208813  0.444187  \n",
       "3     {'alpha': 0.53623586010342}             0.208818  0.536236  \n",
       "4   {'alpha': 0.7252189487445193}             0.208827  0.725219  \n",
       "5   {'alpha': 0.5558016213920624}             0.209135  0.555802  \n",
       "6   {'alpha': 0.6995044937418831}             0.209139  0.699504  \n",
       "7   {'alpha': 0.7406154516747153}             0.209140  0.740615  \n",
       "8   {'alpha': 0.8509374761370117}             0.209143  0.850937  \n",
       "9   {'alpha': 0.9809565564007693}             0.209146  0.980957  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    np.array([['weighted_average', np.array([1, 2, 3, 4]),\n",
    "        {'alpha': 0.23598059857016607}, 0.20880273566554797,\n",
    "        0.23598059857016607],\n",
    "    [list(['weighted_average']*2), np.array([1, 2, 3, 4]),\n",
    "        {'alpha': 0.398196343012209}, 0.2088108334929226,\n",
    "        0.398196343012209],\n",
    "    [list(['weighted_average']*2), np.array([1, 2, 3, 4]),\n",
    "        {'alpha': 0.4441865222328282}, 0.2088131177203144,\n",
    "        0.4441865222328282],\n",
    "    [list(['weighted_average']*2), np.array([1, 2, 3, 4]),\n",
    "        {'alpha': 0.53623586010342}, 0.2088176743151537,\n",
    "        0.53623586010342],\n",
    "    [list(['weighted_average']*2), np.array([1, 2, 3, 4]),\n",
    "        {'alpha': 0.7252189487445193}, 0.2088269659234972,\n",
    "        0.7252189487445193],\n",
    "    [list(['weighted_average']*2), np.array([1, 2]), {'alpha': 0.5558016213920624},\n",
    "        0.20913532870732662, 0.5558016213920624],\n",
    "    [list(['weighted_average']*2), np.array([1, 2]), {'alpha': 0.6995044937418831},\n",
    "        0.20913908163271674, 0.6995044937418831],\n",
    "    [list(['weighted_average']*2), np.array([1, 2]), {'alpha': 0.7406154516747153},\n",
    "        0.20914015254956903, 0.7406154516747153],\n",
    "    [list(['weighted_average']*2), np.array([1, 2]), {'alpha': 0.8509374761370117},\n",
    "        0.20914302038975385, 0.8509374761370117],\n",
    "    [list(['weighted_average']*2), np.array([1, 2]), {'alpha': 0.9809565564007693},\n",
    "        0.20914638910039665, 0.9809565564007693]], dtype=object),\n",
    "    columns=['aggregation', 'lags', 'params', 'mean_absolute_error', 'alpha'],\n",
    "    index=pd.Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n",
    ").astype({\n",
    "    'mean_absolute_error': float, \n",
    "    'alpha': float\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78d62c1713fdacd99ef7c429003c7324b36fbb551fb8b6860a7ea73e9338235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
