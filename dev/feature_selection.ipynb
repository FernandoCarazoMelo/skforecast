{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\JoaquÃ­n Amat\\\\Documents\\\\GitHub\\\\skforecast'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "str(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.datasets import fetch_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astral.sun import sun\n",
    "from astral import LocationInfo\n",
    "from skforecast.datasets import fetch_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(\n",
    "    selector: object,\n",
    "    forecaster: object,\n",
    "    y: Union[pd.Series, pd.DataFrame],\n",
    "    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n",
    "    force_lags_selection: bool = False,\n",
    "    subsample: Union[int, float] = 0.5,\n",
    "    verbose: bool = True,\n",
    ") -> Union[list, list]:\n",
    "    \"\"\"\n",
    "    Feature selection using any of the classes from sklearn.feature_selection \n",
    "    module (such as RFECV, SelectFromModel, etc).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selector : object\n",
    "        A feature selector from sklearn.feature_selection.\n",
    "    forecaster : object\n",
    "        A forecaster from skforecast.\n",
    "    y : pd.Series or pd.DataFrame   \n",
    "        Target time series to which feature selection will be applied.\n",
    "    exog : pd.Series or pd.DataFrame, optional (default=None)   \n",
    "        Exogenous variables.\n",
    "    force_lags_selection : bool, optional (default=True)\n",
    "        Whether to include all lagged variables as part of the selected features.\n",
    "        If `False` (default), lagged variables are also evaluated by the selector\n",
    "        and only those that are selected are included in the final list of selected\n",
    "        features. If `True`, exogenous variables are evaluated without the presence\n",
    "        of lagged variables and then all lagged variables are added to the selected\n",
    "        exogenous variables.\n",
    "    subsample : int or float, optional (default=0.5)\n",
    "        Number of records to use for feature selection. If int, number of records\n",
    "        to use. If float, proportion of records to use.\n",
    "    verbose : bool, optional (default=True)\n",
    "        Print information about feature selection process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    selected_lags : list\n",
    "        List of selected lags.\n",
    "    selected_exog : list\n",
    "        List of selected exogenous variables.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, y_train = forecaster.create_train_X_y(\n",
    "                            y    = y,\n",
    "                            exog = exog,\n",
    "                        )\n",
    "\n",
    "    if force_lags_selection:\n",
    "        lags_cols = [col for col in X_train.columns if col.startswith(\"lag_\")]\n",
    "        X_train = X_train.drop(columns=lags_cols)\n",
    "\n",
    "    if isinstance(subsample, float):\n",
    "        subsample = int(len(X_train)*subsample)\n",
    "\n",
    "    rng = np.random.default_rng(seed=785412)\n",
    "    sample = rng.choice(X_train.index, size=subsample, replace=False)\n",
    "    X_train_sample = X_train.loc[sample, :]\n",
    "    y_train_sample = y_train.loc[sample]\n",
    "    \n",
    "    selector.fit(X_train_sample, y_train_sample)\n",
    "    selected_features = selector.get_feature_names_out()\n",
    "\n",
    "    if force_lags_selection:\n",
    "        selected_lags = lags_cols\n",
    "    else:\n",
    "        selected_lags = [\n",
    "            int(feature.replace(\"lag_\", \"\"))\n",
    "            for feature in selected_features\n",
    "            if feature.startswith(\"lag_\")\n",
    "        ]\n",
    "        \n",
    "    selected_exog = [\n",
    "        feature\n",
    "        for feature in selected_features\n",
    "        if not feature.startswith(\"lag_\")\n",
    "    ]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Recursive feature elimination\")\n",
    "        print(\"-----------------------------\")\n",
    "        print(f\"Total number of features available: {X_train.shape[1]}\") \n",
    "        print(f\"Total number of records available: {X_train.shape[0]}\")\n",
    "        print(f\"Total number of records used for feature selection: {X_train_sample.shape[0]}\")\n",
    "        print(f\"Number of features selected: {len(selected_features)}\")\n",
    "        print(f\"Selected lags: {selected_lags}\")\n",
    "        print(f\"Selected exog : \\n {selected_exog}\")\n",
    "\n",
    "    return selected_lags, selected_exog    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_sharing\n",
      "------------\n",
      "Hourly usage of the bike share system in the city of Washington D.C. during the\n",
      "years 2011 and 2012. In addition to the number of users per hour, information\n",
      "about weather conditions and holidays is available.\n",
      "Fanaee-T,Hadi. (2013). Bike Sharing Dataset. UCI Machine Learning Repository.\n",
      "https://doi.org/10.24432/C5W894.\n",
      "Shape of the dataset: (17544, 12)\n"
     ]
    }
   ],
   "source": [
    "# Downloading data\n",
    "# ==============================================================================\n",
    "data = fetch_dataset('bike_sharing', raw=True)\n",
    "\n",
    "# Preprocessing data (setting index and frequency)\n",
    "# ==============================================================================\n",
    "data = data[['date_time', 'users', 'holiday', 'weather', 'temp', 'atemp', 'hum', 'windspeed']]\n",
    "data['date_time'] = pd.to_datetime(data['date_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "data = data.set_index('date_time')\n",
    "data = data.asfreq('H')\n",
    "data = data.sort_index()\n",
    "data.head()\n",
    "\n",
    "# Calendar features\n",
    "# ==============================================================================\n",
    "calendar_features = pd.DataFrame(index=data.index)\n",
    "calendar_features['month'] = calendar_features.index.month\n",
    "calendar_features['week_of_year'] = calendar_features.index.isocalendar().week\n",
    "calendar_features['week_day'] = calendar_features.index.day_of_week + 1\n",
    "calendar_features['hour_day'] = calendar_features.index.hour + 1\n",
    "\n",
    "# Sunlight features\n",
    "# ==============================================================================\n",
    "location = LocationInfo(\n",
    "    name='Washington DC',\n",
    "    region='USA',\n",
    "    timezone='US/Eastern',\n",
    "    latitude=40.516666666666666,\n",
    "    longitude=-77.03333333333333\n",
    ")\n",
    "sunrise_hour = [\n",
    "    sun(location.observer, date=date, tzinfo=location.timezone)['sunrise'].hour\n",
    "    for date in data.index\n",
    "]\n",
    "sunset_hour = [\n",
    "    sun(location.observer, date=date, tzinfo=location.timezone)['sunset'].hour\n",
    "    for date in data.index\n",
    "]\n",
    "sun_light_features = pd.DataFrame({\n",
    "                         'sunrise_hour': sunrise_hour,\n",
    "                         'sunset_hour': sunset_hour}, \n",
    "                         index = data.index\n",
    "                     )\n",
    "sun_light_features['daylight_hours'] = (\n",
    "    sun_light_features['sunset_hour'] - sun_light_features['sunrise_hour']\n",
    ")\n",
    "sun_light_features['is_daylight'] = np.where(\n",
    "                                        (data.index.hour >= sun_light_features['sunrise_hour']) & \\\n",
    "                                        (data.index.hour < sun_light_features['sunset_hour']),\n",
    "                                        1,\n",
    "                                        0\n",
    "                                    )\n",
    "\n",
    "# Holiday features\n",
    "# ==============================================================================\n",
    "holiday_features = data[['holiday']].astype(int)\n",
    "holiday_features['holiday_previous_day'] = holiday_features['holiday'].shift(24)\n",
    "holiday_features['holiday_next_day'] = holiday_features['holiday'].shift(-24)\n",
    "\n",
    "# Temperature features\n",
    "# ==============================================================================\n",
    "temp_features = data[['temp']].copy()\n",
    "temp_features['temp_roll_mean_1_day'] = temp_features['temp'].rolling(24, closed='left').mean()\n",
    "temp_features['temp_roll_mean_7_day'] = temp_features['temp'].rolling(24*7, closed='left').mean()\n",
    "temp_features['temp_roll_max_1_day'] = temp_features['temp'].rolling(24, closed='left').max()\n",
    "temp_features['temp_roll_min_1_day'] = temp_features['temp'].rolling(24, closed='left').min()\n",
    "temp_features['temp_roll_max_7_day'] = temp_features['temp'].rolling(24*7, closed='left').max()\n",
    "temp_features['temp_roll_min_7_day'] = temp_features['temp'].rolling(24*7, closed='left').min()\n",
    "\n",
    "\n",
    "# Merge all exogenous variables\n",
    "# ==============================================================================\n",
    "df_exogenous_features = pd.concat([\n",
    "                            calendar_features,\n",
    "                            sun_light_features,\n",
    "                            temp_features,\n",
    "                            holiday_features\n",
    "                        ], axis=1)\n",
    "\n",
    "df_exogenous_features.head(4)\n",
    "\n",
    "# Cliclical encoding of calendar and sunlight features\n",
    "# ==============================================================================\n",
    "def cyclical_encoding(data: pd.Series, cycle_length: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode a cyclical feature with two new features sine and cosine.\n",
    "    The minimum value of the feature is assumed to be 0. The maximum value\n",
    "    of the feature is passed as an argument.\n",
    "      \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Series\n",
    "        Series with the feature to encode.\n",
    "    cycle_length : int\n",
    "        The length of the cycle. For example, 12 for months, 24 for hours, etc.\n",
    "        This value is used to calculate the angle of the sin and cos.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : pd.DataFrame\n",
    "        Dataframe with the two new features sin and cos.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sin = np.sin(2 * np.pi * data/cycle_length)\n",
    "    cos = np.cos(2 * np.pi * data/cycle_length)\n",
    "    result =  pd.DataFrame({\n",
    "                  f\"{data.name}_sin\": sin,\n",
    "                  f\"{data.name}_cos\": cos\n",
    "              })\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "month_encoded = cyclical_encoding(df_exogenous_features['month'], cycle_length=12)\n",
    "week_of_year_encoded = cyclical_encoding(df_exogenous_features['week_of_year'], cycle_length=52)\n",
    "week_day_encoded = cyclical_encoding(df_exogenous_features['week_day'], cycle_length=7)\n",
    "hour_day_encoded = cyclical_encoding(df_exogenous_features['hour_day'], cycle_length=24)\n",
    "sunrise_hour_encoded = cyclical_encoding(df_exogenous_features['sunrise_hour'], cycle_length=24)\n",
    "sunset_hour_encoded = cyclical_encoding(df_exogenous_features['sunset_hour'], cycle_length=24)\n",
    "\n",
    "cyclical_features = pd.concat([\n",
    "                        month_encoded,\n",
    "                        week_of_year_encoded,\n",
    "                        week_day_encoded,\n",
    "                        hour_day_encoded,\n",
    "                        sunrise_hour_encoded,\n",
    "                        sunset_hour_encoded\n",
    "                    ], axis=1)\n",
    "\n",
    "df_exogenous_features = pd.concat([df_exogenous_features, cyclical_features], axis=1)\n",
    "df_exogenous_features.head(3)\n",
    "\n",
    "# Interaction between exogenous variables\n",
    "# ==============================================================================\n",
    "transformer_poly = PolynomialFeatures(\n",
    "                       degree           = 2,\n",
    "                       interaction_only = True,\n",
    "                       include_bias     = False\n",
    "                   ).set_output(transform=\"pandas\")\n",
    "\n",
    "poly_cols = [\n",
    "    'month_sin', \n",
    "    'month_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos',\n",
    "    'week_day_sin',\n",
    "    'week_day_cos',\n",
    "    'hour_day_sin',\n",
    "    'hour_day_cos',\n",
    "    'sunrise_hour_sin',\n",
    "    'sunrise_hour_cos',\n",
    "    'sunset_hour_sin',\n",
    "    'sunset_hour_cos',\n",
    "    'daylight_hours',\n",
    "    'is_daylight',\n",
    "    'holiday_previous_day',\n",
    "    'holiday_next_day',\n",
    "    'temp_roll_mean_1_day',\n",
    "    'temp_roll_mean_7_day',\n",
    "    'temp_roll_max_1_day',\n",
    "    'temp_roll_min_1_day',\n",
    "    'temp_roll_max_7_day',\n",
    "    'temp_roll_min_7_day',\n",
    "    'temp',\n",
    "    'holiday'\n",
    "]\n",
    "\n",
    "poly_features = transformer_poly.fit_transform(df_exogenous_features[poly_cols].dropna())\n",
    "poly_features = poly_features.drop(columns=poly_cols)\n",
    "poly_features.columns = [f\"poly_{col}\" for col in poly_features.columns]\n",
    "poly_features.columns = poly_features.columns.str.replace(\" \", \"__\")\n",
    "df_exogenous_features = pd.concat([df_exogenous_features, poly_features], axis=1)\n",
    "df_exogenous_features.head(4)\n",
    "\n",
    "# Select exogenous variables to be included in the model\n",
    "# ==============================================================================\n",
    "exog_features = []\n",
    "# Columns that ends with _sin or _cos are selected\n",
    "exog_features.extend(df_exogenous_features.filter(regex='_sin$|_cos$').columns.tolist())\n",
    "# columns that start with temp_ are selected\n",
    "exog_features.extend(df_exogenous_features.filter(regex='^temp_.*').columns.tolist())\n",
    "# Columns that start with holiday_ are selected\n",
    "exog_features.extend(df_exogenous_features.filter(regex='^holiday_.*').columns.tolist())\n",
    "exog_features.extend(['temp', 'holiday'])\n",
    "\n",
    "df_exogenous_features = df_exogenous_features.filter(exog_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exogenous_features = df_exogenous_features.dropna()\n",
    "data = data.loc[df_exogenous_features.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination\n",
      "-----------------------------\n",
      "Total number of features available: 257\n",
      "Total number of records available: 17183\n",
      "Total number of records used for feature selection: 1718\n",
      "Number of features selected: 106\n",
      "Selected lags: [1, 2, 3, 8, 9, 22, 24, 25, 47, 96, 97, 120, 121, 144, 145, 146, 147, 159, 160, 161, 162, 166, 167, 168, 169]\n",
      "Selected exog : \n",
      " ['month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos', 'week_day_sin', 'week_day_cos', 'hour_day_sin', 'hour_day_cos', 'sunrise_hour_sin', 'sunrise_hour_cos', 'sunset_hour_sin', 'sunset_hour_cos', 'poly_month_sin__month_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__week_day_sin', 'poly_month_sin__week_day_cos', 'poly_month_sin__hour_day_sin', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_sin', 'poly_month_sin__sunrise_hour_cos', 'poly_month_sin__sunset_hour_sin', 'poly_month_sin__sunset_hour_cos', 'poly_month_cos__week_day_sin', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_sin', 'poly_month_cos__hour_day_cos', 'poly_month_cos__sunrise_hour_sin', 'poly_month_cos__sunrise_hour_cos', 'poly_month_cos__sunset_hour_sin', 'poly_month_cos__sunset_hour_cos', 'poly_week_of_year_sin__week_of_year_cos', 'poly_week_of_year_sin__week_day_sin', 'poly_week_of_year_sin__week_day_cos', 'poly_week_of_year_sin__hour_day_sin', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_sin', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_sin__sunset_hour_sin', 'poly_week_of_year_sin__sunset_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_sin', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_of_year_cos__sunset_hour_sin', 'poly_week_of_year_cos__sunset_hour_cos', 'poly_week_day_sin__hour_day_sin', 'poly_week_day_sin__hour_day_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_sin__sunset_hour_sin', 'poly_week_day_sin__sunset_hour_cos', 'poly_week_day_cos__hour_day_cos', 'poly_week_day_cos__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__hour_day_cos', 'poly_hour_day_sin__sunrise_hour_sin', 'poly_hour_day_sin__sunrise_hour_cos', 'poly_hour_day_sin__sunset_hour_sin', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunrise_hour_cos', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos', 'poly_sunrise_hour_sin__sunrise_hour_cos', 'poly_sunrise_hour_sin__sunset_hour_sin', 'poly_sunrise_hour_sin__sunset_hour_cos', 'poly_sunrise_hour_cos__sunset_hour_sin', 'poly_sunrise_hour_cos__sunset_hour_cos', 'poly_sunset_hour_sin__sunset_hour_cos', 'temp_roll_mean_1_day', 'temp_roll_mean_7_day', 'temp_roll_max_1_day', 'temp_roll_min_1_day', 'temp_roll_max_7_day', 'holiday_previous_day', 'holiday_next_day', 'temp']\n"
     ]
    }
   ],
   "source": [
    "forecaster = ForecasterAutoreg(\n",
    "                regressor = Ridge(random_state=123),\n",
    "                lags      = 169,\n",
    "            )\n",
    "\n",
    "selector = RFECV(\n",
    "    estimator              = forecaster.regressor,\n",
    "    min_features_to_select = 1,\n",
    "    cv                     = 5,\n",
    ")\n",
    "\n",
    "selected_lags, selected_exog = select_features(\n",
    "    selector             = selector,\n",
    "    forecaster           = forecaster,\n",
    "    y                    = data['users'],\n",
    "    exog                 = df_exogenous_features,\n",
    "    force_lags_selection = False,\n",
    "    subsample            = 0.1,\n",
    "    verbose              = True,\n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursive feature elimination\n",
      "-----------------------------\n",
      "Total number of features available: 257\n",
      "Total number of records available: 17183\n",
      "Total number of records used for feature selection: 1718\n",
      "Number of features selected: 25\n",
      "Selected lags: []\n",
      "Selected exog : \n",
      " ['week_of_year_sin', 'week_day_sin', 'poly_month_sin__month_cos', 'poly_month_sin__week_of_year_sin', 'poly_month_sin__week_of_year_cos', 'poly_month_sin__hour_day_cos', 'poly_month_sin__sunrise_hour_sin', 'poly_month_sin__sunrise_hour_cos', 'poly_month_cos__week_day_cos', 'poly_month_cos__hour_day_cos', 'poly_week_of_year_sin__hour_day_cos', 'poly_week_of_year_sin__sunrise_hour_cos', 'poly_week_of_year_cos__week_day_sin', 'poly_week_of_year_cos__week_day_cos', 'poly_week_of_year_cos__hour_day_sin', 'poly_week_of_year_cos__hour_day_cos', 'poly_week_of_year_cos__sunrise_hour_cos', 'poly_week_day_sin__sunrise_hour_sin', 'poly_week_day_cos__sunrise_hour_cos', 'poly_week_day_cos__sunset_hour_sin', 'poly_week_day_cos__sunset_hour_cos', 'poly_hour_day_sin__sunset_hour_cos', 'poly_hour_day_cos__sunrise_hour_sin', 'poly_hour_day_cos__sunset_hour_sin', 'poly_hour_day_cos__sunset_hour_cos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(\n",
    "    estimator    = forecaster.regressor,\n",
    "    threshold    = 0.25,\n",
    "    max_features = 25\n",
    ")\n",
    "\n",
    "selected_lags, selected_exog = select_features(\n",
    "    selector             = selector,\n",
    "    forecaster           = forecaster,\n",
    "    y                    = data['users'],\n",
    "    exog                 = df_exogenous_features,\n",
    "    force_lags_selection = False,\n",
    "    subsample            = 0.1,\n",
    "    verbose              = True,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShuffleSplit\n\u001b[0;32m      4\u001b[0m selector \u001b[38;5;241m=\u001b[39m SequentialFeatureSelector(\n\u001b[0;32m      5\u001b[0m     estimator    \u001b[38;5;241m=\u001b[39m forecaster\u001b[38;5;241m.\u001b[39mregressor,\n\u001b[0;32m      6\u001b[0m     n_features_to_select \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     n_jobs       \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m selected_lags, selected_exog \u001b[38;5;241m=\u001b[39m \u001b[43mselect_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforecaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf_exogenous_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_lags_selection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubsample\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m  \n",
      "Cell \u001b[1;32mIn[3], line 62\u001b[0m, in \u001b[0;36mselect_features\u001b[1;34m(selector, forecaster, y, exog, force_lags_selection, subsample, verbose)\u001b[0m\n\u001b[0;32m     59\u001b[0m X_train_sample \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mloc[sample, :]\n\u001b[0;32m     60\u001b[0m y_train_sample \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mloc[sample]\n\u001b[1;32m---> 62\u001b[0m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_lags_selection:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:248\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    246\u001b[0m is_auto_select \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_to_select \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[1;32m--> 248\u001b[0m     new_feature_idx, new_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_best_new_feature_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloned_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_mask\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_auto_select \u001b[38;5;129;01mand\u001b[39;00m ((new_score \u001b[38;5;241m-\u001b[39m old_score) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol):\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:279\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[1;34m(self, estimator, X, y, cv, current_mask)\u001b[0m\n\u001b[0;32m    277\u001b[0m         candidate_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mcandidate_mask\n\u001b[0;32m    278\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[:, candidate_mask]\n\u001b[1;32m--> 279\u001b[0m     scores[feature_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    287\u001b[0m new_feature_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(scores, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\joblib\\parallel.py:1648\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1646\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detach_generator_exit:\n\u001b[1;32m-> 1648\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_terminate_and_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_remaining_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1651\u001b[0m     batched_results \u001b[38;5;241m=\u001b[39m _remaining_outputs\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\joblib\\parallel.py:1351\u001b[0m, in \u001b[0;36mParallel._terminate_and_reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend:\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\joblib\\_parallel_backends.py:622\u001b[0m, in \u001b[0;36mLokyBackend.terminate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;66;03m# Don't terminate the workers as we want to reuse them in later\u001b[39;00m\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;66;03m# calls, but cleanup the temporary resources that the Parallel call\u001b[39;00m\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;66;03m# created. This 'hack' requires a private, low-level operation.\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temp_folder_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_temporary_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_batch_stats()\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\joblib\\_memmapping_reducer.py:636\u001b[0m, in \u001b[0;36mTemporaryResourcesManager._clean_temporary_resources\u001b[1;34m(self, context_id, force, allow_non_empty)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;66;03m# Clean up the folder if possible, either if it is empty or\u001b[39;00m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;66;03m# if none of the files in it are in used and allow_non_empty.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 636\u001b[0m     \u001b[43mdelete_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_non_empty\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# Forget the folder once it has been deleted\u001b[39;00m\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_temp_folders\u001b[38;5;241m.\u001b[39mpop(context_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\skforecast_10_py11\\Lib\\site-packages\\joblib\\disk.py:136\u001b[0m, in \u001b[0;36mdelete_folder\u001b[1;34m(folder_path, onerror, allow_non_empty)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err_count \u001b[38;5;241m>\u001b[39m RM_SUBDIRS_N_RETRY:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;66;03m# the folder cannot be deleted right now. It maybe\u001b[39;00m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;66;03m# because some temporary files have not been deleted\u001b[39;00m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;66;03m# yet.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(RM_SUBDIRS_RETRY_TIME)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "selector = SequentialFeatureSelector(\n",
    "    estimator    = forecaster.regressor,\n",
    "    n_features_to_select = 25,\n",
    "    direction    = 'forward',\n",
    "    cv           = ShuffleSplit(n_splits=1, test_size=0.3, random_state=951),\n",
    "    scoring      = 'neg_mean_absolute_error',\n",
    "    n_jobs       = -1\n",
    ")\n",
    "\n",
    "selected_lags, selected_exog = select_features(\n",
    "    selector             = selector,\n",
    "    forecaster           = forecaster,\n",
    "    y                    = data['users'],\n",
    "    exog                 = df_exogenous_features,\n",
    "    force_lags_selection = False,\n",
    "    subsample            = 0.1,\n",
    "    verbose              = True,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def feature_selection_rfecv(\n",
    "#     forecaster: object, \n",
    "#     y: Union[pd.Series, pd.DataFrame],\n",
    "#     exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n",
    "#     min_features_to_select: int = 1,\n",
    "#     cv: Union[int, object, None] = None,\n",
    "#     select_lags: bool = True,\n",
    "#     subsample: Union[int, float] = 0.5,\n",
    "#     verbose: bool = True,\n",
    "#     **kwargs\n",
    "# ) -> list:\n",
    "#     \"\"\"\n",
    "#     Recursive feature elimination with cross-validation to select features based\n",
    "#     on sklearn.feature_selection.RFECV.\n",
    "\n",
    "#     Given a regressor that assigns weights to features (such as coef_, \n",
    "#     feature_importances_), the goal of recursive feature elimination (RFE) is to\n",
    "#     select features by recursively considering smaller and smaller sets of features.\n",
    "#     First, the regressor is trained with all the available features and the importance\n",
    "#     of each feature is obtained. Then, the least important features are pruned\n",
    "#     from the current set of features and the performance of the regressor is\n",
    "#     evaluated using cross-validation, if the performance of the model has not\n",
    "#     decreased, then the pruned features are discarded. The process is repeated\n",
    "#     until the desired number of features to select is reached or there are no\n",
    "#     more features to discard. \n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     forecaster : object\n",
    "#         A forecaster from skforecast.\n",
    "#     y : pd.Series or pd.DataFrame\n",
    "#         Target time series to which feature selection will be applied.\n",
    "#     exog : pd.Series or pd.DataFrame, optional (default=None)\n",
    "#         Exogenous variables.\n",
    "#     min_features_to_select : int, optional (default=1)\n",
    "#         The minimum number of features to be selected.\n",
    "#     include_lags: bool, optional (default=True)\n",
    "#         Whether to include lagged variables in feature selection. If `False`, exogenous\n",
    "#         variables are evaluated without the presence of lagged variables.\n",
    "#     subsample : int or float, optional (default=0.5)\n",
    "#         Number of records to use for feature selection. If int, number of records\n",
    "#         to use. If float, proportion of records to use.\n",
    "#     cv : int, cross-validation generator or an iterable, default=None\n",
    "#         Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "#         - None, to use the default 5-fold cross-validation\n",
    "#         - integer, to specify the number of folds\n",
    "#         - CV splitter\n",
    "#         - An iterable yielding (train, test) splits as arrays of indices    \n",
    "#     **kwargs : optional\n",
    "#         Aditional arguments of sklearn.feature_selection.RFECV.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     selected_features : list\n",
    "#         List of selected features.\n",
    "#     \"\"\"\n",
    "\n",
    "#     X_train, y_train = forecaster.create_train_X_y(\n",
    "#                         y    = y,\n",
    "#                         exog = exog,\n",
    "#                     )\n",
    "\n",
    "#     if not select_lags:\n",
    "#         lags_cols = [col for col in X_train.columns if col.startswith(\"lag_\")]\n",
    "#         X_train = X_train.drop(columns=lags_cols)\n",
    "\n",
    "#     if isinstance(subsample, float):\n",
    "#         subsample = int(len(X_train)*subsample)\n",
    "\n",
    "#     rng = np.random.default_rng(seed=785412)\n",
    "#     sample = rng.choice(X_train.index, size=subsample, replace=False)\n",
    "#     X_train_sample = X_train.loc[sample, :]\n",
    "#     y_train_sample = y_train.loc[sample]\n",
    "\n",
    "#     selector = RFECV(\n",
    "#         estimator              = forecaster.regressor,\n",
    "#         min_features_to_select = min_features_to_select,\n",
    "#         cv                     = cv,\n",
    "#         **kwargs\n",
    "#     )\n",
    "#     selector.fit(X_train_sample, y_train_sample)\n",
    "#     selected_features = selector.get_feature_names_out()\n",
    "#     selected_lags = [\n",
    "#         int(feature.replace(\"lag_\", \"\"))\n",
    "#         for feature in selected_features\n",
    "#         if feature.startswith(\"lag_\")\n",
    "#     ]\n",
    "#     selected_exog = [\n",
    "#         feature\n",
    "#         for feature in selected_features\n",
    "#         if not feature.startswith(\"lag_\")\n",
    "#     ]\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"Recursive feature elimination\")\n",
    "#         print(\"-----------------------------\")\n",
    "#         print(f\"Total number of features available: {X_train.shape[1]}\") \n",
    "#         print(f\"Total number of records available: {X_train.shape[0]}\")\n",
    "#         print(f\"Total number of records used for feature selection: {X_train_sample.shape[0]}\")\n",
    "#         print(f\"Number of features selected: {len(selected_features)}\")\n",
    "#         print(f\"Selected lags: {selected_lags}\")\n",
    "#         print(f\"Selected exog : \\n {selected_exog}\")\n",
    "\n",
    "#     return selected_lags, selected_exog\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection_rfecv(\n",
    "#     forecaster             = forecaster,\n",
    "#     y                      = data['users'],\n",
    "#     exog                   = df_exogenous_features,\n",
    "#     select_lags            = True,\n",
    "#     min_features_to_select = 25,\n",
    "#     cv                     = 3,\n",
    "#     subsample              = 0.5,\n",
    "#     step                   = 1,\n",
    "#     scoring                = 'neg_mean_absolute_error',\n",
    "#     n_jobs                 = -1\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78d62c1713fdacd99ef7c429003c7324b36fbb551fb8b6860a7ea73e9338235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
