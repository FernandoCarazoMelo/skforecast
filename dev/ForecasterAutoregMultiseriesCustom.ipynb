{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/varios/skforecast'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "str(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, List, Tuple, Any, Optional, Callable\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.pipeline\n",
    "from sklearn.base import clone\n",
    "from copy import copy, deepcopy\n",
    "import inspect\n",
    "\n",
    "import skforecast\n",
    "from skforecast.ForecasterBase import ForecasterBase\n",
    "from skforecast.utils import initialize_lags\n",
    "from skforecast.utils import initialize_weights\n",
    "from skforecast.utils import check_y\n",
    "from skforecast.utils import check_exog\n",
    "from skforecast.utils import check_interval\n",
    "from skforecast.utils import preprocess_y\n",
    "from skforecast.utils import preprocess_last_window\n",
    "from skforecast.utils import preprocess_exog\n",
    "from skforecast.utils import expand_index\n",
    "from skforecast.utils import check_predict_input\n",
    "from skforecast.utils import transform_series\n",
    "from skforecast.utils import transform_dataframe\n",
    "\n",
    "logging.basicConfig(\n",
    "    format = '%(name)-10s %(levelname)-5s %(message)s', \n",
    "    level  = logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecasterAutoregMultiSeriesCustom(ForecasterBase):\n",
    "    \"\"\"\n",
    "    This class turns any regressor compatible with the scikit-learn API into a\n",
    "    recursive autoregressive (multi-step) forecaster for multiple series with a custom\n",
    "    function to create predictors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor : regressor or pipeline compatible with the scikit-learn API\n",
    "        An instance of a regressor or pipeline compatible with the scikit-learn API.\n",
    "        \n",
    "    fun_predictors : Callable\n",
    "        Function that takes a numpy ndarray as a window of values as input and  \n",
    "        returns a numpy ndarray with the predictors associated with that window.\n",
    "        \n",
    "    window_size : int\n",
    "        Size of the window needed by `fun_predictors` to create the predictors.\n",
    "\n",
    "    transformer_series : transformer (preprocessor) or dict of transformers, default `None`\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n",
    "        If a single transformer is passed, it is cloned and applied to all series. If a\n",
    "        dict, a different transformer can be used for each series. Transformation is\n",
    "        applied to each `series` before training the forecaster.\n",
    "        ColumnTransformers are not allowed since they do not have inverse_transform method.\n",
    "    \n",
    "    transformer_exog : transformer, default `None`\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API. The transformation is applied to `exog` before training the\n",
    "        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n",
    "\n",
    "    weight_func : callable, dict, default `None`\n",
    "        Function that defines the individual weights for each sample based on the\n",
    "        index. For example, a function that assigns a lower weight to certain dates.\n",
    "        If dict {'series_column_name' : callable} a different function can be\n",
    "        used for each series, a weight of 1 is given to all series not present\n",
    "        in `weight_func`. Ignored if `regressor` does not have the argument \n",
    "        `sample_weight` in its `fit` method. See Notes section for more details \n",
    "        on the use of the weights. \n",
    "\n",
    "    series_weights : dict, default `None`\n",
    "        Weights associated with each series {'series_column_name' : float}. It is only\n",
    "        applied if the `regressor` used accepts `sample_weight` in its `fit` method.\n",
    "        If `series_weights` is provided, a weight of 1 is given to all series not present\n",
    "        in `series_weights`. If `None`, all levels have the same weight. See Notes section\n",
    "        for more details on the use of the weights.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    regressor : regressor or pipeline compatible with the scikit-learn API\n",
    "        An instance of a regressor or pipeline compatible with the scikit-learn API.\n",
    "        \n",
    "    create_predictors : Callable\n",
    "        Function that takes a numpy ndarray as a window of values as input and  \n",
    "        returns a numpy ndarray with the predictors associated with that window.\n",
    "\n",
    "    source_code_create_predictors : str\n",
    "        Source code of the custom function used to create the predictors.\n",
    "        \n",
    "    window_size : int\n",
    "        Size of the window needed by `fun_predictors` to create the predictors.\n",
    "\n",
    "    transformer_series : transformer (preprocessor) or dict of transformers, default `None`\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n",
    "        If a single transformer is passed, it is cloned and applied to all series. If a\n",
    "        dict, a different transformer can be used for each series. Transformation is\n",
    "        applied to each `series` before training the forecaster.\n",
    "        ColumnTransformers are not allowed since they do not have inverse_transform method.\n",
    "        \n",
    "    transformer_series_ : dict\n",
    "        Dictionary with the transformer for each series. It is created cloning the objects\n",
    "        in `transformer_series` and is used internally to avoid overwriting.\n",
    "\n",
    "    transformer_exog : transformer (preprocessor), default `None`\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API. The transformation is applied to `exog` before training the\n",
    "        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n",
    "\n",
    "    weight_func : callable, dict, default `None`\n",
    "        Function that defines the individual weights of each sample based on the\n",
    "        index. For example, a function that assigns a lower weight to certain dates.\n",
    "        If dict {'series_column_name': callable} a different function can be\n",
    "        used for each series, a weight of 1 is given to all series not present\n",
    "        in `weight_func`. Ignored if `regressor` does not have the argument \n",
    "        `sample_weight` in its `fit` method. See Notes section for more details \n",
    "        on the use of the weights.\n",
    "\n",
    "    weight_func_ : dict\n",
    "        Dictionary with the `weight_func` for each series. It is created cloning the objects\n",
    "        in `weight_func` and is used internally to avoid overwriting.\n",
    "\n",
    "    source_code_weight_func : str, dict\n",
    "        Source code of the custom function(s) used to create weights.\n",
    "\n",
    "    series_weights : dict, default `None`\n",
    "        Weights associated with each series {'series_column_name': float}. It is only\n",
    "        applied if the `regressor` used accepts `sample_weight` in its `fit` method.\n",
    "        If `series_weights` is provided, a weight of 1 is given to all series not present\n",
    "        in `series_weights`. If `None`, all levels have the same weight. See Notes section\n",
    "        for more details on the use of the weights.\n",
    "\n",
    "    series_weights_ : dict\n",
    "        Weights associated with each series.It is created as a clone of `series_weights`\n",
    "        and is used internally to avoid overwriting.\n",
    "        \n",
    "    window_size : int\n",
    "        Size of the window needed by `fun_predictors` to create the predictors.\n",
    "\n",
    "    last_window : pandas Series\n",
    "        Last window the forecaster has seen during training. It stores the\n",
    "        values needed to predict the next `step` right after the training data.\n",
    "        \n",
    "    index_type : type\n",
    "        Type of index of the input used in training.\n",
    "        \n",
    "    index_freq : str\n",
    "        Frequency of Index of the input used in training.\n",
    "\n",
    "    index_values : pandas Index\n",
    "        Values of Index of the input used in training.\n",
    "\n",
    "    training_range: pandas Index\n",
    "        First and last values of index of the data used during training.\n",
    "        \n",
    "    included_exog : bool\n",
    "        If the forecaster has been trained using exogenous variable/s.\n",
    "        \n",
    "    exog_type : type\n",
    "        Type of exogenous variable/s used in training.\n",
    "        \n",
    "    exog_col_names : list\n",
    "        Names of columns of `exog` if `exog` used in training was a pandas\n",
    "        DataFrame.\n",
    "\n",
    "    series_col_names : list\n",
    "        Names of the series (levels) used during training.\n",
    "\n",
    "    X_train_col_names : list\n",
    "        Names of columns of the matrix created internally for training.\n",
    "        \n",
    "    in_sample_residuals : dict\n",
    "        Residuals of the model when predicting training data. Only stored up to\n",
    "        1000 values in the form `{level: residuals}`.\n",
    "        \n",
    "    out_sample_residuals : dict\n",
    "        Residuals of the model when predicting non-training data. Only stored\n",
    "        up to 1000 values in the form `{level: residuals}`. Use \n",
    "        `set_out_sample_residuals` to set values.\n",
    "        \n",
    "    fitted : Bool\n",
    "        Tag to identify if the regressor has been fitted (trained).\n",
    "\n",
    "    creation_date : str\n",
    "        Date of creation.\n",
    "\n",
    "    fit_date : str\n",
    "        Date of last fit.\n",
    "\n",
    "    skforcast_version : str\n",
    "        Version of skforecast library used to create the forecaster.\n",
    "\n",
    "    python_version : str\n",
    "        Version of python used to create the forecaster.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    The weights are used to control the influence that each observation has on the\n",
    "    training of the model. `ForecasterAutoregMultiseries` accepts two types of weights:\n",
    "\n",
    "    + series_weights : controls the relative importance of each series. If a series has\n",
    "    twice as much weight as the others, the observations of that series influence the\n",
    "    training twice as much. The higher the weight of a series relative to the others,\n",
    "    the more the model will focus on trying to learn that series.\n",
    "\n",
    "    + weight_func : controls the relative importance of each observation according to its\n",
    "    index value. For example, a function that assigns a lower weight to certain dates.\n",
    "\n",
    "    If the two types of weights are indicated, they are multiplied to create the final\n",
    "    weights. The resulting `sample_weight` cannot have negative values.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        regressor: object,\n",
    "        fun_predictors: callable, \n",
    "        window_size: int,\n",
    "        transformer_series: Optional[Union[object, dict]]=None,\n",
    "        transformer_exog: Optional[object]=None,\n",
    "        weight_func: Optional[Union[callable, dict]]=None,\n",
    "        series_weights: Optional[dict]=None\n",
    "    ) -> None:\n",
    "        \n",
    "        self.regressor                     = regressor\n",
    "        self.create_predictors             = fun_predictors\n",
    "        self.source_code_create_predictors = None\n",
    "        self.window_size                   = window_size\n",
    "        self.transformer_series            = transformer_series\n",
    "        self.transformer_series_           = None\n",
    "        self.transformer_exog              = transformer_exog\n",
    "        self.weight_func                   = weight_func\n",
    "        self.weight_func_                  = None\n",
    "        self.source_code_weight_func       = None\n",
    "        self.series_weights                = series_weights\n",
    "        self.series_weights_               = None\n",
    "        self.index_type                    = None\n",
    "        self.index_freq                    = None\n",
    "        self.index_values                  = None\n",
    "        self.training_range                = None\n",
    "        self.last_window                   = None\n",
    "        self.included_exog                 = False\n",
    "        self.exog_type                     = None\n",
    "        self.exog_col_names                = None\n",
    "        self.series_col_names              = None\n",
    "        self.X_train_col_names             = None\n",
    "        self.in_sample_residuals           = None\n",
    "        self.out_sample_residuals          = None\n",
    "        self.fitted                        = False\n",
    "        self.creation_date                 = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        self.fit_date                      = None\n",
    "        self.skforcast_version             = skforecast.__version__\n",
    "        self.python_version                = sys.version.split(\" \")[0]\n",
    "\n",
    "        if not isinstance(window_size, int):\n",
    "            raise TypeError(\n",
    "                f'Argument `window_size` must be an int. Got {type(window_size)}.'\n",
    "            )\n",
    "\n",
    "        if not callable(fun_predictors):\n",
    "            raise TypeError(\n",
    "                f'Argument `fun_predictors` must be a callable. Got {type(fun_predictors)}.'\n",
    "            )\n",
    "    \n",
    "        self.source_code_create_predictors = inspect.getsource(fun_predictors)\n",
    "\n",
    "        self.weight_func, self.source_code_weight_func, self.series_weights = initialize_weights(\n",
    "            forecaster_type = type(self).__name__, \n",
    "            regressor       = regressor, \n",
    "            weight_func     = weight_func, \n",
    "            series_weights  = series_weights\n",
    "        )\n",
    "\n",
    "\n",
    "    def __repr__(\n",
    "        self\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Information displayed when a ForecasterAutoregMultiSeriesCustom object is printed.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n",
    "            name_pipe_steps = tuple(name + \"__\" for name in self.regressor.named_steps.keys())\n",
    "            params = {key : value for key, value in self.regressor.get_params().items() \\\n",
    "                      if key.startswith(name_pipe_steps)}\n",
    "        else:\n",
    "            params = self.regressor.get_params()\n",
    "\n",
    "        info = (\n",
    "            f\"{'=' * len(type(self).__name__)} \\n\"\n",
    "            f\"{type(self).__name__} \\n\"\n",
    "            f\"{'=' * len(type(self).__name__)} \\n\"\n",
    "            f\"Regressor: {self.regressor} \\n\"\n",
    "            f\"Predictors created with function: {self.create_predictors.__name__} \\n\"\n",
    "            f\"Transformer for series: {self.transformer_series} \\n\"\n",
    "            f\"Transformer for exog: {self.transformer_exog} \\n\"\n",
    "            f\"Window size: {self.window_size} \\n\"\n",
    "            f\"Series levels (names): {self.series_col_names} \\n\"\n",
    "            f\"Series weights: {self.series_weights} \\n\"\n",
    "            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n",
    "            f\"Exogenous included: {self.included_exog} \\n\"\n",
    "            f\"Type of exogenous variable: {self.exog_type} \\n\"\n",
    "            f\"Exogenous variables names: {self.exog_col_names} \\n\"\n",
    "            f\"Training range: {self.training_range.to_list() if self.fitted else None} \\n\"\n",
    "            f\"Training index type: {str(self.index_type).split('.')[-1][:-2] if self.fitted else None} \\n\"\n",
    "            f\"Training index frequency: {self.index_freq if self.fitted else None} \\n\"\n",
    "            f\"Regressor parameters: {params} \\n\"\n",
    "            f\"Creation date: {self.creation_date} \\n\"\n",
    "            f\"Last fit date: {self.fit_date} \\n\"\n",
    "            f\"Skforecast version: {self.skforcast_version} \\n\"\n",
    "            f\"Python version: {self.python_version} \\n\"\n",
    "        )\n",
    "\n",
    "        return info\n",
    "\n",
    "\n",
    "    def create_train_X_y(\n",
    "        self,\n",
    "        series: pd.DataFrame,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series, pd.Index, pd.Index]:\n",
    "        \"\"\"\n",
    "        Create training matrices from multiple time series and exogenous\n",
    "        variables.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        series : pandas DataFrame\n",
    "            Training time series.\n",
    "            \n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s. Must have the same\n",
    "            number of observations as `series` and their indexes must be aligned.\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        X_train : pandas DataFrame\n",
    "            Pandas DataFrame with the training values (predictors).\n",
    "            \n",
    "        y_train : pandas Series, shape (len(series) - self.max_lag, )\n",
    "            Values (target) of the time series related to each row of `X_train`.\n",
    "\n",
    "        y_index : pandas Index\n",
    "            Index of `series`.\n",
    "\n",
    "        y_train_index: pandas Index\n",
    "            Index of `y_train`.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(series, pd.DataFrame):\n",
    "            raise TypeError(f'`series` must be a pandas DataFrame. Got {type(series)}.')\n",
    "\n",
    "        series_col_names = list(series.columns)\n",
    "\n",
    "        if self.transformer_series is None:\n",
    "            self.transformer_series_ = {serie: None for serie in series_col_names}\n",
    "        elif not isinstance(self.transformer_series, dict):\n",
    "            self.transformer_series_ = {serie: clone(self.transformer_series) \n",
    "                                        for serie in series_col_names}\n",
    "        else:\n",
    "            self.transformer_series_ = {serie: None for serie in series_col_names}\n",
    "            # Only elements already present in transformer_series_ are updated\n",
    "            self.transformer_series_.update(\n",
    "                (k, v) for k, v in deepcopy(self.transformer_series).items() if k in self.transformer_series_\n",
    "            )\n",
    "            series_not_in_transformer_series = set(series.columns) - set(self.transformer_series.keys())\n",
    "            if series_not_in_transformer_series:\n",
    "                    warnings.warn(\n",
    "                        f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n",
    "                        f\" No transformation is applied to these series.\"\n",
    "                    )\n",
    "        \n",
    "        X_levels = []\n",
    "        for i, serie in enumerate(series.columns):\n",
    "\n",
    "            y = series[serie]\n",
    "            check_y(y=y)\n",
    "            y = transform_series(\n",
    "                    series            = y,\n",
    "                    transformer       = self.transformer_series_[serie],\n",
    "                    fit               = True,\n",
    "                    inverse_transform = False\n",
    "                )\n",
    "\n",
    "            y_values, y_index = preprocess_y(y=y)\n",
    "\n",
    "            temp_X_train  = []\n",
    "            temp_y_train  = []\n",
    "\n",
    "            for j in range(len(y) - self.window_size):\n",
    "\n",
    "                train_index = np.arange(j, self.window_size + j)\n",
    "                test_index  = self.window_size + j\n",
    "\n",
    "                temp_X_train.append(self.create_predictors(y=y_values[train_index]))\n",
    "                temp_y_train.append(y_values[test_index])\n",
    "\n",
    "            X_train_values = np.vstack(temp_X_train)\n",
    "            y_train_values = np.array(temp_y_train)\n",
    "\n",
    "            if i == 0:\n",
    "                X_train = X_train_values\n",
    "                y_train = y_train_values\n",
    "            else:\n",
    "                X_train = np.vstack((X_train, X_train_values))\n",
    "                y_train = np.append(y_train, y_train_values)\n",
    "\n",
    "            X_level = [serie]*len(X_train_values)\n",
    "            X_levels.extend(X_level)\n",
    "        X_train_col_names = [f\"custom_predictor_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "        if exog is not None:\n",
    "            if len(exog) != len(series):\n",
    "                raise ValueError(\n",
    "                    f'`exog` must have same number of samples as `series`. '\n",
    "                    f'length `exog`: ({len(exog)}), length `series`: ({len(series)})'\n",
    "                )\n",
    "            check_exog(exog=exog)\n",
    "            if isinstance(exog, pd.Series):\n",
    "                exog = transform_series(\n",
    "                            series            = exog,\n",
    "                            transformer       = self.transformer_exog,\n",
    "                            fit               = True,\n",
    "                            inverse_transform = False\n",
    "                       )\n",
    "            else:\n",
    "                exog = transform_dataframe(\n",
    "                            df                = exog,\n",
    "                            transformer       = self.transformer_exog,\n",
    "                            fit               = True,\n",
    "                            inverse_transform = False\n",
    "                       )\n",
    "            exog_values, exog_index = preprocess_exog(exog=exog)\n",
    "            if not (exog_index[:len(y_index)] == y_index).all():\n",
    "                raise ValueError(\n",
    "                    ('Different index for `series` and `exog`. They must be equal '\n",
    "                     'to ensure the correct alignment of values.')      \n",
    "                )\n",
    "            col_names_exog = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n",
    "            X_train_col_names.extend(col_names_exog)\n",
    "\n",
    "            # The first `self.window_size` positions have to be removed from exog\n",
    "            # since they are not in X_train. Then exog is cloned as many times\n",
    "            # as series.\n",
    "            if exog_values.ndim == 1:\n",
    "                X_train = np.column_stack((\n",
    "                              X_train,\n",
    "                              np.tile(exog_values[self.window_size:, ], series.shape[1])\n",
    "                          )) \n",
    "\n",
    "            else:\n",
    "                X_train = np.column_stack((\n",
    "                              X_train,\n",
    "                              np.tile(exog_values[self.window_size:, ], [series.shape[1], 1])\n",
    "                          ))\n",
    "\n",
    "        X_levels = pd.Series(X_levels)\n",
    "        X_levels = pd.get_dummies(X_levels, dtype=float)\n",
    "        X_train_col_names.extend(X_levels.columns)\n",
    "        X_train = np.column_stack((X_train, X_levels.values))\n",
    "\n",
    "        X_train = pd.DataFrame(\n",
    "                      data    = X_train,\n",
    "                      columns = X_train_col_names\n",
    "                  )\n",
    "\n",
    "        y_train = pd.Series(\n",
    "                      data = y_train,\n",
    "                      name = 'y'\n",
    "                  )\n",
    "\n",
    "        y_train_index = pd.Index(\n",
    "                            np.tile(\n",
    "                                y_index[self.window_size: ].values,\n",
    "                                reps = len(series_col_names)\n",
    "                            )\n",
    "                        )\n",
    "        \n",
    "        self.X_train_col_names = X_train_col_names\n",
    "\n",
    "        return X_train, y_train, y_index, y_train_index\n",
    "\n",
    "    \n",
    "    def create_sample_weights(\n",
    "        self,\n",
    "        series: pd.DataFrame,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train_index: pd.Index,\n",
    "    )-> np.ndarray:\n",
    "        \"\"\"\n",
    "        Crate weights for each observation according to the forecaster's attributes\n",
    "        `series_weights` and `weight_func`. The resulting weights are product of both\n",
    "        types of weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        series : pandas DataFrame\n",
    "            Time series used to create `X_train` with the method `create_train_X_y`.\n",
    "        X_train : pandas DataFrame\n",
    "            Dataframe generated with the method `create_train_X_y`, first return.\n",
    "        y_train_index : pandas Index\n",
    "            Index of `y_train` generated with the method `create_train_X_y`, fourth return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        weights : numpy ndarray\n",
    "            Weights to use in `fit` method.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        weights = None\n",
    "        weights_samples = None\n",
    "        weights_series = None\n",
    "\n",
    "        if self.series_weights is not None:\n",
    "            # Series not present in series_weights have a weight of 1 in all their samples.\n",
    "            # Keys in series_weights not present in series are ignored.\n",
    "            series_not_in_series_weights = set(series.columns) - set(self.series_weights.keys())\n",
    "            if series_not_in_series_weights:\n",
    "                warnings.warn(\n",
    "                    f\"{series_not_in_series_weights} not present in `series_weights`.\"\n",
    "                    f\" A weight of 1 is given to all their samples.\"\n",
    "                )\n",
    "            self.series_weights_ = dict.fromkeys(series.columns, 1.)\n",
    "            self.series_weights_.update((k, v) for k, v in self.series_weights.items() if k in self.series_weights_)\n",
    "            weights_series = [np.repeat(self.series_weights_[serie], sum(X_train[serie])) \n",
    "                              for serie in series.columns]\n",
    "            weights_series = np.concatenate(weights_series)\n",
    "\n",
    "        if self.weight_func is not None:\n",
    "            if isinstance(self.weight_func, Callable):\n",
    "                self.weight_func_ = dict.fromkeys(series.columns, self.weight_func)\n",
    "            else:\n",
    "                # Series not present in weight_func have a weight of 1 in all their samples\n",
    "                series_not_in_weight_func = set(series.columns) - set(self.weight_func.keys())\n",
    "                if series_not_in_weight_func:\n",
    "                    warnings.warn(\n",
    "                        f\"{series_not_in_weight_func} not present in `weight_func`.\"\n",
    "                        f\" A weight of 1 is given to all their samples.\"\n",
    "                    )\n",
    "                self.weight_func_ = dict.fromkeys(series.columns, lambda index: np.ones_like(index, dtype=float))\n",
    "                self.weight_func_.update((k, v) for k, v in self.weight_func.items() if k in self.weight_func_)\n",
    "                \n",
    "            weights_samples = []\n",
    "            for key in self.weight_func_.keys():\n",
    "                index = y_train_index[X_train[X_train[key] == 1.0].index]\n",
    "                weights_samples.append(self.weight_func_[key](index))\n",
    "            weights_samples = np.concatenate(weights_samples)\n",
    "\n",
    "        if weights_series is not None:\n",
    "            weights = weights_series\n",
    "            if weights_samples is not None:\n",
    "                weights = weights * weights_samples\n",
    "        else:\n",
    "            if weights_samples is not None:\n",
    "                weights = weights_samples\n",
    "\n",
    "        if weights is not None:\n",
    "            if np.isnan(weights).any():\n",
    "                raise ValueError(\n",
    "                    \"The resulting `weights` cannot have NaN values.\"\n",
    "                )\n",
    "            if np.any(weights < 0):\n",
    "                raise ValueError(\n",
    "                    \"The resulting `weights` cannot have negative values.\"\n",
    "                )\n",
    "            if np.sum(weights) == 0:\n",
    "                raise ValueError(\n",
    "                    (\"The resulting `weights` cannot be normalized because \"\n",
    "                     \"the sum of the weights is zero.\")\n",
    "                )\n",
    "\n",
    "        return weights\n",
    "\n",
    "        \n",
    "    def fit(\n",
    "        self,\n",
    "        series: pd.DataFrame,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        store_in_sample_residuals: bool=True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Training Forecaster.\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        series : pandas DataFrame\n",
    "            Training time series.\n",
    "            \n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s. Must have the same\n",
    "            number of observations as `series` and their indexes must be aligned so\n",
    "            that series[i] is regressed on exog[i].\n",
    "\n",
    "        store_in_sample_residuals : bool, default `True`\n",
    "            if True, in_sample_residuals are stored.\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Reset values in case the forecaster has already been fitted.\n",
    "        self.index_type          = None\n",
    "        self.index_freq          = None\n",
    "        self.index_values        = None\n",
    "        self.last_window         = None\n",
    "        self.included_exog       = False\n",
    "        self.exog_type           = None\n",
    "        self.exog_col_names      = None\n",
    "        self.series_col_names    = None\n",
    "        self.X_train_col_names   = None\n",
    "        self.in_sample_residuals = None\n",
    "        self.fitted              = False\n",
    "        self.training_range      = None\n",
    "        \n",
    "        self.series_col_names = list(series.columns)\n",
    "\n",
    "        if exog is not None:\n",
    "            self.included_exog = True\n",
    "            self.exog_type = type(exog)\n",
    "            self.exog_col_names = \\\n",
    "                 exog.columns.to_list() if isinstance(exog, pd.DataFrame) else [exog.name]\n",
    "\n",
    "            if len(set(self.exog_col_names) - set(self.series_col_names)) != len(self.exog_col_names):\n",
    "                raise ValueError(\n",
    "                    (f'`exog` cannot contain a column named the same as one of the series'\n",
    "                     f' (column names of series).\\n'\n",
    "                     f'    `series` columns : {self.series_col_names}.\\n'\n",
    "                     f'    `exog`   columns : {self.exog_col_names}.')\n",
    "                )\n",
    "\n",
    "        X_train, y_train, y_index, y_train_index = self.create_train_X_y(series=series, exog=exog)\n",
    "        sample_weight = self.create_sample_weights(\n",
    "                            series        = series,\n",
    "                            X_train       = X_train,\n",
    "                            y_train_index = y_train_index,\n",
    "                        )\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight)\n",
    "        else:\n",
    "            self.regressor.fit(X=X_train, y=y_train)\n",
    "            \n",
    "        self.fitted = True\n",
    "        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        self.training_range = y_index[[0, -1]]\n",
    "        self.index_type = type(y_index)\n",
    "        if isinstance(y_index, pd.DatetimeIndex):\n",
    "            self.index_freq = y_index.freqstr\n",
    "        else: \n",
    "            self.index_freq = y_index.step\n",
    "        self.index_values = y_index\n",
    "\n",
    "        in_sample_residuals = {}\n",
    "        \n",
    "        # This is done to save time during fit in functions such as backtesting()\n",
    "        if store_in_sample_residuals:\n",
    "\n",
    "            residuals = y_train - self.regressor.predict(X_train)\n",
    "\n",
    "            for serie in series.columns:\n",
    "                in_sample_residuals[serie] = residuals.values[X_train[serie] == 1.]\n",
    "                if len(in_sample_residuals[serie]) > 1000:\n",
    "                    # Only up to 1000 residuals are stored\n",
    "                    rng = np.random.default_rng(seed=123)\n",
    "                    in_sample_residuals[serie] = rng.choice(\n",
    "                                                     a       = in_sample_residuals[serie], \n",
    "                                                     size    = 1000, \n",
    "                                                     replace = False\n",
    "                                                 )\n",
    "        else:\n",
    "            for serie in series.columns:\n",
    "                in_sample_residuals[serie] = np.array([None])\n",
    "\n",
    "        self.in_sample_residuals = in_sample_residuals\n",
    "\n",
    "        # The last time window of training data is stored so that predictors in\n",
    "        # the first iteration of `predict()` can be calculated.\n",
    "        self.last_window = series.iloc[-self.window_size:].copy()\n",
    "\n",
    "\n",
    "    def _recursive_predict(\n",
    "        self,\n",
    "        steps: int,\n",
    "        level: str,\n",
    "        last_window: np.ndarray,\n",
    "        exog: Optional[np.ndarray]=None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict n steps ahead. It is an iterative process in which, each prediction,\n",
    "        is used as a predictor for the next step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int\n",
    "            Number of future steps predicted.\n",
    "            \n",
    "        level : str\n",
    "            Time series to be predicted.\n",
    "        \n",
    "        last_window : numpy ndarray\n",
    "            Values of the series used to create the predictors (lags) need in the \n",
    "            first iteration of prediction (t + 1).\n",
    "            \n",
    "        exog : numpy ndarray, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        predictions : numpy ndarray\n",
    "            Predicted values.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = np.full(shape=steps, fill_value=np.nan)\n",
    "\n",
    "        for i in range(steps):\n",
    "            X = last_window[-self.lags].reshape(1, -1)\n",
    "            if exog is not None:\n",
    "                X = np.column_stack((X, exog[i, ].reshape(1, -1)))\n",
    "            \n",
    "            levels_dummies = np.zeros(shape=(1, len(self.series_col_names)), dtype=float)\n",
    "            levels_dummies[0][self.series_col_names.index(level)] = 1.\n",
    "\n",
    "            X = np.column_stack((X, levels_dummies.reshape(1, -1)))\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                # Suppress scikit-learn warning: \"X does not have valid feature names,\n",
    "                # but NoOpTransformer was fitted with feature names\".\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                prediction = self.regressor.predict(X)\n",
    "                predictions[i] = prediction.ravel()[0]\n",
    "\n",
    "            # Update `last_window` values. The first position is discarded and \n",
    "            # the new prediction is added at the end.\n",
    "            last_window = np.append(last_window[1:], prediction)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "            \n",
    "    def predict(\n",
    "        self,\n",
    "        steps: int,\n",
    "        levels: Optional[Union[str, list]]=None,\n",
    "        last_window: Optional[pd.DataFrame]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict n steps ahead. It is an recursive process in which, each prediction,\n",
    "        is used as a predictor for the next step.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int\n",
    "            Number of future steps predicted.\n",
    "\n",
    "        levels : str, list, default `None`\n",
    "            Time series to be predicted. If `None` all levels will be predicted.\n",
    "            \n",
    "\n",
    "        last_window : pandas DataFrame, default `None`\n",
    "            Values of the series used to create the predictors (lags) need in the\n",
    "            first iteration of prediction (t + 1).\n",
    "\n",
    "            If `last_window = None`, the values stored in `self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "\n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : pandas DataFrame\n",
    "            Predicted values, one column for each level.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if levels is None:\n",
    "            levels = self.series_col_names\n",
    "        elif isinstance(levels, str):\n",
    "            levels = [levels]\n",
    "\n",
    "        if last_window is None:\n",
    "            last_window = deepcopy(self.last_window)\n",
    "        \n",
    "        check_predict_input(\n",
    "            forecaster_type  = type(self).__name__,\n",
    "            steps            = steps,\n",
    "            fitted           = self.fitted,\n",
    "            included_exog    = self.included_exog,\n",
    "            index_type       = self.index_type,\n",
    "            index_freq       = self.index_freq,\n",
    "            window_size      = self.window_size,\n",
    "            last_window      = last_window,\n",
    "            exog             = exog,\n",
    "            exog_type        = self.exog_type,\n",
    "            exog_col_names   = self.exog_col_names,\n",
    "            interval         = None,\n",
    "            max_steps        = None,\n",
    "            levels           = levels,\n",
    "            series_col_names = self.series_col_names\n",
    "        )\n",
    "        \n",
    "        if exog is not None:\n",
    "            if isinstance(exog, pd.DataFrame):\n",
    "                exog = transform_dataframe(\n",
    "                           df                = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = False,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "            else:\n",
    "                exog = transform_series(\n",
    "                           series            = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = False,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "            \n",
    "            exog_values, _ = preprocess_exog(\n",
    "                                 exog = exog.iloc[:steps, ]\n",
    "                             )\n",
    "        else:\n",
    "            exog_values = None\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for level in levels:\n",
    "\n",
    "            last_window_level = transform_series(\n",
    "                                    series            = last_window[level],\n",
    "                                    transformer       = self.transformer_series_[level],\n",
    "                                    fit               = False,\n",
    "                                    inverse_transform = False\n",
    "                                )\n",
    "            last_window_values, last_window_index = preprocess_last_window(\n",
    "                                                        last_window = last_window_level\n",
    "                                                    )\n",
    "                \n",
    "            preds_level = self._recursive_predict(\n",
    "                              steps       = steps,\n",
    "                              level       = level,\n",
    "                              last_window = copy(last_window_values),\n",
    "                              exog        = copy(exog_values)\n",
    "                          )\n",
    "\n",
    "            preds_level = pd.Series(\n",
    "                              data  = preds_level,\n",
    "                              index = expand_index(\n",
    "                                          index = last_window_index,\n",
    "                                          steps = steps\n",
    "                                      ),\n",
    "                              name = level\n",
    "                          )\n",
    "\n",
    "            preds_level = transform_series(\n",
    "                              series            = preds_level,\n",
    "                              transformer       = self.transformer_series_[level],\n",
    "                              fit               = False,\n",
    "                              inverse_transform = True\n",
    "                          )\n",
    "\n",
    "            predictions.append(preds_level)    \n",
    "\n",
    "        predictions = pd.concat(predictions, axis=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def predict_bootstrapping(\n",
    "        self,\n",
    "        steps: int,\n",
    "        levels: Optional[Union[str, list]]=None,\n",
    "        last_window: Optional[pd.DataFrame]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        n_boot: int=500,\n",
    "        random_state: int=123,\n",
    "        in_sample_residuals: bool=True\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Generate multiple forecasting predictions using a bootstrapping process. \n",
    "        By sampling from a collection of past observed errors (the residuals),\n",
    "        each iteration of bootstrapping generates a different set of predictions. \n",
    "        See the Notes section for more information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------   \n",
    "        steps : int\n",
    "            Number of future steps predicted.\n",
    "\n",
    "        levels : str, list, default `None`\n",
    "            Time series to be predicted. If `None` all levels will be predicted.\n",
    "            \n",
    "        last_window : pandas DataFrame, default `None`\n",
    "            Values of the series used to create the predictors (lags) need in the \n",
    "            first iteration of prediction (t + 1).\n",
    "    \n",
    "            If `last_window = None`, the values stored in `self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "            \n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "            \n",
    "        n_boot : int, default `500`\n",
    "            Number of bootstrapping iterations used to estimate prediction\n",
    "            intervals.\n",
    "\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random generator, so that boot intervals are always \n",
    "            deterministic.\n",
    "                        \n",
    "        in_sample_residuals : bool, default `True`\n",
    "            If `True`, residuals from the training data are used as proxy of\n",
    "            prediction error to create prediction intervals. If `False`, out of\n",
    "            sample residuals are used. In the latter case, the user should have\n",
    "            calculated and stored the residuals within the forecaster (see\n",
    "            `set_out_sample_residuals()`).\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        boot_predictions : dict\n",
    "            Predictions generated by bootstrapping for each level. \n",
    "            {level: pandas DataFrame, shape (steps, n_boot)}\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        More information about prediction intervals in forecasting:\n",
    "        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n",
    "        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if levels is None:\n",
    "            levels = self.series_col_names\n",
    "        elif isinstance(levels, str):\n",
    "            levels = [levels]\n",
    "\n",
    "        for level in levels:\n",
    "            if in_sample_residuals and (self.in_sample_residuals[level] == None).any():\n",
    "                raise ValueError(\n",
    "                    (f\"`forecaster.in_sample_residuals['{level}']` contains `None` values. \"\n",
    "                      \"Try using `fit` method with `in_sample_residuals=True` or set in \"\n",
    "                      \"`predict_interval()`, `predict_bootstrapping()` or \"\n",
    "                      \"`predict_dist()` method `in_sample_residuals=False` and use \"\n",
    "                      \"`out_sample_residuals` (see `set_out_sample_residuals()`).\")\n",
    "                )\n",
    "\n",
    "        if not in_sample_residuals and self.out_sample_residuals is None:\n",
    "            raise ValueError(\n",
    "                ('`forecaster.out_sample_residuals` is `None`. Use '\n",
    "                 '`in_sample_residuals=True` or method `set_out_sample_residuals()` '\n",
    "                 'before `predict_interval()`, `predict_bootstrapping()` or '\n",
    "                 '`predict_dist()`.')\n",
    "            )\n",
    "\n",
    "        if not in_sample_residuals and len(set(levels) - set(self.out_sample_residuals.keys())) != 0:\n",
    "            raise ValueError(\n",
    "                (f'Not `forecaster.out_sample_residuals` for levels: {set(levels) - set(self.out_sample_residuals.keys())}. '\n",
    "                 f'Use method `set_out_sample_residuals()`.')\n",
    "            )\n",
    "\n",
    "        if last_window is None:\n",
    "            last_window = deepcopy(self.last_window)\n",
    "\n",
    "        check_predict_input(\n",
    "            forecaster_type  = type(self).__name__,\n",
    "            steps            = steps,\n",
    "            fitted           = self.fitted,\n",
    "            included_exog    = self.included_exog,\n",
    "            index_type       = self.index_type,\n",
    "            index_freq       = self.index_freq,\n",
    "            window_size      = self.window_size,\n",
    "            last_window      = last_window,\n",
    "            last_window_exog = None,\n",
    "            exog             = exog,\n",
    "            exog_type        = self.exog_type,\n",
    "            exog_col_names   = self.exog_col_names,\n",
    "            interval         = None,\n",
    "            alpha            = None,\n",
    "            max_steps        = None,\n",
    "            levels           = levels,\n",
    "            series_col_names = self.series_col_names\n",
    "        )\n",
    "\n",
    "        if exog is not None:\n",
    "            if isinstance(exog, pd.DataFrame):\n",
    "                exog = transform_dataframe(\n",
    "                           df                = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = False,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "            else:\n",
    "                exog = transform_series(\n",
    "                           series            = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = False,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "            \n",
    "            exog_values, _ = preprocess_exog(\n",
    "                                 exog = exog.iloc[:steps, ]\n",
    "                             )\n",
    "        else:\n",
    "            exog_values = None\n",
    "        \n",
    "        boot_predictions = {}\n",
    "\n",
    "        for level in levels:\n",
    "        \n",
    "            last_window_level = transform_series(\n",
    "                                    series            = last_window[level],\n",
    "                                    transformer       = self.transformer_series_[level],\n",
    "                                    fit               = False,\n",
    "                                    inverse_transform = False\n",
    "                                )\n",
    "            last_window_values, last_window_index = preprocess_last_window(\n",
    "                                                        last_window = last_window_level\n",
    "                                                    )\n",
    "\n",
    "            level_boot_predictions = np.full(\n",
    "                                         shape      = (steps, n_boot),\n",
    "                                         fill_value = np.nan,\n",
    "                                         dtype      = float\n",
    "                                     )\n",
    "            rng = np.random.default_rng(seed=random_state)\n",
    "            seeds = rng.integers(low=0, high=10000, size=n_boot)\n",
    "\n",
    "            if in_sample_residuals:\n",
    "                residuals = self.in_sample_residuals[level]\n",
    "            else:\n",
    "                residuals = self.out_sample_residuals[level]\n",
    "\n",
    "            for i in range(n_boot):\n",
    "                # In each bootstraping iteration the initial last_window and exog \n",
    "                # need to be restored.\n",
    "                last_window_boot = last_window_values.copy()\n",
    "                exog_boot = exog_values.copy() if exog is not None else None\n",
    "\n",
    "                rng = np.random.default_rng(seed=seeds[i])\n",
    "                sample_residuals = rng.choice(\n",
    "                                       a       = residuals,\n",
    "                                       size    = steps,\n",
    "                                       replace = True\n",
    "                                   )\n",
    "\n",
    "                for step in range(steps):\n",
    "\n",
    "                    prediction = self._recursive_predict(\n",
    "                                     steps       = 1,\n",
    "                                     level       = level,\n",
    "                                     last_window = last_window_boot,\n",
    "                                     exog        = exog_boot \n",
    "                                 )\n",
    "                    \n",
    "                    prediction_with_residual = prediction + sample_residuals[step]\n",
    "                    level_boot_predictions[step, i] = prediction_with_residual\n",
    "\n",
    "                    last_window_boot = np.append(\n",
    "                                           last_window_boot[1:],\n",
    "                                           prediction_with_residual\n",
    "                                       )\n",
    "                    if exog is not None:\n",
    "                        exog_boot = exog_boot[1:]\n",
    "\n",
    "            level_boot_predictions = pd.DataFrame(\n",
    "                                         data    = level_boot_predictions,\n",
    "                                         index   = expand_index(last_window_index, steps=steps),\n",
    "                                         columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n",
    "                                     )\n",
    "\n",
    "            if self.transformer_series_[level]:\n",
    "                for col in level_boot_predictions.columns:\n",
    "                    level_boot_predictions[col] = transform_series(\n",
    "                                                      series            = level_boot_predictions[col],\n",
    "                                                      transformer       = self.transformer_series_[level],\n",
    "                                                      fit               = False,\n",
    "                                                      inverse_transform = True\n",
    "                                                  )\n",
    "            \n",
    "            boot_predictions[level] = level_boot_predictions\n",
    "        \n",
    "        return boot_predictions\n",
    "\n",
    "\n",
    "    def predict_interval(\n",
    "        self,\n",
    "        steps: int,\n",
    "        levels: Optional[Union[str, list]]=None,\n",
    "        last_window: Optional[pd.DataFrame]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        interval: list=[5, 95],\n",
    "        n_boot: int=500,\n",
    "        random_state: int=123,\n",
    "        in_sample_residuals: bool=True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Iterative process in which, each prediction, is used as a predictor\n",
    "        for the next step and bootstrapping is used to estimate prediction\n",
    "        intervals. Both predictions and intervals are returned.\n",
    "        \n",
    "        Parameters\n",
    "        ---------- \n",
    "        steps : int\n",
    "            Number of future steps predicted.\n",
    "\n",
    "        levels : str, list, default `None`\n",
    "            Time series to be predicted. If `None` all levels will be predicted.  \n",
    "              \n",
    "            \n",
    "        last_window : pandas DataFrame, default `None`\n",
    "            Values of the series used to create the predictors (lags) needed in the \n",
    "            first iteration of prediction (t + 1).\n",
    "    \n",
    "            If `last_window = None`, the values stored in` self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "            \n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "            \n",
    "        interval : list, default `[5, 95]`\n",
    "            Confidence of the prediction interval estimated. Sequence of \n",
    "            percentiles to compute, which must be between 0 and 100 inclusive. \n",
    "            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n",
    "            \n",
    "        n_boot : int, default `500`\n",
    "            Number of bootstrapping iterations used to estimate prediction\n",
    "            intervals.\n",
    "\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random generator, so that boot intervals are always \n",
    "            deterministic.\n",
    "            \n",
    "        in_sample_residuals : bool, default `True`\n",
    "            If `True`, residuals from the training data are used as proxy of\n",
    "            prediction error to create prediction intervals. If `False`, out of\n",
    "            sample residuals are used. In the latter case, the user should have\n",
    "            calculated and stored the residuals within the forecaster (see\n",
    "            `set_out_sample_residuals()`).\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        predictions : pandas DataFrame\n",
    "            Values predicted by the forecaster and their estimated interval.\n",
    "                level: predictions.\n",
    "                level_lower_bound: lower bound of the interval.\n",
    "                level_upper_bound: upper bound interval of the interval.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        More information about prediction intervals in forecasting:\n",
    "        https://otexts.com/fpp2/prediction-intervals.html\n",
    "        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n",
    "        George Athanasopoulos.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if levels is None:\n",
    "            levels = self.series_col_names\n",
    "        elif isinstance(levels, str):\n",
    "            levels = [levels]\n",
    "        \n",
    "        check_interval(interval=interval)\n",
    "\n",
    "        preds = self.predict(\n",
    "                    steps       = steps,\n",
    "                    levels      = levels,\n",
    "                    last_window = last_window,\n",
    "                    exog        = exog\n",
    "                )\n",
    "\n",
    "        boot_predictions = self.predict_bootstrapping(\n",
    "                               steps               = steps,\n",
    "                               levels              = levels,\n",
    "                               last_window         = last_window,\n",
    "                               exog                = exog,\n",
    "                               n_boot              = n_boot,\n",
    "                               random_state        = random_state,\n",
    "                               in_sample_residuals = in_sample_residuals\n",
    "                           )\n",
    "\n",
    "        interval = np.array(interval)/100\n",
    "        predictions = []\n",
    "\n",
    "        for level in levels:\n",
    "            preds_interval = boot_predictions[level].quantile(q=interval, axis=1).transpose()\n",
    "            preds_interval.columns = [f'{level}_lower_bound', f'{level}_upper_bound']\n",
    "            predictions.append(preds[level])\n",
    "            predictions.append(preds_interval)\n",
    "        \n",
    "        predictions = pd.concat(predictions, axis=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def predict_dist(\n",
    "        self,\n",
    "        steps: int,\n",
    "        distribution: object,\n",
    "        levels: Optional[Union[str, list]]=None,\n",
    "        last_window: Optional[pd.DataFrame]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        n_boot: int=500,\n",
    "        random_state: int=123,\n",
    "        in_sample_residuals: bool=True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit a given probability distribution for each step. After generating \n",
    "        multiple forecasting predictions through a bootstrapping process, each \n",
    "        step is fitted to the given distribution.\n",
    "        \n",
    "        Parameters\n",
    "        ---------- \n",
    "        steps : int\n",
    "            Number of future steps predicted.\n",
    "\n",
    "        distribution : Object\n",
    "            A distribution object from scipy.stats.\n",
    "\n",
    "        levels : str, list, default `None`\n",
    "            Time series to be predicted. If `None` all levels will be predicted.  \n",
    "              \n",
    "            \n",
    "        last_window : pandas DataFrame, default `None`\n",
    "            Values of the series used to create the predictors (lags) needed in the \n",
    "            first iteration of prediction (t + 1).\n",
    "    \n",
    "            If `last_window = None`, the values stored in` self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "            \n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "            \n",
    "        n_boot : int, default `500`\n",
    "            Number of bootstrapping iterations used to estimate prediction\n",
    "            intervals.\n",
    "\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random generator, so that boot intervals are always \n",
    "            deterministic.\n",
    "            \n",
    "        in_sample_residuals : bool, default `True`\n",
    "            If `True`, residuals from the training data are used as proxy of\n",
    "            prediction error to create prediction intervals. If `False`, out of\n",
    "            sample residuals are used. In the latter case, the user should have\n",
    "            calculated and stored the residuals within the forecaster (see\n",
    "            `set_out_sample_residuals()`).\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        predictions : pandas DataFrame\n",
    "            Distribution parameters estimated for each step and level.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if levels is None:\n",
    "            levels = self.series_col_names\n",
    "        elif isinstance(levels, str):\n",
    "            levels = [levels]\n",
    "\n",
    "        boot_samples = self.predict_bootstrapping(\n",
    "                           steps               = steps,\n",
    "                           levels              = levels,\n",
    "                           last_window         = last_window,\n",
    "                           exog                = exog,\n",
    "                           n_boot              = n_boot,\n",
    "                           random_state        = random_state,\n",
    "                           in_sample_residuals = in_sample_residuals\n",
    "                       )\n",
    "\n",
    "        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p=='x'] + [\"loc\",\"scale\"]\n",
    "        predictions = []\n",
    "\n",
    "        for level in levels:\n",
    "            param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples[level])\n",
    "            level_param_names = [f'{level}_{p}' for p in param_names]\n",
    "\n",
    "            pred_level = pd.DataFrame(\n",
    "                             data    = param_values,\n",
    "                             columns = level_param_names,\n",
    "                             index   = boot_samples[level].index\n",
    "                         )\n",
    "            \n",
    "            predictions.append(pred_level)\n",
    "        \n",
    "        predictions = pd.concat(predictions, axis=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def set_params(\n",
    "        self, \n",
    "        **params: dict\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Set new values to the parameters of the scikit learn model stored in the\n",
    "        forecaster.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dict\n",
    "            Parameters values.\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        self\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.regressor = clone(self.regressor)\n",
    "        self.regressor.set_params(**params)\n",
    "        \n",
    "        \n",
    "    def set_lags(\n",
    "        self, \n",
    "        lags: Union[int, list, np.ndarray, range]\n",
    "    ) -> None:\n",
    "        \"\"\"      \n",
    "        Set new value to the attribute `lags`.\n",
    "        Attributes `max_lag` and `window_size` are also updated.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lags : int, list, 1D np.array, range\n",
    "            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n",
    "                `int`: include lags from 1 to `lags`.\n",
    "                `list` or `np.array`: include only lags present in `lags`.\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.lags = initialize_lags(type(self).__name__, lags)            \n",
    "        self.max_lag  = max(self.lags)\n",
    "        self.window_size = max(self.lags)\n",
    "        \n",
    "        \n",
    "    def set_out_sample_residuals(\n",
    "        self, \n",
    "        residuals: pd.DataFrame,\n",
    "        append: bool=True,\n",
    "        transform: bool=True,\n",
    "        random_state: int=123\n",
    "    )-> None:\n",
    "        \"\"\"\n",
    "        Set new values to the attribute `out_sample_residuals`. Out of sample\n",
    "        residuals are meant to be calculated using observations that did not\n",
    "        participate in the training process.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        residuals : pandas DataFrame\n",
    "            Values of residuals. If len(residuals) > 1000, only a random sample\n",
    "            of 1000 values are stored. Columns must be the same as `levels`.\n",
    "            \n",
    "        append : bool, default `True`\n",
    "            If `True`, new residuals are added to the once already stored in the\n",
    "            attribute `out_sample_residuals`. Once the limit of 1000 values is\n",
    "            reached, no more values are appended. If False, `out_sample_residuals`\n",
    "            is overwritten with the new residuals.\n",
    "\n",
    "        transform : bool, default `True`\n",
    "            If `True`, new residuals are transformed using self.transformer_series.\n",
    "\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random sampling for reproducible output.\n",
    "        \n",
    "        Returns \n",
    "        -------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(residuals, pd.DataFrame):\n",
    "            raise TypeError(\n",
    "                f\"`residuals` argument must be a pandas DataFrame. Got {type(residuals)}.\"\n",
    "            )\n",
    "\n",
    "        if not self.fitted:\n",
    "            raise sklearn.exceptions.NotFittedError(\n",
    "                (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n",
    "                 \"arguments before using `set_out_sample_residuals()`.\")\n",
    "            )\n",
    "        \n",
    "        out_sample_residuals = {}\n",
    "\n",
    "        for level in residuals.columns:\n",
    "\n",
    "            if not level in self.series_col_names:\n",
    "                continue\n",
    "            else:      \n",
    "\n",
    "                residuals_level = residuals[level]\n",
    "\n",
    "                if not transform and self.transformer_series_[level] is not None:\n",
    "                    warnings.warn(\n",
    "                        ('Argument `transform` is set to `False` but forecaster was trained '\n",
    "                         f'using a transformer {self.transformer_series_[level]} for level {level}. '\n",
    "                         'Ensure that the new residuals are already transformed or set `transform=True`.')\n",
    "                    )\n",
    "\n",
    "                if transform and self.transformer_series_ and self.transformer_series_[level]:\n",
    "                    warnings.warn(\n",
    "                        ('Residuals will be transformed using the same transformer used '\n",
    "                         f'when training the forecaster for level {level} : ({self.transformer_series_[level]}). '\n",
    "                         'Ensure that the new residuals are on the same scale as the '\n",
    "                         'original time series. ')\n",
    "                    )\n",
    "\n",
    "                    residuals_level = transform_series(\n",
    "                                          series            = residuals_level,\n",
    "                                          transformer       = self.transformer_series_[level],\n",
    "                                          fit               = False,\n",
    "                                          inverse_transform = False\n",
    "                                      )\n",
    "\n",
    "                if len(residuals_level) > 1000:\n",
    "                    rng = np.random.default_rng(seed=random_state)\n",
    "                    residuals_level = rng.choice(a=residuals_level, size=1000, replace=False)\n",
    "        \n",
    "                if append and self.out_sample_residuals is not None:\n",
    "\n",
    "                    if not level in self.out_sample_residuals.keys():\n",
    "                        raise ValueError(\n",
    "                            f'{level} does not exists in `forecaster.out_sample_residuals` keys: {list(self.out_sample_residuals.keys())}'\n",
    "                        )\n",
    "\n",
    "                    free_space = max(0, 1000 - len(self.out_sample_residuals[level]))\n",
    "                    if len(residuals_level) < free_space:\n",
    "                        residuals_level = np.hstack((\n",
    "                                              self.out_sample_residuals[level],\n",
    "                                              residuals_level\n",
    "                                          ))\n",
    "                    else:\n",
    "                        residuals_level = np.hstack((\n",
    "                                              self.out_sample_residuals[level],\n",
    "                                              residuals_level[:free_space]\n",
    "                                          ))\n",
    "\n",
    "                out_sample_residuals[level] = np.array(residuals_level)\n",
    "\n",
    "        self.out_sample_residuals = out_sample_residuals\n",
    "\n",
    "    \n",
    "    def get_feature_importance(\n",
    "        self\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"      \n",
    "        Return feature importance of the regressor stored in the\n",
    "        forecaster. Only valid when regressor stores internally the feature\n",
    "        importance in the attribute `feature_importances_` or `coef_`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importance : pandas DataFrame\n",
    "            Feature importance associated with each predictor.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if self.fitted == False:\n",
    "            raise sklearn.exceptions.NotFittedError(\n",
    "                (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n",
    "                 \"arguments before using `get_feature_importance()`.\")\n",
    "            )\n",
    "\n",
    "        if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n",
    "            estimator = self.regressor[-1]\n",
    "        else:\n",
    "            estimator = self.regressor\n",
    "\n",
    "        try:\n",
    "            feature_importance = pd.DataFrame({\n",
    "                                    'feature': self.X_train_col_names,\n",
    "                                    'importance' : estimator.feature_importances_\n",
    "                                })\n",
    "        except:   \n",
    "            try:\n",
    "                feature_importance = pd.DataFrame({\n",
    "                                        'feature': self.X_train_col_names,\n",
    "                                        'importance' : estimator.coef_\n",
    "                                    })\n",
    "            except:\n",
    "                warnings.warn(\n",
    "                    f\"Impossible to access feature importance for regressor of type {type(estimator)}. \"\n",
    "                    f\"This method is only valid when the regressor stores internally \"\n",
    "                    f\"the feature importance in the attribute `feature_importances_` \"\n",
    "                    f\"or `coef_`.\"\n",
    "                )\n",
    "\n",
    "                feature_importance = None\n",
    "\n",
    "        return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "# ==============================================================================\n",
    "url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/data/h2o.csv')\n",
    "data = pd.read_csv(url, sep=',', header=0, names=['y', 'date'])\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y/%m/%d')\n",
    "data = data.set_index('date')\n",
    "data = data.asfreq('MS')\n",
    "data = data.y\n",
    "data = pd.concat((data, data*10), axis=1)\n",
    "data.columns = ['series_1', 'series_2']\n",
    "exog_1 = pd.Series(np.arange(len(data)), index=data.index)\n",
    "exog_2 = exog_1 * 10\n",
    "exog = pd.concat((exog_1, exog_2), axis=1)\n",
    "exog.columns = ['exog_1', 'exog_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to create predictors\n",
    "# ==============================================================================\n",
    "def create_predictors(y):\n",
    "    \"\"\"\n",
    "    Create first 3 lags of a time series.\n",
    "    Calculate moving average with window 20.\n",
    "    \"\"\"\n",
    "\n",
    "    lags = y[-1:-3:-1]\n",
    "    mean = np.mean(y[-20:])\n",
    "    predictors = np.hstack([lags, mean])\n",
    "\n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(create_predictors, Callable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoregMultiSeriesCustom(\n",
    "                 regressor      = Ridge(random_state=123),\n",
    "                 fun_predictors = create_predictors,\n",
    "                 window_size    = 20\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, y_index, y_train_index = forecaster.create_train_X_y(series=data, exog=exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_predictor_0</th>\n",
       "      <th>custom_predictor_1</th>\n",
       "      <th>custom_predictor_2</th>\n",
       "      <th>exog_1</th>\n",
       "      <th>exog_2</th>\n",
       "      <th>series_1</th>\n",
       "      <th>series_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.387554</td>\n",
       "      <td>0.751503</td>\n",
       "      <td>0.496401</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.427283</td>\n",
       "      <td>0.387554</td>\n",
       "      <td>0.496275</td>\n",
       "      <td>21.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.413890</td>\n",
       "      <td>0.427283</td>\n",
       "      <td>0.496924</td>\n",
       "      <td>22.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.428859</td>\n",
       "      <td>0.413890</td>\n",
       "      <td>0.496759</td>\n",
       "      <td>23.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.470126</td>\n",
       "      <td>0.428859</td>\n",
       "      <td>0.495638</td>\n",
       "      <td>24.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>12.199410</td>\n",
       "      <td>11.765890</td>\n",
       "      <td>9.803897</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>7.618220</td>\n",
       "      <td>12.199410</td>\n",
       "      <td>9.785823</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>6.494350</td>\n",
       "      <td>7.618220</td>\n",
       "      <td>9.668384</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>8.278870</td>\n",
       "      <td>6.494350</td>\n",
       "      <td>9.557504</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>8.162550</td>\n",
       "      <td>8.278870</td>\n",
       "      <td>9.467777</td>\n",
       "      <td>203.0</td>\n",
       "      <td>2030.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     custom_predictor_0  custom_predictor_1  custom_predictor_2  exog_1  \\\n",
       "0              0.387554            0.751503            0.496401    20.0   \n",
       "1              0.427283            0.387554            0.496275    21.0   \n",
       "2              0.413890            0.427283            0.496924    22.0   \n",
       "3              0.428859            0.413890            0.496759    23.0   \n",
       "4              0.470126            0.428859            0.495638    24.0   \n",
       "..                  ...                 ...                 ...     ...   \n",
       "363           12.199410           11.765890            9.803897   199.0   \n",
       "364            7.618220           12.199410            9.785823   200.0   \n",
       "365            6.494350            7.618220            9.668384   201.0   \n",
       "366            8.278870            6.494350            9.557504   202.0   \n",
       "367            8.162550            8.278870            9.467777   203.0   \n",
       "\n",
       "     exog_2  series_1  series_2  \n",
       "0     200.0       1.0       0.0  \n",
       "1     210.0       1.0       0.0  \n",
       "2     220.0       1.0       0.0  \n",
       "3     230.0       1.0       0.0  \n",
       "4     240.0       1.0       0.0  \n",
       "..      ...       ...       ...  \n",
       "363  1990.0       0.0       1.0  \n",
       "364  2000.0       0.0       1.0  \n",
       "365  2010.0       0.0       1.0  \n",
       "366  2020.0       0.0       1.0  \n",
       "367  2030.0       0.0       1.0  \n",
       "\n",
       "[368 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================== \n",
       "ForecasterAutoregMultiSeriesCustom \n",
       "================================== \n",
       "Regressor: Ridge(random_state=123) \n",
       "Predictors created with function: create_predictors \n",
       "Transformer for series: None \n",
       "Transformer for exog: None \n",
       "Window size: 20 \n",
       "Series levels (names): ['series_1', 'series_2'] \n",
       "Series weights: None \n",
       "Weight function included: False \n",
       "Exogenous included: True \n",
       "Type of exogenous variable: <class 'pandas.core.frame.DataFrame'> \n",
       "Exogenous variables names: ['exog_1', 'exog_2'] \n",
       "Training range: [Timestamp('1991-07-01 00:00:00'), Timestamp('2008-06-01 00:00:00')] \n",
       "Training index type: DatetimeIndex \n",
       "Training index frequency: MS \n",
       "Regressor parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001} \n",
       "Creation date: 2023-03-01 11:53:12 \n",
       "Last fit date: 2023-03-01 11:53:14 \n",
       "Skforecast version: 0.7.0 \n",
       "Python version: 3.10.9 "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster.fit(series=data, exog=exog)\n",
    "forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "To make predictions `exog` must start one step ahead of `last_window` end.\n    `last_window` ends at : 2008-06-01 00:00:00.\n    Expected index        : 2008-07-01 00:00:00.\n    `exog` starts at      : 1991-07-01 00:00:00.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m forecaster\u001b[39m.\u001b[39;49mpredict(levels\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mseries_1\u001b[39;49m\u001b[39m'\u001b[39;49m, steps\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, exog\u001b[39m=\u001b[39;49mexog)\n",
      "Cell \u001b[0;32mIn[9], line 762\u001b[0m, in \u001b[0;36mForecasterAutoregMultiSeriesCustom.predict\u001b[0;34m(self, steps, levels, last_window, exog)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mif\u001b[39;00m last_window \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m     last_window \u001b[39m=\u001b[39m deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_window)\n\u001b[0;32m--> 762\u001b[0m check_predict_input(\n\u001b[1;32m    763\u001b[0m     forecaster_type  \u001b[39m=\u001b[39;49m \u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[1;32m    764\u001b[0m     steps            \u001b[39m=\u001b[39;49m steps,\n\u001b[1;32m    765\u001b[0m     fitted           \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitted,\n\u001b[1;32m    766\u001b[0m     included_exog    \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mincluded_exog,\n\u001b[1;32m    767\u001b[0m     index_type       \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex_type,\n\u001b[1;32m    768\u001b[0m     index_freq       \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex_freq,\n\u001b[1;32m    769\u001b[0m     window_size      \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_size,\n\u001b[1;32m    770\u001b[0m     last_window      \u001b[39m=\u001b[39;49m last_window,\n\u001b[1;32m    771\u001b[0m     exog             \u001b[39m=\u001b[39;49m exog,\n\u001b[1;32m    772\u001b[0m     exog_type        \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexog_type,\n\u001b[1;32m    773\u001b[0m     exog_col_names   \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexog_col_names,\n\u001b[1;32m    774\u001b[0m     interval         \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    775\u001b[0m     max_steps        \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    776\u001b[0m     levels           \u001b[39m=\u001b[39;49m levels,\n\u001b[1;32m    777\u001b[0m     series_col_names \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseries_col_names\n\u001b[1;32m    778\u001b[0m )\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m exog \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exog, pd\u001b[39m.\u001b[39mDataFrame):\n",
      "File \u001b[0;32m~/varios/skforecast/skforecast/utils/utils.py:526\u001b[0m, in \u001b[0;36mcheck_predict_input\u001b[0;34m(forecaster_type, steps, fitted, included_exog, index_type, index_freq, window_size, last_window, last_window_exog, exog, exog_type, exog_col_names, interval, alpha, max_steps, levels, series_col_names)\u001b[0m\n\u001b[1;32m    524\u001b[0m     expected_index \u001b[39m=\u001b[39m expand_index(last_window\u001b[39m.\u001b[39mindex, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    525\u001b[0m     \u001b[39mif\u001b[39;00m expected_index \u001b[39m!=\u001b[39m exog\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 526\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m             (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTo make predictions `exog` must start one step ahead of `last_window` end.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    528\u001b[0m              \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m    `last_window` ends at : \u001b[39m\u001b[39m{\u001b[39;00mlast_window\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    529\u001b[0m              \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m    Expected index        : \u001b[39m\u001b[39m{\u001b[39;00mexpected_index\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    530\u001b[0m              \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m    `exog` starts at      : \u001b[39m\u001b[39m{\u001b[39;00mexog\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    533\u001b[0m \u001b[39m# Checks ForecasterSarimax\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39mif\u001b[39;00m forecaster_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mForecasterSarimax\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    535\u001b[0m     \u001b[39m# Check last_window_exog type, len, nulls and index (type and freq)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: To make predictions `exog` must start one step ahead of `last_window` end.\n    `last_window` ends at : 2008-06-01 00:00:00.\n    Expected index        : 2008-07-01 00:00:00.\n    `exog` starts at      : 1991-07-01 00:00:00."
     ]
    }
   ],
   "source": [
    "forecaster.predict(levels='series_1', steps=3, exog=exog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cf372b23b9916b33b4c23f9ee2e7d08e597eb711b766c11830fb054f7812cb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
