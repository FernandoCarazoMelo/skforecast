{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\jaesc2\\\\GitHub\\\\skforecast'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "str(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                         ForecasterAutoregDirect                              #\n",
    "#                                                                              #\n",
    "# This work by Joaquin Amat Rodrigo and Javier Escobar Ortiz is licensed       #\n",
    "# under the BSD 3-Clause License.                                              #\n",
    "################################################################################\n",
    "# coding=utf-8\n",
    "\n",
    "from typing import Union, Dict, List, Tuple, Any, Optional, Callable\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.pipeline\n",
    "from sklearn.base import clone\n",
    "import inspect\n",
    "from copy import copy\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "import skforecast\n",
    "from skforecast.ForecasterBase import ForecasterBase\n",
    "from skforecast.utils import initialize_lags\n",
    "from skforecast.utils import initialize_weights\n",
    "from skforecast.utils import check_select_fit_kwargs\n",
    "from skforecast.utils import check_y\n",
    "from skforecast.utils import check_exog\n",
    "from skforecast.utils import get_exog_dtypes\n",
    "from skforecast.utils import check_exog_dtypes\n",
    "from skforecast.utils import check_predict_input\n",
    "from skforecast.utils import check_interval\n",
    "from skforecast.utils import preprocess_y\n",
    "from skforecast.utils import preprocess_last_window\n",
    "from skforecast.utils import preprocess_exog\n",
    "from skforecast.utils import exog_to_direct\n",
    "from skforecast.utils import exog_to_direct_numpy\n",
    "from skforecast.utils import expand_index\n",
    "from skforecast.utils import transform_series\n",
    "from skforecast.utils import transform_dataframe\n",
    "\n",
    "logging.basicConfig(\n",
    "    format = '%(name)-10s %(levelname)-5s %(message)s', \n",
    "    level  = logging.INFO,\n",
    ")\n",
    "\n",
    "\n",
    "class ForecasterAutoregDirect2(ForecasterBase):\n",
    "    \"\"\"\n",
    "    This class turns any regressor compatible with the scikit-learn API into a\n",
    "    autoregressive direct multi-step forecaster. A separate model is created for\n",
    "    each forecast time step. See documentation for more details.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor : regressor or pipeline compatible with the scikit-learn API\n",
    "        An instance of a regressor or pipeline compatible with the scikit-learn API.\n",
    "    steps : int\n",
    "        Maximum number of future steps the forecaster will predict when using\n",
    "        method `predict()`. Since a different model is created for each step,\n",
    "        this value should be defined before training.\n",
    "    lags : int, list, numpy ndarray, range\n",
    "        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n",
    "\n",
    "            - `int`: include lags from 1 to `lags` (included).\n",
    "            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n",
    "            `lags`, all elements must be int.\n",
    "    transformer_y : object transformer (preprocessor), default `None`\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n",
    "        ColumnTransformers are not allowed since they do not have inverse_transform method.\n",
    "        The transformation is applied to `y` before training the forecaster.\n",
    "    transformer_exog : object transformer (preprocessor), default `None`\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API. The transformation is applied to `exog` before training the\n",
    "        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n",
    "    weight_func : Callable, default `None`\n",
    "        Function that defines the individual weights for each sample based on the\n",
    "        index. For example, a function that assigns a lower weight to certain dates.\n",
    "        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n",
    "        method. The resulting `sample_weight` cannot have negative values.\n",
    "    fit_kwargs : dict, default `None`\n",
    "        Additional arguments to be passed to the `fit` method of the regressor.\n",
    "        **New in version 0.8.0**\n",
    "    forecaster_id : str, int, default `None`\n",
    "        Name used as an identifier of the forecaster.\n",
    "        **New in version 0.7.0**\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    regressor : regressor or pipeline compatible with the scikit-learn API\n",
    "        An instance of a regressor or pipeline compatible with the scikit-learn API.\n",
    "        An instance of this regressor is trained for each step. All of them \n",
    "        are stored in `self.regressors_`.\n",
    "    regressors_ : dict\n",
    "        Dictionary with regressors trained for each step. They are initialized \n",
    "        as a copy of `regressor`.\n",
    "    steps : int\n",
    "        Number of future steps the forecaster will predict when using method\n",
    "        `predict()`. Since a different model is created for each step, this value\n",
    "        should be defined before training.\n",
    "    lags : numpy ndarray\n",
    "        Lags used as predictors.\n",
    "    transformer_y : object transformer (preprocessor)\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n",
    "        ColumnTransformers are not allowed since they do not have inverse_transform method.\n",
    "        The transformation is applied to `y` before training the forecaster.\n",
    "    transformer_exog : object transformer (preprocessor)\n",
    "        An instance of a transformer (preprocessor) compatible with the scikit-learn\n",
    "        preprocessing API. The transformation is applied to `exog` before training the\n",
    "        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n",
    "    weight_func : Callable\n",
    "        Function that defines the individual weights for each sample based on the\n",
    "        index. For example, a function that assigns a lower weight to certain dates.\n",
    "        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n",
    "        method. The resulting `sample_weight` cannot have negative values.\n",
    "    source_code_weight_func : str\n",
    "        Source code of the custom function used to create weights.\n",
    "    max_lag : int\n",
    "        Maximum value of lag included in `lags`.\n",
    "    window_size : int\n",
    "        Size of the window needed to create the predictors. It is equal to `max_lag`.\n",
    "    last_window : pandas Series\n",
    "        Last window the forecaster has seen during training. It stores the\n",
    "        values needed to predict the next `step` immediately after the training data.\n",
    "    index_type : type\n",
    "        Type of index of the input used in training.\n",
    "    index_freq : str\n",
    "        Frequency of Index of the input used in training.\n",
    "    training_range : pandas Index\n",
    "        First and last values of index of the data used during training.\n",
    "    included_exog : bool\n",
    "        If the forecaster has been trained using exogenous variable/s.\n",
    "    exog_type : type\n",
    "        Type of exogenous variable/s used in training.\n",
    "    exog_dtypes : dict\n",
    "        Type of each exogenous variable/s used in training. If `transformer_exog` \n",
    "        is used, the dtypes are calculated after the transformation.\n",
    "    exog_col_names : list\n",
    "        Names of columns of `exog` if `exog` used in training was a pandas\n",
    "        DataFrame.\n",
    "    X_train_col_names : list\n",
    "        Names of columns of the matrix created internally for training.\n",
    "    fit_kwargs : dict\n",
    "        Additional arguments to be passed to the `fit` method of the regressor.\n",
    "        **New in version 0.8.0**\n",
    "    in_sample_residuals : dict\n",
    "        Residuals of the models when predicting training data. Only stored up to\n",
    "        1000 values per model in the form `{step: residuals}`. If `transformer_y` \n",
    "        is not `None`, residuals are stored in the transformed scale.\n",
    "    out_sample_residuals : dict\n",
    "        Residuals of the models when predicting non training data. Only stored\n",
    "        up to 1000 values per model in the form `{step: residuals}`. If `transformer_y` \n",
    "        is not `None`, residuals are assumed to be in the transformed scale. Use \n",
    "        `set_out_sample_residuals()` method to set values.\n",
    "    fitted : bool\n",
    "        Tag to identify if the regressor has been fitted (trained).\n",
    "    creation_date : str\n",
    "        Date of creation.\n",
    "    fit_date : str\n",
    "        Date of last fit.\n",
    "    skforcast_version : str\n",
    "        Version of skforecast library used to create the forecaster.\n",
    "    python_version : str\n",
    "        Version of python used to create the forecaster.\n",
    "    forecaster_id : str, int default `None`\n",
    "        Name used as an identifier of the forecaster.\n",
    "    fit_kwargs : dict, default `None`\n",
    "        Additional parameters passed to the `fit` method of the regressor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    A separate model is created for each forecasting time step. It is important to\n",
    "    note that all models share the same parameter and hyperparameter configuration.\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        regressor: object,\n",
    "        steps: int,\n",
    "        lags: Union[int, np.ndarray, list],\n",
    "        transformer_y: Optional[object]=None,\n",
    "        transformer_exog: Optional[object]=None,\n",
    "        weight_func: Optional[Callable]=None,\n",
    "        fit_kwargs: Optional[dict]=None,\n",
    "        forecaster_id: Optional[Union[str, int]]=None,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.regressor               = regressor\n",
    "        self.steps                   = steps\n",
    "        self.transformer_y           = transformer_y\n",
    "        self.transformer_exog        = transformer_exog\n",
    "        self.weight_func             = weight_func\n",
    "        self.source_code_weight_func = None\n",
    "        self.last_window             = None\n",
    "        self.index_type              = None\n",
    "        self.index_freq              = None\n",
    "        self.training_range          = None\n",
    "        self.included_exog           = False\n",
    "        self.exog_type               = None\n",
    "        self.exog_dtypes             = None\n",
    "        self.exog_col_names          = None\n",
    "        self.X_train_col_names       = None\n",
    "        self.fitted                  = False\n",
    "        self.creation_date           = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        self.fit_date                = None\n",
    "        self.skforcast_version       = skforecast.__version__\n",
    "        self.python_version          = sys.version.split(\" \")[0]\n",
    "        self.forecaster_id           = forecaster_id\n",
    "\n",
    "        if not isinstance(steps, int):\n",
    "            raise TypeError(\n",
    "                (f\"`steps` argument must be an int greater than or equal to 1. \"\n",
    "                 f\"Got {type(steps)}.\")\n",
    "            )\n",
    "\n",
    "        if steps < 1:\n",
    "            raise ValueError(\n",
    "                f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n",
    "            )\n",
    "        \n",
    "        self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n",
    "        self.lags = initialize_lags(type(self).__name__, lags)\n",
    "        self.max_lag = max(self.lags)\n",
    "        self.window_size = self.max_lag\n",
    "\n",
    "        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n",
    "            forecaster_name = type(self).__name__, \n",
    "            regressor       = regressor, \n",
    "            weight_func     = weight_func, \n",
    "            series_weights  = None\n",
    "        )\n",
    "\n",
    "        self.fit_kwargs = check_select_fit_kwargs(\n",
    "                              regressor  = regressor,\n",
    "                              fit_kwargs = fit_kwargs\n",
    "                          )\n",
    "\n",
    "        self.in_sample_residuals = {step: None for step in range(1, steps + 1)}\n",
    "        self.out_sample_residuals = None\n",
    "    \n",
    "\n",
    "    def __repr__(\n",
    "        self\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Information displayed when a ForecasterAutoregDirect object is printed.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n",
    "            name_pipe_steps = tuple(name + \"__\" for name in self.regressor.named_steps.keys())\n",
    "            params = {key : value for key, value in self.regressor.get_params().items() \\\n",
    "                      if key.startswith(name_pipe_steps)}\n",
    "        else:\n",
    "            params = self.regressor.get_params()\n",
    "\n",
    "        info = (\n",
    "            f\"{'=' * len(type(self).__name__)} \\n\"\n",
    "            f\"{type(self).__name__} \\n\"\n",
    "            f\"{'=' * len(type(self).__name__)} \\n\"\n",
    "            f\"Regressor: {self.regressor} \\n\"\n",
    "            f\"Lags: {self.lags} \\n\"\n",
    "            f\"Transformer for y: {self.transformer_y} \\n\"\n",
    "            f\"Transformer for exog: {self.transformer_exog} \\n\"\n",
    "            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n",
    "            f\"Window size: {self.window_size} \\n\"\n",
    "            f\"Maximum steps predicted: {self.steps} \\n\"\n",
    "            f\"Exogenous included: {self.included_exog} \\n\"\n",
    "            f\"Type of exogenous variable: {self.exog_type} \\n\"\n",
    "            f\"Exogenous variables names: {self.exog_col_names} \\n\"\n",
    "            f\"Training range: {self.training_range.to_list() if self.fitted else None} \\n\"\n",
    "            f\"Training index type: {str(self.index_type).split('.')[-1][:-2] if self.fitted else None} \\n\"\n",
    "            f\"Training index frequency: {self.index_freq if self.fitted else None} \\n\"\n",
    "            f\"Regressor parameters: {params} \\n\"\n",
    "            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n",
    "            f\"Creation date: {self.creation_date} \\n\"\n",
    "            f\"Last fit date: {self.fit_date} \\n\"\n",
    "            f\"Skforecast version: {self.skforcast_version} \\n\"\n",
    "            f\"Python version: {self.python_version} \\n\"\n",
    "            f\"Forecaster id: {self.forecaster_id} \\n\"\n",
    "        )\n",
    "\n",
    "        return info\n",
    "    \n",
    "    \n",
    "    def _create_lags(\n",
    "        self, \n",
    "        y: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Transforms a 1d array into a 2d array (X) and a 1d array (y). Each row\n",
    "        in X is associated with a value of y and it represents the lags that\n",
    "        precede it.\n",
    "        \n",
    "        Notice that, the returned matrix X_data, contains the lag 1 in the first\n",
    "        column, the lag 2 in the second column and so on.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy ndarray\n",
    "            1d numpy ndarray Training time series.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_data : numpy ndarray\n",
    "            2d numpy ndarray with the lagged values (predictors). \n",
    "            Shape: (samples - max(self.lags), len(self.lags))\n",
    "        y_data : numpy ndarray\n",
    "            1d numpy ndarray with the values of the time series related to each \n",
    "            row of `X_data`. \n",
    "            Shape: (samples - max(self.lags), )\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        n_splits = len(y) - self.max_lag - (self.steps - 1) # rows of y_data\n",
    "        if n_splits <= 0:\n",
    "            raise ValueError(\n",
    "                (f\"The maximum lag ({self.max_lag}) must be less than the length \"\n",
    "                 f\"of the series minus the number of steps ({len(y)-(self.steps-1)}).\")\n",
    "            )\n",
    "        \n",
    "        X_data = np.full(shape=(n_splits, len(self.lags)), fill_value=np.nan, dtype=float)\n",
    "        for i, lag in enumerate(self.lags):\n",
    "            X_data[:, i] = y[self.max_lag - lag : -(lag + self.steps - 1)] \n",
    "\n",
    "        y_data = np.full(shape=(n_splits, self.steps), fill_value=np.nan, dtype=float)\n",
    "        for step in range(self.steps):\n",
    "            y_data[:, step] = y[self.max_lag + step : self.max_lag + step + n_splits]\n",
    "            \n",
    "        return X_data, y_data\n",
    "\n",
    "\n",
    "    def create_train_X_y(\n",
    "        self,\n",
    "        y: pd.Series,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Create training matrices from univariate time series and exogenous\n",
    "        variables. The resulting matrices contain the target variable and predictors\n",
    "        needed to train all the regressors (one per step).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pandas Series\n",
    "            Training time series.\n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s. Must have the same\n",
    "            number of observations as `y` and their indexes must be aligned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train : pandas DataFrame\n",
    "            Training values (predictors) for each step.\n",
    "            Shape: (len(y) - self.max_lag, len(self.lags))\n",
    "        y_train : pandas DataFrame\n",
    "            Values (target) of the time series related to each row of `X_train` \n",
    "            for each step.\n",
    "            Shape: (len(y) - self.max_lag, )\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if len(y) < self.max_lag + self.steps:\n",
    "            raise ValueError(\n",
    "                (f\"Minimum length of `y` for training this forecaster is \"\n",
    "                 f\"{self.max_lag + self.steps}. Got {len(y)}. Reduce the \"\n",
    "                 f\"number of predicted steps, {self.steps}, or the maximum \"\n",
    "                 f\"lag, {self.max_lag}, if no more data is available.\")\n",
    "            )\n",
    "\n",
    "        check_y(y=y)\n",
    "        y = transform_series(\n",
    "                series            = y,\n",
    "                transformer       = self.transformer_y,\n",
    "                fit               = True,\n",
    "                inverse_transform = False\n",
    "            )\n",
    "        y_values, y_index = preprocess_y(y=y)\n",
    "\n",
    "        if exog is not None:\n",
    "            if len(exog) != len(y):\n",
    "                raise ValueError(\n",
    "                    (f\"`exog` must have same number of samples as `y`. \"\n",
    "                     f\"length `exog`: ({len(exog)}), length `y`: ({len(y)})\")\n",
    "                )\n",
    "            check_exog(exog=exog, allow_nan=True)\n",
    "            # Need here for filter_train_X_y_for_step to work without fitting\n",
    "            self.included_exog = True\n",
    "            if isinstance(exog, pd.Series):\n",
    "                exog = transform_series(\n",
    "                           series            = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = True,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "            else:\n",
    "                exog = transform_dataframe(\n",
    "                           df                = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = True,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "                \n",
    "            check_exog(exog=exog, allow_nan=False)\n",
    "            check_exog_dtypes(exog)\n",
    "            self.exog_dtypes = get_exog_dtypes(exog=exog)\n",
    "\n",
    "            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n",
    "            if not (exog_index[:len(y_index)] == y_index).all():\n",
    "                raise ValueError(\n",
    "                    (\"Different index for `y` and `exog`. They must be equal \"\n",
    "                     \"to ensure the correct alignment of values.\")      \n",
    "                )\n",
    "\n",
    "        X_train, y_train = self._create_lags(y=y_values)\n",
    "        X_train_col_names = [f\"lag_{i}\" for i in self.lags]\n",
    "        X_train = pd.DataFrame(\n",
    "                      data    = X_train,\n",
    "                      columns = X_train_col_names,\n",
    "                      index   = y_index[self.max_lag + (self.steps -1): ]\n",
    "                  )\n",
    "\n",
    "        if exog is not None:\n",
    "            # Transform exog to match direct format\n",
    "            # The first `self.max_lag` positions have to be removed from X_exog\n",
    "            # since they are not in X_lags.\n",
    "            exog_to_train = exog_to_direct(\n",
    "                                exog  = exog,\n",
    "                                steps = self.steps\n",
    "                            ).iloc[-X_train.shape[0]:, :]\n",
    "            X_train = pd.concat((X_train, exog_to_train), axis=1)\n",
    "\n",
    "        self.X_train_col_names = X_train.columns.to_list()\n",
    "\n",
    "        y_train_col_names = [f\"y_step_{i+1}\" for i in range(self.steps)]\n",
    "        y_train = pd.DataFrame(\n",
    "                      data    = y_train,\n",
    "                      index   = y_index[self.max_lag + (self.steps -1): ],\n",
    "                      columns = y_train_col_names,\n",
    "                  )\n",
    "        \n",
    "        return X_train, y_train\n",
    "\n",
    "    \n",
    "    def filter_train_X_y_for_step(\n",
    "        self,\n",
    "        step: int,\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        remove_suffix: bool=False\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Select the columns needed to train a forecaster for a specific step.  \n",
    "        The input matrices should be created using `create_train_X_y()`. If \n",
    "        `remove_suffix=True` the suffix \"_step_i\" will be removed from the \n",
    "        column names. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        step : int\n",
    "            Step for which columns must be selected selected. Starts at 1.\n",
    "        X_train : pandas DataFrame\n",
    "            Dataframe generated with the method `create_train_X_y`, first return.\n",
    "        y_train : pandas DataFrame\n",
    "            Dataframe generated with the method `create_train_X_y`, second return.\n",
    "        remove_suffix : bool, default `False`\n",
    "            If True, suffix \"_step_i\" is removed from the column names.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train_step : pandas DataFrame\n",
    "            Training values (predictors) for the selected step.\n",
    "        y_train_step : pandas Series\n",
    "            Values (target) of the time series related to each row of `X_train`.\n",
    "            Shape: (len(y) - self.max_lag)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if (step < 1) or (step > self.steps):\n",
    "            raise ValueError(\n",
    "                (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n",
    "                 f\"and the maximum step is {self.steps}.\")\n",
    "            )\n",
    "\n",
    "        # Matrices X_train and y_train start at index 0.\n",
    "        y_train_step = y_train.iloc[:, step - 1] \n",
    "\n",
    "        if not self.included_exog:\n",
    "            X_train_step = X_train\n",
    "        else:\n",
    "            idx_columns_lags = np.arange(len(self.lags))\n",
    "            n_exog = (len(self.X_train_col_names) - len(self.lags)) / self.steps\n",
    "            idx_columns_exog = (\n",
    "                np.arange((step-1)*n_exog, (step)*n_exog) + idx_columns_lags[-1] + 1\n",
    "            )\n",
    "            idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n",
    "            X_train_step = X_train.iloc[:, idx_columns]\n",
    "\n",
    "        if remove_suffix:\n",
    "            X_train_step.columns = [col_name.replace(f\"_step_{step}\", \"\")\n",
    "                                    for col_name in X_train_step.columns]\n",
    "            y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n",
    "\n",
    "        return  X_train_step, y_train_step\n",
    "\n",
    "\n",
    "    def create_sample_weights(\n",
    "        self,\n",
    "        X_train: pd.DataFrame,\n",
    "    )-> np.ndarray:\n",
    "        \"\"\"\n",
    "        Crate weights for each observation according to the forecaster's attribute\n",
    "        `weight_func`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : pandas DataFrame\n",
    "            Dataframe generated with the methods `create_train_X_y` and \n",
    "            `filter_train_X_y_for_step`, first return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample_weight : numpy ndarray\n",
    "            Weights to use in `fit` method.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        sample_weight = None\n",
    "\n",
    "        if self.weight_func is not None:\n",
    "            sample_weight = self.weight_func(X_train.index)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            if np.isnan(sample_weight).any():\n",
    "                raise ValueError(\n",
    "                    \"The resulting `sample_weight` cannot have NaN values.\"\n",
    "                )\n",
    "            if np.any(sample_weight < 0):\n",
    "                raise ValueError(\n",
    "                    \"The resulting `sample_weight` cannot have negative values.\"\n",
    "                )\n",
    "            if np.sum(sample_weight) == 0:\n",
    "                raise ValueError(\n",
    "                    (\"The resulting `sample_weight` cannot be normalized because \"\n",
    "                     \"the sum of the weights is zero.\")\n",
    "                )\n",
    "\n",
    "        return sample_weight\n",
    "    \n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        y: pd.Series,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        store_in_sample_residuals: bool=True,\n",
    "        n_jobs:int=-1\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Training Forecaster.\n",
    "\n",
    "        Additional arguments to be passed to the `fit` method of the regressor \n",
    "        can be added with the `fit_kwargs` argument when initializing the forecaster.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : pandas Series\n",
    "            Training time series.\n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s. Must have the same\n",
    "            number of observations as `y` and their indexes must be aligned so\n",
    "            that y[i] is regressed on exog[i].\n",
    "        store_in_sample_residuals : bool, default `True`\n",
    "            If True, in-sample residuals will be stored in the forecaster object\n",
    "            after fitting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Reset values in case the forecaster has already been fitted.\n",
    "        self.index_type          = None\n",
    "        self.index_freq          = None\n",
    "        self.last_window         = None\n",
    "        self.included_exog       = False\n",
    "        self.exog_type           = None\n",
    "        self.exog_dtypes         = None\n",
    "        self.exog_col_names      = None\n",
    "        self.X_train_col_names   = None\n",
    "        self.in_sample_residuals = {step: None for step in range(1, self.steps + 1)}\n",
    "        self.fitted              = False\n",
    "        self.training_range      = None\n",
    "\n",
    "        if exog is not None:\n",
    "            self.included_exog = True\n",
    "            self.exog_type = type(exog)\n",
    "            self.exog_col_names = \\\n",
    "                 exog.columns.to_list() if isinstance(exog, pd.DataFrame) else exog.name\n",
    "\n",
    "        X_train, y_train = self.create_train_X_y(y=y, exog=exog)\n",
    "\n",
    "        def fit_forecaster(X_train, y_train, step, store_in_sample_residuals):\n",
    "\n",
    "            # self.regressors_ and self.filter_train_X_y_for_step expect\n",
    "            # first step to start at value 1\n",
    "            X_train_step, y_train_step = self.filter_train_X_y_for_step(\n",
    "                                             step          = step,\n",
    "                                             X_train       = X_train,\n",
    "                                             y_train       = y_train,\n",
    "                                             remove_suffix = True\n",
    "                                         )\n",
    "            sample_weight = self.create_sample_weights(X_train=X_train_step)\n",
    "            if sample_weight is not None:\n",
    "                regressor_fitted = self.regressors_[step].fit(\n",
    "                                       X             = X_train_step,\n",
    "                                       y             = y_train_step,\n",
    "                                       sample_weight = sample_weight,\n",
    "                                       **self.fit_kwargs\n",
    "                                   )\n",
    "            else:\n",
    "                regressor_fitted = self.regressors_[step].fit(\n",
    "                                       X = X_train_step,\n",
    "                                       y = y_train_step,\n",
    "                                       **self.fit_kwargs\n",
    "                                   )\n",
    "\n",
    "            # This is done to save time during fit in functions such as backtesting()\n",
    "            if store_in_sample_residuals:\n",
    "                residuals = (\n",
    "                    (y_train_step - regressor_fitted.predict(X_train_step))\n",
    "                ).to_numpy()\n",
    "\n",
    "                if len(residuals) > 1000:\n",
    "                    # Only up to 1000 residuals are stored\n",
    "                        rng = np.random.default_rng(seed=123)\n",
    "                        residuals = rng.choice(\n",
    "                                        a       = residuals, \n",
    "                                        size    = 1000, \n",
    "                                        replace = False\n",
    "                                    )\n",
    "            else:\n",
    "                residuals = None\n",
    "\n",
    "            return step, regressor_fitted, residuals\n",
    "\n",
    "        step_regressors_residuals = (\n",
    "            Parallel(n_jobs=n_jobs)\n",
    "            (delayed(fit_forecaster)\n",
    "            (X_train=X_train, y_train=y_train, step=step, store_in_sample_residuals=store_in_sample_residuals)\n",
    "            for step in range(1, self.steps + 1))\n",
    "        )\n",
    "\n",
    "        self.regressors_ = {step_regressor[0]: step_regressor[1] \n",
    "                            for step_regressor in step_regressors_residuals}\n",
    "\n",
    "        if store_in_sample_residuals:\n",
    "            self.in_sample_residuals = {step_residuals[0]: step_residuals[2] \n",
    "                                        for step_residuals in step_regressors_residuals}\n",
    "\n",
    "        self.fitted = True\n",
    "        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        self.training_range = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n",
    "        self.index_type = type(X_train.index)\n",
    "        if isinstance(X_train.index, pd.DatetimeIndex):\n",
    "            self.index_freq = X_train.index.freqstr\n",
    "        else: \n",
    "            self.index_freq = X_train.index.step\n",
    "\n",
    "        self.last_window = y.iloc[-self.max_lag:].copy()\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        steps: Optional[Union[int, list]]=None,\n",
    "        last_window: Optional[pd.Series]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Predict n steps ahead.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int, list, None, default `None`\n",
    "            Predict n steps. The value of `steps` must be less than or equal to the \n",
    "            value of steps defined when initializing the forecaster. Starts at 1.\n",
    "        \n",
    "                - If `int`: Only steps within the range of 1 to int are predicted.\n",
    "                - If `list`: List of ints. Only the steps contained in the list \n",
    "                are predicted.\n",
    "                - If `None`: As many steps are predicted as were defined at \n",
    "                initialization.\n",
    "        last_window : pandas Series, default `None`\n",
    "            Series values used to create the predictors (lags) needed in the \n",
    "            first iteration of the prediction (t + 1).\n",
    "            If `last_window = None`, the values stored in` self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : pandas Series\n",
    "            Predicted values.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(steps, int):\n",
    "            steps = list(np.arange(steps) + 1)\n",
    "        elif steps is None:\n",
    "            steps = list(np.arange(self.steps) + 1)\n",
    "        elif isinstance(steps, list):\n",
    "            steps = list(np.array(steps))\n",
    "\n",
    "        for step in steps:\n",
    "            if not isinstance(step, (int, np.int64, np.int32)):\n",
    "                raise TypeError(\n",
    "                    (f\"`steps` argument must be an int, a list of ints or `None`. \"\n",
    "                     f\"Got {type(steps)}.\")\n",
    "                )\n",
    "\n",
    "        if last_window is None:\n",
    "            last_window = copy(self.last_window)\n",
    "\n",
    "        check_predict_input(\n",
    "            forecaster_name  = type(self).__name__,\n",
    "            steps            = steps,\n",
    "            fitted           = self.fitted,\n",
    "            included_exog    = self.included_exog,\n",
    "            index_type       = self.index_type,\n",
    "            index_freq       = self.index_freq,\n",
    "            window_size      = self.window_size,\n",
    "            last_window      = last_window,\n",
    "            last_window_exog = None,\n",
    "            exog             = exog,\n",
    "            exog_type        = self.exog_type,\n",
    "            exog_col_names   = self.exog_col_names,\n",
    "            interval         = None,\n",
    "            alpha            = None,\n",
    "            max_steps        = self.steps,\n",
    "            levels           = None,\n",
    "            series_col_names = None\n",
    "        ) \n",
    "\n",
    "        if exog is not None:\n",
    "            if isinstance(exog, pd.DataFrame):\n",
    "                exog = transform_dataframe(\n",
    "                           df                = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = False,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "            else:\n",
    "                exog = transform_series(\n",
    "                           series            = exog,\n",
    "                           transformer       = self.transformer_exog,\n",
    "                           fit               = False,\n",
    "                           inverse_transform = False\n",
    "                       )\n",
    "            check_exog_dtypes(exog=exog)\n",
    "            exog_values = exog_to_direct_numpy(\n",
    "                              exog  = exog.to_numpy()[:max(steps)],\n",
    "                              steps = max(steps)\n",
    "                          )[0]\n",
    "        else:\n",
    "            exog_values = None\n",
    "\n",
    "        last_window = transform_series(\n",
    "                          series            = last_window,\n",
    "                          transformer       = self.transformer_y,\n",
    "                          fit               = False,\n",
    "                          inverse_transform = False\n",
    "                      )\n",
    "        last_window_values, last_window_index = preprocess_last_window(\n",
    "                                                    last_window = last_window\n",
    "                                                )\n",
    "\n",
    "        X_lags = last_window_values[-self.lags].reshape(1, -1)\n",
    "\n",
    "        if exog is None:\n",
    "            Xs = [X_lags] * len(steps)\n",
    "        else:\n",
    "            n_exog = exog.shape[1] if isinstance(exog, pd.DataFrame) else 1\n",
    "            Xs = [\n",
    "                np.hstack([X_lags, exog_values[(step-1)*n_exog:(step)*n_exog].reshape(1, -1)])\n",
    "                for step in steps\n",
    "            ]\n",
    "\n",
    "        regressors = [self.regressors_[step] for step in steps]\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress scikit-learn warning: \"X does not have valid feature names,\n",
    "            # but NoOpTransformer was fitted with feature names\".\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            predictions = [\n",
    "                regressor.predict(X)[0] for regressor, X in zip(regressors, Xs)\n",
    "            ]\n",
    "\n",
    "        idx = expand_index(index=last_window_index, steps=max(steps))\n",
    "        predictions = pd.Series(\n",
    "                          data  = predictions,\n",
    "                          index = idx[np.array(steps)-1],\n",
    "                          name  = 'pred'\n",
    "                      )\n",
    "\n",
    "        predictions = transform_series(\n",
    "                          series            = predictions,\n",
    "                          transformer       = self.transformer_y,\n",
    "                          fit               = False,\n",
    "                          inverse_transform = True\n",
    "                      )\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def predict_bootstrapping(\n",
    "        self,\n",
    "        steps: Optional[Union[int, list]]=None,\n",
    "        last_window: Optional[pd.Series]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        n_boot: int=500,\n",
    "        random_state: int=123,\n",
    "        in_sample_residuals: bool=True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate multiple forecasting predictions using a bootstrapping process. \n",
    "        By sampling from a collection of past observed errors (the residuals),\n",
    "        each iteration of bootstrapping generates a different set of predictions. \n",
    "        See the Notes section for more information. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int, list, None, default `None`\n",
    "            Predict n steps. The value of `steps` must be less than or equal to the \n",
    "            value of steps defined when initializing the forecaster. Starts at 1.\n",
    "        \n",
    "                - If `int`: Only steps within the range of 1 to int are predicted.\n",
    "                - If `list`: List of ints. Only the steps contained in the list \n",
    "                are predicted.\n",
    "                - If `None`: As many steps are predicted as were defined at \n",
    "                initialization.\n",
    "        last_window : pandas Series, default `None`\n",
    "            Series values used to create the predictors (lags) needed in the \n",
    "            first iteration of the prediction (t + 1).\n",
    "            If `last_window = None`, the values stored in` self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "        n_boot : int, default `500`\n",
    "            Number of bootstrapping iterations used to estimate prediction\n",
    "            intervals.\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random generator, so that boot intervals are always \n",
    "            deterministic.\n",
    "        in_sample_residuals : bool, default `True`\n",
    "            If `True`, residuals from the training data are used as proxy of\n",
    "            prediction error to create prediction intervals. If `False`, out of\n",
    "            sample residuals are used. In the latter case, the user should have\n",
    "            calculated and stored the residuals within the forecaster (see\n",
    "            `set_out_sample_residuals()`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        boot_predictions : pandas DataFrame\n",
    "            Predictions generated by bootstrapping.\n",
    "            Shape: (steps, n_boot)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        More information about prediction intervals in forecasting:\n",
    "        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n",
    "        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(steps, int):\n",
    "            steps = list(np.arange(steps) + 1)\n",
    "        elif steps is None:\n",
    "            steps = list(np.arange(self.steps) + 1)\n",
    "        elif isinstance(steps, list):\n",
    "            steps = list(np.array(steps))\n",
    "        \n",
    "        if in_sample_residuals:\n",
    "            if not set(steps).issubset(set(self.in_sample_residuals.keys())):\n",
    "                raise ValueError(\n",
    "                    (f\"Not `forecaster.in_sample_residuals` for steps: \"\n",
    "                     f\"{set(steps) - set(self.in_sample_residuals.keys())}.\")\n",
    "                )\n",
    "            residuals = self.in_sample_residuals\n",
    "        else:\n",
    "            if self.out_sample_residuals is None:\n",
    "                raise ValueError(\n",
    "                    (\"`forecaster.out_sample_residuals` is `None`. Use \"\n",
    "                     \"`in_sample_residuals=True` or method `set_out_sample_residuals()` \"\n",
    "                     \"before `predict_interval()`, `predict_bootstrapping()` or \"\n",
    "                     \"`predict_dist()`.\")\n",
    "                )\n",
    "            else:\n",
    "                if not set(steps).issubset(set(self.out_sample_residuals.keys())):\n",
    "                    raise ValueError(\n",
    "                        (f\"Not `forecaster.out_sample_residuals` for steps: \"\n",
    "                         f\"{set(steps) - set(self.out_sample_residuals.keys())}. \"\n",
    "                         f\"Use method `set_out_sample_residuals()`.\")\n",
    "                    )\n",
    "            residuals = self.out_sample_residuals\n",
    "        \n",
    "        check_residuals = (\n",
    "            \"forecaster.in_sample_residuals\" if in_sample_residuals\n",
    "             else \"forecaster.out_sample_residuals\"\n",
    "        )\n",
    "        for step in steps:\n",
    "            if residuals[step] is None:\n",
    "                raise ValueError(\n",
    "                    (f\"forecaster residuals for step {step} are `None`. \"\n",
    "                     f\"Check {check_residuals}.\")\n",
    "                )\n",
    "            elif (residuals[step] == None).any():\n",
    "                raise ValueError(\n",
    "                    (f\"forecaster residuals for step {step} contains `None` values. \"\n",
    "                     f\"Check {check_residuals}.\")\n",
    "                )\n",
    "\n",
    "        predictions = self.predict(\n",
    "                          steps       = steps,\n",
    "                          last_window = last_window,\n",
    "                          exog        = exog \n",
    "                      )\n",
    "\n",
    "        # Predictions must be in the transformed scale before adding residuals\n",
    "        predictions = transform_series(\n",
    "                          series            = predictions,\n",
    "                          transformer       = self.transformer_y,\n",
    "                          fit               = False,\n",
    "                          inverse_transform = False\n",
    "                      )\n",
    "        boot_predictions = pd.concat([predictions] * n_boot, axis=1)\n",
    "        boot_predictions.columns= [f\"pred_boot_{i}\" for i in range(n_boot)]\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            rng = np.random.default_rng(seed=random_state)\n",
    "            sample_residuals = rng.choice(\n",
    "                                   a       = residuals[step],\n",
    "                                   size    = n_boot,\n",
    "                                   replace = True\n",
    "                               )\n",
    "            boot_predictions.iloc[i, :] = boot_predictions.iloc[i, :] + sample_residuals\n",
    "\n",
    "        if self.transformer_y:\n",
    "            for col in boot_predictions.columns:\n",
    "                boot_predictions[col] = transform_series(\n",
    "                                            series            = boot_predictions[col],\n",
    "                                            transformer       = self.transformer_y,\n",
    "                                            fit               = False,\n",
    "                                            inverse_transform = True\n",
    "                                        )\n",
    "        \n",
    "        return boot_predictions\n",
    "\n",
    "    \n",
    "    def predict_interval(\n",
    "        self,\n",
    "        steps: Optional[Union[int, list]]=None,\n",
    "        last_window: Optional[pd.Series]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        interval: list=[5, 95],\n",
    "        n_boot: int=500,\n",
    "        random_state: int=123,\n",
    "        in_sample_residuals: bool=True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Bootstrapping based prediction intervals.\n",
    "        Both predictions and intervals are returned.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : int, list, None, default `None`\n",
    "            Predict n steps. The value of `steps` must be less than or equal to the \n",
    "            value of steps defined when initializing the forecaster. Starts at 1.\n",
    "        \n",
    "                - If `int`: Only steps within the range of 1 to int are predicted.\n",
    "                - If `list`: List of ints. Only the steps contained in the list \n",
    "                are predicted.\n",
    "                - If `None`: As many steps are predicted as were defined at \n",
    "                initialization.\n",
    "        last_window : pandas Series, default `None`\n",
    "            Series values used to create the predictors (lags) needed in the \n",
    "            first iteration of the prediction (t + 1).\n",
    "            If `last_window = None`, the values stored in` self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "        interval : list, default `[5, 95]`\n",
    "            Confidence of the prediction interval estimated. Sequence of \n",
    "            percentiles to compute, which must be between 0 and 100 inclusive. \n",
    "            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n",
    "        n_boot : int, default `500`\n",
    "            Number of bootstrapping iterations used to estimate prediction\n",
    "            intervals.\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random generator, so that boot intervals are always \n",
    "            deterministic.\n",
    "        in_sample_residuals : bool, default `True`\n",
    "            If `True`, residuals from the training data are used as proxy of\n",
    "            prediction error to create prediction intervals. If `False`, out of\n",
    "            sample residuals are used. In the latter case, the user should have\n",
    "            calculated and stored the residuals within the forecaster (see\n",
    "            `set_out_sample_residuals()`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : pandas DataFrame\n",
    "            Values predicted by the forecaster and their estimated interval.\n",
    "\n",
    "                - pred: predictions.\n",
    "                - lower_bound: lower bound of the interval.\n",
    "                - upper_bound: upper bound of the interval.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        More information about prediction intervals in forecasting:\n",
    "        https://otexts.com/fpp2/prediction-intervals.html\n",
    "        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n",
    "        George Athanasopoulos.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        check_interval(interval=interval)\n",
    "\n",
    "        predictions = self.predict(\n",
    "                          steps       = steps,\n",
    "                          last_window = last_window,\n",
    "                          exog        = exog\n",
    "                      )\n",
    "\n",
    "        boot_predictions = self.predict_bootstrapping(\n",
    "                               steps               = steps,\n",
    "                               last_window         = last_window,\n",
    "                               exog                = exog,\n",
    "                               n_boot              = n_boot,\n",
    "                               random_state        = random_state,\n",
    "                               in_sample_residuals = in_sample_residuals\n",
    "                           )\n",
    "\n",
    "        interval = np.array(interval)/100\n",
    "        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n",
    "        predictions_interval.columns = ['lower_bound', 'upper_bound']\n",
    "        predictions = pd.concat((predictions, predictions_interval), axis=1)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    def predict_dist(\n",
    "        self,\n",
    "        distribution: object,\n",
    "        steps: Optional[Union[int, list]]=None,\n",
    "        last_window: Optional[pd.Series]=None,\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame]]=None,\n",
    "        n_boot: int=500,\n",
    "        random_state: int=123,\n",
    "        in_sample_residuals: bool=True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fit a given probability distribution for each step. After generating \n",
    "        multiple forecasting predictions through a bootstrapping process, each \n",
    "        step is fitted to the given distribution.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        distribution : Object\n",
    "            A distribution object from scipy.stats.\n",
    "        steps : int, list, None, default `None`\n",
    "            Predict n steps. The value of `steps` must be less than or equal to the \n",
    "            value of steps defined when initializing the forecaster. Starts at 1.\n",
    "        \n",
    "                - If `int`: Only steps within the range of 1 to int are predicted.\n",
    "                - If `list`: List of ints. Only the steps contained in the list \n",
    "                are predicted.\n",
    "                - If `None`: As many steps are predicted as were defined at \n",
    "                initialization.\n",
    "        last_window : pandas Series, default `None`\n",
    "            Series values used to create the predictors (lags) needed in the \n",
    "            first iteration of the prediction (t + 1).\n",
    "            If `last_window = None`, the values stored in` self.last_window` are\n",
    "            used to calculate the initial predictors, and the predictions start\n",
    "            right after training data.\n",
    "        exog : pandas Series, pandas DataFrame, default `None`\n",
    "            Exogenous variable/s included as predictor/s.\n",
    "        n_boot : int, default `500`\n",
    "            Number of bootstrapping iterations used to estimate prediction\n",
    "            intervals.\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random generator, so that boot intervals are always \n",
    "            deterministic.\n",
    "        in_sample_residuals : bool, default `True`\n",
    "            If `True`, residuals from the training data are used as proxy of\n",
    "            prediction error to create prediction intervals. If `False`, out of\n",
    "            sample residuals are used. In the latter case, the user should have\n",
    "            calculated and stored the residuals within the forecaster (see\n",
    "            `set_out_sample_residuals()`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : pandas DataFrame\n",
    "            Distribution parameters estimated for each step.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        boot_samples = self.predict_bootstrapping(\n",
    "                           steps               = steps,\n",
    "                           last_window         = last_window,\n",
    "                           exog                = exog,\n",
    "                           n_boot              = n_boot,\n",
    "                           random_state        = random_state,\n",
    "                           in_sample_residuals = in_sample_residuals\n",
    "                       )       \n",
    "\n",
    "        param_names = [p for p in inspect.signature(distribution._pdf).parameters\n",
    "                       if not p=='x'] + [\"loc\",\"scale\"]\n",
    "        param_values = np.apply_along_axis(\n",
    "                            lambda x: distribution.fit(x),\n",
    "                            axis = 1,\n",
    "                            arr  = boot_samples\n",
    "                       )\n",
    "        predictions = pd.DataFrame(\n",
    "                          data    = param_values,\n",
    "                          columns = param_names,\n",
    "                          index   = boot_samples.index\n",
    "                      )\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    def set_params(\n",
    "        self, \n",
    "        params: dict\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Set new values to the parameters of the scikit learn model stored in the\n",
    "        forecaster. It is important to note that all models share the same \n",
    "        configuration of parameters and hyperparameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dict\n",
    "            Parameters values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.regressor = clone(self.regressor)\n",
    "        self.regressor.set_params(**params)\n",
    "        self.regressors_ = {step: clone(self.regressor)\n",
    "                            for step in range(1, self.steps + 1)}\n",
    "\n",
    "\n",
    "    def set_fit_kwargs(\n",
    "        self, \n",
    "        fit_kwargs: dict\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Set new values for the additional keyword arguments passed to the `fit` \n",
    "        method of the regressor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fit_kwargs : dict\n",
    "            Dict of the form {\"argument\": new_value}.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n",
    "\n",
    "\n",
    "    def set_lags(\n",
    "        self, \n",
    "        lags: Union[int, list, np.ndarray, range]\n",
    "    ) -> None:\n",
    "        \"\"\"      \n",
    "        Set new value to the attribute `lags`.\n",
    "        Attributes `max_lag` and `window_size` are also updated.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lags : int, list, numpy ndarray, range\n",
    "            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n",
    "\n",
    "            - `int`: include lags from 1 to `lags` (included).\n",
    "            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n",
    "            `lags`, all elements must be int.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.lags = initialize_lags(type(self).__name__, lags)\n",
    "        self.max_lag = max(self.lags)\n",
    "        self.window_size = max(self.lags)\n",
    "\n",
    "\n",
    "    def set_out_sample_residuals(\n",
    "        self, \n",
    "        residuals: dict, \n",
    "        append: bool=True,\n",
    "        transform: bool=True,\n",
    "        random_state: int=123\n",
    "    )-> None:\n",
    "        \"\"\"\n",
    "        Set new values to the attribute `out_sample_residuals`. Out of sample\n",
    "        residuals are meant to be calculated using observations that did not\n",
    "        participate in the training process.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        residuals : dict\n",
    "            Dictionary of numpy ndarrays with the residuals of each model in the\n",
    "            form {step: residuals}. If len(residuals) > 1000, only a random \n",
    "            sample of 1000 values are stored.\n",
    "        append : bool, default `True`\n",
    "            If `True`, new residuals are added to the once already stored in the\n",
    "            attribute `out_sample_residuals`. Once the limit of 1000 values is\n",
    "            reached, no more values are appended. If False, `out_sample_residuals`\n",
    "            is overwritten with the new residuals.\n",
    "        transform : bool, default `True`\n",
    "            If `True`, new residuals are transformed using self.transformer_y.\n",
    "        random_state : int, default `123`\n",
    "            Sets a seed to the random sampling for reproducible output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(residuals, dict) or not all(isinstance(x, np.ndarray) for x in residuals.values()):\n",
    "            raise TypeError(\n",
    "                (f\"`residuals` argument must be a dict of numpy ndarrays in the form \"\n",
    "                 \"`{step: residuals}`. \" \n",
    "                 f\"Got {type(residuals)}.\")\n",
    "            )\n",
    "\n",
    "        if not self.fitted:\n",
    "            raise sklearn.exceptions.NotFittedError(\n",
    "                (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n",
    "                 \"arguments before using `set_out_sample_residuals()`.\")\n",
    "            )\n",
    "        \n",
    "        if self.out_sample_residuals is None:\n",
    "            self.out_sample_residuals = {step: None \n",
    "                                         for step in range(1, self.steps + 1)}\n",
    "        \n",
    "        if not set(self.out_sample_residuals.keys()).issubset(set(residuals.keys())):\n",
    "            warnings.warn(\n",
    "                f\"\"\"\n",
    "                Only residuals of models (steps) \n",
    "                {set(self.out_sample_residuals.keys()).intersection(set(residuals.keys()))} \n",
    "                are updated.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        residuals = {key: value \n",
    "                     for key, value in residuals.items() \n",
    "                     if key in self.out_sample_residuals.keys()}\n",
    "\n",
    "        if not transform and self.transformer_y is not None:\n",
    "            warnings.warn(\n",
    "                (f\"Argument `transform` is set to `False` but forecaster was trained \"\n",
    "                 f\"using a transformer {self.transformer_y}. Ensure that the new \"\n",
    "                 f\"residuals are already transformed or set `transform=True`.\")\n",
    "            )\n",
    "\n",
    "        if transform and self.transformer_y is not None:\n",
    "            warnings.warn(\n",
    "                (f\"Residuals will be transformed using the same transformer used \"\n",
    "                 f\"when training the forecaster ({self.transformer_y}). Ensure the \"\n",
    "                 f\"new residuals are on the same scale as the original time series.\")\n",
    "            )\n",
    "            for key, value in residuals.items():\n",
    "                residuals[key] = transform_series(\n",
    "                                     series            = pd.Series(value, name='residuals'),\n",
    "                                     transformer       = self.transformer_y,\n",
    "                                     fit               = False,\n",
    "                                     inverse_transform = False\n",
    "                                 ).to_numpy()\n",
    "    \n",
    "        for key, value in residuals.items():\n",
    "            if len(value) > 1000:\n",
    "                rng = np.random.default_rng(seed=random_state)\n",
    "                value = rng.choice(a=value, size=1000, replace=False)\n",
    "\n",
    "            if append and self.out_sample_residuals[key] is not None:\n",
    "                free_space = max(0, 1000 - len(self.out_sample_residuals[key]))\n",
    "                if len(value) < free_space:\n",
    "                    value = np.hstack((\n",
    "                                self.out_sample_residuals[key],\n",
    "                                value\n",
    "                            ))\n",
    "                else:\n",
    "                    value = np.hstack((\n",
    "                                self.out_sample_residuals[key],\n",
    "                                value[:free_space]\n",
    "                            ))\n",
    "            \n",
    "            self.out_sample_residuals[key] = value\n",
    "\n",
    " \n",
    "    def get_feature_importances(\n",
    "        self, \n",
    "        step: int\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return feature importance of the model stored in the forecaster for a\n",
    "        specific step. Since a separate model is created for each forecast time\n",
    "        step, it is necessary to select the model from which retrieve information.\n",
    "        Only valid when regressor stores internally the feature importances in\n",
    "        the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n",
    "        `None`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        step : int\n",
    "            Model from which retrieve information (a separate model is created \n",
    "            for each forecast time step). First step is 1.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importances : pandas DataFrame\n",
    "            Feature importances associated with each predictor.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(step, int):\n",
    "            raise TypeError(\n",
    "                f'`step` must be an integer. Got {type(step)}.'\n",
    "            )\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise sklearn.exceptions.NotFittedError(\n",
    "                (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n",
    "                 \"arguments before using `get_feature_importances()`.\")\n",
    "            )\n",
    "\n",
    "        if (step < 1) or (step > self.steps):\n",
    "            raise ValueError(\n",
    "                (f\"The step must have a value from 1 to the maximum number of steps \"\n",
    "                 f\"({self.steps}). Got {step}.\")\n",
    "            )\n",
    "\n",
    "        if isinstance(self.regressor, sklearn.pipeline.Pipeline):\n",
    "            estimator = self.regressors_[step][-1]\n",
    "        else:\n",
    "            estimator = self.regressors_[step]\n",
    "\n",
    "        idx_columns_lags = np.arange(len(self.lags))\n",
    "        if self.included_exog:\n",
    "            idx_columns_exog = np.flatnonzero(\n",
    "                                [name.endswith(f\"step_{step}\") for name in self.X_train_col_names]\n",
    "                               )\n",
    "        else:\n",
    "            idx_columns_exog = np.array([], dtype=int)\n",
    "\n",
    "        idx_columns = np.hstack((idx_columns_lags, idx_columns_exog))\n",
    "        feature_names = [self.X_train_col_names[i].replace(f\"_step_{step}\", \"\") \n",
    "                         for i in idx_columns]\n",
    "\n",
    "        if hasattr(estimator, 'feature_importances_'):\n",
    "            feature_importances = estimator.feature_importances_\n",
    "        elif hasattr(estimator, 'coef_'):\n",
    "            feature_importances = estimator.coef_\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                (f\"Impossible to access feature importances for regressor of type \"\n",
    "                 f\"{type(estimator)}. This method is only valid when the \"\n",
    "                 f\"regressor stores internally the feature importances in the \"\n",
    "                 f\"attribute `feature_importances_` or `coef_`.\")\n",
    "            )\n",
    "            feature_importances = None\n",
    "\n",
    "        if feature_importances is not None:\n",
    "            feature_importances = pd.DataFrame({\n",
    "                                      'feature': feature_names,\n",
    "                                      'importance': feature_importances\n",
    "                                  })\n",
    "\n",
    "        return feature_importances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skforecast.ForecasterAutoregDirect import ForecasterAutoregDirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "# ==============================================================================\n",
    "url = (\n",
    "    'https://raw.githubusercontent.com/JoaquinAmatRodrigo/skforecast/master/'\n",
    "    'data/h2o.csv'\n",
    ")\n",
    "data = pd.read_csv(url, sep=',', header=0, names=['y', 'datetime'])\n",
    "\n",
    "# Data preprocessing\n",
    "# ==============================================================================\n",
    "data['datetime'] = pd.to_datetime(data['datetime'], format='%Y-%m-%d')\n",
    "data = data.set_index('datetime')\n",
    "data = data.asfreq('MS')\n",
    "data = data['y']\n",
    "data = data.sort_index()\n",
    "\n",
    "# Split train-test\n",
    "# ==============================================================================\n",
    "steps = 36\n",
    "data_train = data[:-steps]\n",
    "data_test  = data[-steps:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoregDirect2(\n",
    "                 regressor = Ridge(),\n",
    "                 steps     = 36,\n",
    "                 lags      = 15\n",
    "             )\n",
    "\n",
    "forecaster.fit(y=data_train, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit forecaster\n",
    "# ==============================================================================\n",
    "forecaster = ForecasterAutoregDirect(\n",
    "                 regressor = Ridge(),\n",
    "                 steps     = 36,\n",
    "                 lags      = 15\n",
    "             )\n",
    "\n",
    "forecaster.fit(y=data_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "series_length = 30000\n",
    "lags_list = [12, 24, 36, 48]\n",
    "n_steps = [20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.087717</td>\n",
       "      <td>0.161954</td>\n",
       "      <td>0.258023</td>\n",
       "      <td>0.328843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.139199</td>\n",
       "      <td>0.263831</td>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.181370</td>\n",
       "      <td>0.405505</td>\n",
       "      <td>0.590656</td>\n",
       "      <td>0.799524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.239333</td>\n",
       "      <td>0.526879</td>\n",
       "      <td>0.776604</td>\n",
       "      <td>1.035463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          20        40        60        80\n",
       "12  0.087717  0.161954  0.258023  0.328843\n",
       "24  0.139199  0.263831  0.440789  0.536000\n",
       "36  0.181370  0.405505  0.590656  0.799524\n",
       "48  0.239333  0.526879  0.776604  1.035463"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Profiling Ridge\n",
    "# ======================================================================================\n",
    "y = pd.Series(data = np.random.normal(size=series_length))\n",
    "results = {}\n",
    "\n",
    "for steps in n_steps:\n",
    "\n",
    "    execution_time = []\n",
    "    \n",
    "    for lags in lags_list:\n",
    "\n",
    "        forecaster = ForecasterAutoregDirect(\n",
    "                        regressor = Ridge(random_state=123),\n",
    "                        steps     = steps,\n",
    "                        lags      = lags\n",
    "                    )\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        _ = forecaster.fit(y=y, store_in_sample_residuals=False)\n",
    "        toc = time.perf_counter()\n",
    "        execution_time.append(toc-tic)\n",
    "\n",
    "    results[steps] = execution_time\n",
    "\n",
    "results = pd.DataFrame(\n",
    "              data  = results,\n",
    "              index = lags_list\n",
    "          )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.810357</td>\n",
       "      <td>0.980174</td>\n",
       "      <td>1.148289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.357650</td>\n",
       "      <td>0.721973</td>\n",
       "      <td>1.082750</td>\n",
       "      <td>1.171890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.378928</td>\n",
       "      <td>0.623603</td>\n",
       "      <td>1.016360</td>\n",
       "      <td>1.175709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.390531</td>\n",
       "      <td>0.774029</td>\n",
       "      <td>1.033570</td>\n",
       "      <td>1.099291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          20        40        60        80\n",
       "12  0.353846  0.810357  0.980174  1.148289\n",
       "24  0.357650  0.721973  1.082750  1.171890\n",
       "36  0.378928  0.623603  1.016360  1.175709\n",
       "48  0.390531  0.774029  1.033570  1.099291"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Profiling Ridge\n",
    "# ======================================================================================\n",
    "y = pd.Series(data = np.random.normal(size=series_length))\n",
    "results2 = {}\n",
    "\n",
    "for steps in n_steps:\n",
    "\n",
    "    execution_time = []\n",
    "    \n",
    "    for lags in lags_list:\n",
    "\n",
    "        forecaster = ForecasterAutoregDirect2(\n",
    "                        regressor = Ridge(random_state=123),\n",
    "                        steps     = steps,\n",
    "                        lags      = lags\n",
    "                    )\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        _ = forecaster.fit(y=y, store_in_sample_residuals=False)\n",
    "        toc = time.perf_counter()\n",
    "        execution_time.append(toc-tic)\n",
    "\n",
    "    results2[steps] = execution_time\n",
    "\n",
    "results2 = pd.DataFrame(\n",
    "              data  = results2,\n",
    "              index = lags_list\n",
    "          )\n",
    "\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       20     40     60     80\n",
       "12  False  False  False  False\n",
       "24  False  False  False  False\n",
       "36  False  False  False  False\n",
       "48  False  False  False  False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results > results2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling LGBM n_jobs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "series_length = np.linspace(20000, 200000, num=5, dtype=int)\n",
    "n_steps = [20, 40, 60, 80]\n",
    "lags = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>12.334686</td>\n",
       "      <td>25.893847</td>\n",
       "      <td>36.617537</td>\n",
       "      <td>51.991066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>24.681887</td>\n",
       "      <td>50.250643</td>\n",
       "      <td>72.825879</td>\n",
       "      <td>104.380211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>36.744002</td>\n",
       "      <td>74.103703</td>\n",
       "      <td>110.117938</td>\n",
       "      <td>159.051209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>51.163507</td>\n",
       "      <td>97.194287</td>\n",
       "      <td>148.133254</td>\n",
       "      <td>199.442506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>60.446293</td>\n",
       "      <td>127.063512</td>\n",
       "      <td>191.199149</td>\n",
       "      <td>247.204250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               20          40          60          80\n",
       "20000   12.334686   25.893847   36.617537   51.991066\n",
       "65000   24.681887   50.250643   72.825879  104.380211\n",
       "110000  36.744002   74.103703  110.117938  159.051209\n",
       "155000  51.163507   97.194287  148.133254  199.442506\n",
       "200000  60.446293  127.063512  191.199149  247.204250"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Profiling `create_train_X_y` for different length of series and number of lags\n",
    "# ======================================================================================\n",
    "import time \n",
    "\n",
    "results = {}\n",
    "\n",
    "for steps in n_steps:\n",
    "    execution_time = []\n",
    "    forecaster = ForecasterAutoregDirect(\n",
    "                    regressor = LGBMRegressor(random_state=123, n_jobs=1),\n",
    "                    steps     = steps,\n",
    "                    lags      = lags\n",
    "                )\n",
    "\n",
    "    for n in series_length:\n",
    "        y = pd.Series(data = np.random.normal(size=n))\n",
    "        tic = time.perf_counter()\n",
    "        _ = forecaster.fit(y=y, store_in_sample_residuals=False)\n",
    "        toc = time.perf_counter()\n",
    "        execution_time.append(toc-tic)\n",
    "\n",
    "    results[steps] = execution_time\n",
    "\n",
    "results = pd.DataFrame(\n",
    "              data =  results,\n",
    "              index = series_length\n",
    "          )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>4.774901</td>\n",
       "      <td>4.018665</td>\n",
       "      <td>5.563404</td>\n",
       "      <td>6.980781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>7.448016</td>\n",
       "      <td>22.675722</td>\n",
       "      <td>46.946233</td>\n",
       "      <td>75.007390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>8.860926</td>\n",
       "      <td>25.243963</td>\n",
       "      <td>48.137902</td>\n",
       "      <td>84.425399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>10.748192</td>\n",
       "      <td>27.839012</td>\n",
       "      <td>54.302805</td>\n",
       "      <td>85.702420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>13.289512</td>\n",
       "      <td>30.067127</td>\n",
       "      <td>56.031581</td>\n",
       "      <td>93.056032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               20         40         60         80\n",
       "20000    4.774901   4.018665   5.563404   6.980781\n",
       "65000    7.448016  22.675722  46.946233  75.007390\n",
       "110000   8.860926  25.243963  48.137902  84.425399\n",
       "155000  10.748192  27.839012  54.302805  85.702420\n",
       "200000  13.289512  30.067127  56.031581  93.056032"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Profiling `create_train_X_y` for different length of series and number of lags\n",
    "# ======================================================================================\n",
    "import time \n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for steps in n_steps:\n",
    "    execution_time = []\n",
    "    forecaster = ForecasterAutoregDirect2(\n",
    "                    regressor = LGBMRegressor(random_state=123, n_jobs=1),\n",
    "                    steps     = steps,\n",
    "                    lags      = lags\n",
    "                )\n",
    "\n",
    "    for n in series_length:\n",
    "        y = pd.Series(data = np.random.normal(size=n))\n",
    "        tic = time.perf_counter()\n",
    "        _ = forecaster.fit(y=y, store_in_sample_residuals=False)\n",
    "        toc = time.perf_counter()\n",
    "        execution_time.append(toc-tic)\n",
    "\n",
    "    results2[steps] = execution_time\n",
    "\n",
    "results2 = pd.DataFrame(\n",
    "              data =  results2,\n",
    "              index = series_length\n",
    "           )\n",
    "\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65000</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110000</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155000</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          20    40    60    80\n",
       "20000   True  True  True  True\n",
       "65000   True  True  True  True\n",
       "110000  True  True  True  True\n",
       "155000  True  True  True  True\n",
       "200000  True  True  True  True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results > results2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling LGBM n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "series_length = np.linspace(20000, 200000, num=5, dtype=int)\n",
    "n_steps = [20, 40, 60, 80]\n",
    "lags = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(size\u001b[39m=\u001b[39mn))\n\u001b[0;32m     17\u001b[0m tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> 18\u001b[0m _ \u001b[39m=\u001b[39m forecaster\u001b[39m.\u001b[39;49mfit(y\u001b[39m=\u001b[39;49my, store_in_sample_residuals\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     19\u001b[0m toc \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m     20\u001b[0m execution_time\u001b[39m.\u001b[39mappend(toc\u001b[39m-\u001b[39mtic)\n",
      "File \u001b[1;32mc:\\Users\\jaesc2\\GitHub\\skforecast\\skforecast\\ForecasterAutoregDirect\\ForecasterAutoregDirect.py:622\u001b[0m, in \u001b[0;36mForecasterAutoregDirect.fit\u001b[1;34m(self, y, exog, store_in_sample_residuals)\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregressors_[step]\u001b[39m.\u001b[39mfit(\n\u001b[0;32m    616\u001b[0m         X             \u001b[39m=\u001b[39m X_train_step,\n\u001b[0;32m    617\u001b[0m         y             \u001b[39m=\u001b[39m y_train_step,\n\u001b[0;32m    618\u001b[0m         sample_weight \u001b[39m=\u001b[39m sample_weight,\n\u001b[0;32m    619\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_kwargs\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregressors_[step]\u001b[39m.\u001b[39mfit(\n\u001b[0;32m    623\u001b[0m         X \u001b[39m=\u001b[39m X_train_step,\n\u001b[0;32m    624\u001b[0m         y \u001b[39m=\u001b[39m y_train_step,\n\u001b[0;32m    625\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_kwargs\n\u001b[0;32m    626\u001b[0m     )\n\u001b[0;32m    628\u001b[0m \u001b[39m# This is done to save time during fit in functions such as backtesting()\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39mif\u001b[39;00m store_in_sample_residuals:\n",
      "File \u001b[1;32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\lightgbm\\sklearn.py:895\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y,\n\u001b[0;32m    889\u001b[0m         sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m         eval_set\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_names\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    891\u001b[0m         eval_init_score\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, eval_metric\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    892\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m'\u001b[39m, feature_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, categorical_feature\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    893\u001b[0m         callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, init_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    894\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 895\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight, init_score\u001b[39m=\u001b[39;49minit_score,\n\u001b[0;32m    896\u001b[0m                 eval_set\u001b[39m=\u001b[39;49meval_set, eval_names\u001b[39m=\u001b[39;49meval_names, eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[0;32m    897\u001b[0m                 eval_init_score\u001b[39m=\u001b[39;49meval_init_score, eval_metric\u001b[39m=\u001b[39;49meval_metric,\n\u001b[0;32m    898\u001b[0m                 early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds, verbose\u001b[39m=\u001b[39;49mverbose, feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[0;32m    899\u001b[0m                 categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature, callbacks\u001b[39m=\u001b[39;49mcallbacks, init_model\u001b[39m=\u001b[39;49minit_model)\n\u001b[0;32m    900\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[39m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[39m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m    749\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    750\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[0;32m    751\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators,\n\u001b[0;32m    752\u001b[0m     valid_sets\u001b[39m=\u001b[39;49mvalid_sets,\n\u001b[0;32m    753\u001b[0m     valid_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[0;32m    754\u001b[0m     fobj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fobj,\n\u001b[0;32m    755\u001b[0m     feval\u001b[39m=\u001b[39;49meval_metrics_callable,\n\u001b[0;32m    756\u001b[0m     init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[0;32m    757\u001b[0m     feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[0;32m    758\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[0;32m    759\u001b[0m )\n\u001b[0;32m    761\u001b[0m \u001b[39mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n",
      "File \u001b[1;32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\lightgbm\\engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[0;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[0;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[0;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[0;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[1;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[0;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaesc2\\Miniconda3\\envs\\skforecast\\lib\\site-packages\\lightgbm\\basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[0;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Profiling `create_train_X_y` for different length of series and number of lags\n",
    "# ======================================================================================\n",
    "import time \n",
    "\n",
    "results = {}\n",
    "\n",
    "for steps in n_steps:\n",
    "    execution_time = []\n",
    "    forecaster = ForecasterAutoregDirect(\n",
    "                    regressor = LGBMRegressor(random_state=123, n_jobs=-1),\n",
    "                    steps     = steps,\n",
    "                    lags      = lags\n",
    "                )\n",
    "\n",
    "    for n in series_length:\n",
    "        y = pd.Series(data = np.random.normal(size=n))\n",
    "        tic = time.perf_counter()\n",
    "        _ = forecaster.fit(y=y, store_in_sample_residuals=False)\n",
    "        toc = time.perf_counter()\n",
    "        execution_time.append(toc-tic)\n",
    "\n",
    "    results[steps] = execution_time\n",
    "\n",
    "results = pd.DataFrame(\n",
    "              data =  results,\n",
    "              index = series_length\n",
    "          )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling `create_train_X_y` for different length of series and number of lags\n",
    "# ======================================================================================\n",
    "import time \n",
    "\n",
    "results2 = {}\n",
    "\n",
    "for steps in n_steps:\n",
    "    execution_time = []\n",
    "    forecaster = ForecasterAutoregDirect2(\n",
    "                    regressor = LGBMRegressor(random_state=123, n_jobs=-1),\n",
    "                    steps     = steps,\n",
    "                    lags      = lags\n",
    "                )\n",
    "\n",
    "    for n in series_length:\n",
    "        y = pd.Series(data = np.random.normal(size=n))\n",
    "        tic = time.perf_counter()\n",
    "        _ = forecaster.fit(y=y, store_in_sample_residuals=False)\n",
    "        toc = time.perf_counter()\n",
    "        execution_time.append(toc-tic)\n",
    "\n",
    "    results2[steps] = execution_time\n",
    "\n",
    "results2 = pd.DataFrame(\n",
    "              data =  results2,\n",
    "              index = series_length\n",
    "           )\n",
    "\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results > results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78d62c1713fdacd99ef7c429003c7324b36fbb551fb8b6860a7ea73e9338235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
