{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/varios/skforecast'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "str(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Union, Tuple, Optional\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_0</th>\n",
       "      <th>series_1</th>\n",
       "      <th>series_2</th>\n",
       "      <th>series_3</th>\n",
       "      <th>series_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.199187</td>\n",
       "      <td>-0.015320</td>\n",
       "      <td>0.656192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532387</td>\n",
       "      <td>0.045695</td>\n",
       "      <td>-1.044882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>0.780637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.729556</td>\n",
       "      <td>-0.409606</td>\n",
       "      <td>0.495430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>-0.337556</td>\n",
       "      <td>0.190319</td>\n",
       "      <td>1.130121</td>\n",
       "      <td>0.307832</td>\n",
       "      <td>0.473396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>-0.264497</td>\n",
       "      <td>0.740949</td>\n",
       "      <td>-1.405627</td>\n",
       "      <td>-1.163562</td>\n",
       "      <td>0.801584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>1.403882</td>\n",
       "      <td>1.371611</td>\n",
       "      <td>-0.398193</td>\n",
       "      <td>-1.452256</td>\n",
       "      <td>0.012552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>0.929296</td>\n",
       "      <td>-2.077337</td>\n",
       "      <td>0.503161</td>\n",
       "      <td>0.071028</td>\n",
       "      <td>1.664818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>-0.396890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.425457</td>\n",
       "      <td>1.157150</td>\n",
       "      <td>-1.593471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>2.014113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.729486</td>\n",
       "      <td>-1.503684</td>\n",
       "      <td>-0.451313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>0.693924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.082551</td>\n",
       "      <td>0.689935</td>\n",
       "      <td>-0.537518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            series_0  series_1  series_2  series_3  series_4\n",
       "2018-01-01       NaN       NaN -0.199187 -0.015320  0.656192\n",
       "2018-01-02       NaN       NaN  0.532387  0.045695 -1.044882\n",
       "2018-01-03  0.780637       NaN  0.729556 -0.409606  0.495430\n",
       "2018-01-04 -0.337556  0.190319  1.130121  0.307832  0.473396\n",
       "2018-01-05 -0.264497  0.740949 -1.405627 -1.163562  0.801584\n",
       "2018-01-06  1.403882  1.371611 -0.398193 -1.452256  0.012552\n",
       "2018-01-07  0.929296 -2.077337  0.503161  0.071028  1.664818\n",
       "2018-01-08 -0.396890       NaN -1.425457  1.157150 -1.593471\n",
       "2018-01-09  2.014113       NaN  1.729486 -1.503684 -0.451313\n",
       "2018-01-10  0.693924       NaN -0.082551  0.689935 -0.537518"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data simulation\n",
    "# ==============================================================================\n",
    "n = 10\n",
    "series = pd.DataFrame(\n",
    "    np.random.randn(n, 5),\n",
    "    index=pd.date_range(\"2018-01-01\", periods=n),\n",
    "    columns=[f\"series_{i}\" for i in range(5)],\n",
    ")\n",
    "\n",
    "exog = {\n",
    "    f\"series_{i}\": pd.DataFrame(\n",
    "        np.random.randn(n, 2),\n",
    "        index=pd.date_range(\"2018-01-01\", periods=n),\n",
    "        columns=[f\"exog_{j}\" for j in range(2)],\n",
    "    )\n",
    "    for i in range(5)\n",
    "}\n",
    "\n",
    "# exog = pd.DataFrame(\n",
    "#     np.random.randn(n, 2),\n",
    "#     index=pd.date_range(\"2018-01-01\", periods=n),\n",
    "#     columns=[\"exog_0\", \"exog_1\"],\n",
    "# )\n",
    "\n",
    "# Injecting missing values\n",
    "series['series_0'].iloc[0:2] = np.nan\n",
    "series['series_1'].iloc[0:3] = np.nan\n",
    "series['series_1'].iloc[-3:] = np.nan\n",
    "# series['series_3'].iloc[[5, 6]] = np.nan Needs to remove the exception in create_train_X_y when there are missing values in y\n",
    "series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=LinearRegression(),\n",
    "    lags=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series lengths after removing leading and trailing nans\n",
      "series_0: 8\n",
      "series_1: 4\n",
      "series_2: 10\n",
      "series_3: 10\n",
      "series_4: 10\n",
      "Exog lengths after removing leading and trailing nans\n",
      "series_0: 10\n",
      "series_1: 10\n",
      "series_2: 10\n",
      "series_3: 10\n",
      "series_4: 10\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# ==============================================================================\n",
    "# Store series and exog as dict\n",
    "if isinstance(series, pd.DataFrame):\n",
    "    series_dict = series.to_dict(\"series\")\n",
    "elif isinstance(series, dict):\n",
    "    series_dict = series\n",
    "\n",
    "if isinstance(exog, pd.DataFrame):\n",
    "    exog_dict = dict.fromkeys(series_dict.keys(), exog)\n",
    "elif isinstance(exog, dict):\n",
    "    exog_dict = exog\n",
    "\n",
    "# Remove leading and trailing nans from each series and exog. This is done\n",
    "# so then there is no need to remove leading and trailing nans in each X_train\n",
    "for k, v in series_dict.items():\n",
    "    series_dict[k] = v.loc[v.first_valid_index():v.last_valid_index()]\n",
    "\n",
    "for k, v in exog_dict.items():\n",
    "    exog_dict[k] = v.loc[v.first_valid_index():v.last_valid_index()]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Series lengths after removing leading and trailing nans\")\n",
    "for k, v in series_dict.items():\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "\n",
    "print(\"Exog lengths after removing leading and trailing nans\")\n",
    "for k, v in exog_dict.items():\n",
    "    print(f\"{k}: {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series_0\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_1\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_2\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_3\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_4\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_train_buffer = []\n",
    "y_train_buffer = []\n",
    "\n",
    "for key in series_dict.keys():\n",
    "    y = series_dict[key]\n",
    "    exog = exog_dict[key]\n",
    "    # TODO: All neded check goes here\n",
    "    exog = exog.loc[y.index]\n",
    "    print(y.name)\n",
    "    print(exog.columns)\n",
    "    X_train, y_train = forecaster.create_train_X_y(y=y, exog=exog)\n",
    "    X_train['level'] = key\n",
    "    X_train_buffer.append(X_train)\n",
    "    y_train_buffer.append(y_train)\n",
    "\n",
    "X_train = pd.concat(X_train_buffer, axis=0)\n",
    "y_train = pd.concat(y_train_buffer, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>exog_0</th>\n",
       "      <th>exog_1</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>-0.264497</td>\n",
       "      <td>-0.337556</td>\n",
       "      <td>0.780637</td>\n",
       "      <td>1.208184</td>\n",
       "      <td>0.652760</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1.403882</td>\n",
       "      <td>-0.264497</td>\n",
       "      <td>-0.337556</td>\n",
       "      <td>0.769003</td>\n",
       "      <td>-0.534458</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.929296</td>\n",
       "      <td>1.403882</td>\n",
       "      <td>-0.264497</td>\n",
       "      <td>-1.039888</td>\n",
       "      <td>0.374818</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-0.396890</td>\n",
       "      <td>0.929296</td>\n",
       "      <td>1.403882</td>\n",
       "      <td>-1.852216</td>\n",
       "      <td>-0.070069</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>2.014113</td>\n",
       "      <td>-0.396890</td>\n",
       "      <td>0.929296</td>\n",
       "      <td>0.648391</td>\n",
       "      <td>-1.214645</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>1.371611</td>\n",
       "      <td>0.740949</td>\n",
       "      <td>0.190319</td>\n",
       "      <td>-1.192708</td>\n",
       "      <td>-0.889509</td>\n",
       "      <td>series_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>0.729556</td>\n",
       "      <td>0.532387</td>\n",
       "      <td>-0.199187</td>\n",
       "      <td>-1.151730</td>\n",
       "      <td>-0.781212</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1.130121</td>\n",
       "      <td>0.729556</td>\n",
       "      <td>0.532387</td>\n",
       "      <td>-0.319204</td>\n",
       "      <td>-0.608719</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>-1.405627</td>\n",
       "      <td>1.130121</td>\n",
       "      <td>0.729556</td>\n",
       "      <td>0.864698</td>\n",
       "      <td>1.365684</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>-0.398193</td>\n",
       "      <td>-1.405627</td>\n",
       "      <td>1.130121</td>\n",
       "      <td>0.634982</td>\n",
       "      <td>0.878234</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.503161</td>\n",
       "      <td>-0.398193</td>\n",
       "      <td>-1.405627</td>\n",
       "      <td>-0.223577</td>\n",
       "      <td>0.280358</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-1.425457</td>\n",
       "      <td>0.503161</td>\n",
       "      <td>-0.398193</td>\n",
       "      <td>0.700996</td>\n",
       "      <td>0.797928</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>1.729486</td>\n",
       "      <td>-1.425457</td>\n",
       "      <td>0.503161</td>\n",
       "      <td>0.535337</td>\n",
       "      <td>0.370566</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>-0.409606</td>\n",
       "      <td>0.045695</td>\n",
       "      <td>-0.015320</td>\n",
       "      <td>-0.066763</td>\n",
       "      <td>1.617363</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.307832</td>\n",
       "      <td>-0.409606</td>\n",
       "      <td>0.045695</td>\n",
       "      <td>-1.593222</td>\n",
       "      <td>-0.666917</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>-1.163562</td>\n",
       "      <td>0.307832</td>\n",
       "      <td>-0.409606</td>\n",
       "      <td>-1.214565</td>\n",
       "      <td>0.122646</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>-1.452256</td>\n",
       "      <td>-1.163562</td>\n",
       "      <td>0.307832</td>\n",
       "      <td>-0.026370</td>\n",
       "      <td>1.220579</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.071028</td>\n",
       "      <td>-1.452256</td>\n",
       "      <td>-1.163562</td>\n",
       "      <td>1.000416</td>\n",
       "      <td>0.910793</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>1.157150</td>\n",
       "      <td>0.071028</td>\n",
       "      <td>-1.452256</td>\n",
       "      <td>-0.780470</td>\n",
       "      <td>-1.412446</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>-1.503684</td>\n",
       "      <td>1.157150</td>\n",
       "      <td>0.071028</td>\n",
       "      <td>1.053640</td>\n",
       "      <td>0.514122</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>0.495430</td>\n",
       "      <td>-1.044882</td>\n",
       "      <td>0.656192</td>\n",
       "      <td>1.487447</td>\n",
       "      <td>0.172942</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.473396</td>\n",
       "      <td>0.495430</td>\n",
       "      <td>-1.044882</td>\n",
       "      <td>0.395885</td>\n",
       "      <td>0.061451</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>0.801584</td>\n",
       "      <td>0.473396</td>\n",
       "      <td>0.495430</td>\n",
       "      <td>-0.470937</td>\n",
       "      <td>0.559996</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.801584</td>\n",
       "      <td>0.473396</td>\n",
       "      <td>-0.785194</td>\n",
       "      <td>0.166997</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1.664818</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.801584</td>\n",
       "      <td>0.864719</td>\n",
       "      <td>-1.999071</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-1.593471</td>\n",
       "      <td>1.664818</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.512366</td>\n",
       "      <td>-0.806493</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>-0.451313</td>\n",
       "      <td>-1.593471</td>\n",
       "      <td>1.664818</td>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.564859</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lag_1     lag_2     lag_3    exog_0    exog_1     level\n",
       "2018-01-06 -0.264497 -0.337556  0.780637  1.208184  0.652760  series_0\n",
       "2018-01-07  1.403882 -0.264497 -0.337556  0.769003 -0.534458  series_0\n",
       "2018-01-08  0.929296  1.403882 -0.264497 -1.039888  0.374818  series_0\n",
       "2018-01-09 -0.396890  0.929296  1.403882 -1.852216 -0.070069  series_0\n",
       "2018-01-10  2.014113 -0.396890  0.929296  0.648391 -1.214645  series_0\n",
       "2018-01-07  1.371611  0.740949  0.190319 -1.192708 -0.889509  series_1\n",
       "2018-01-04  0.729556  0.532387 -0.199187 -1.151730 -0.781212  series_2\n",
       "2018-01-05  1.130121  0.729556  0.532387 -0.319204 -0.608719  series_2\n",
       "2018-01-06 -1.405627  1.130121  0.729556  0.864698  1.365684  series_2\n",
       "2018-01-07 -0.398193 -1.405627  1.130121  0.634982  0.878234  series_2\n",
       "2018-01-08  0.503161 -0.398193 -1.405627 -0.223577  0.280358  series_2\n",
       "2018-01-09 -1.425457  0.503161 -0.398193  0.700996  0.797928  series_2\n",
       "2018-01-10  1.729486 -1.425457  0.503161  0.535337  0.370566  series_2\n",
       "2018-01-04 -0.409606  0.045695 -0.015320 -0.066763  1.617363  series_3\n",
       "2018-01-05  0.307832 -0.409606  0.045695 -1.593222 -0.666917  series_3\n",
       "2018-01-06 -1.163562  0.307832 -0.409606 -1.214565  0.122646  series_3\n",
       "2018-01-07 -1.452256 -1.163562  0.307832 -0.026370  1.220579  series_3\n",
       "2018-01-08  0.071028 -1.452256 -1.163562  1.000416  0.910793  series_3\n",
       "2018-01-09  1.157150  0.071028 -1.452256 -0.780470 -1.412446  series_3\n",
       "2018-01-10 -1.503684  1.157150  0.071028  1.053640  0.514122  series_3\n",
       "2018-01-04  0.495430 -1.044882  0.656192  1.487447  0.172942  series_4\n",
       "2018-01-05  0.473396  0.495430 -1.044882  0.395885  0.061451  series_4\n",
       "2018-01-06  0.801584  0.473396  0.495430 -0.470937  0.559996  series_4\n",
       "2018-01-07  0.012552  0.801584  0.473396 -0.785194  0.166997  series_4\n",
       "2018-01-08  1.664818  0.012552  0.801584  0.864719 -1.999071  series_4\n",
       "2018-01-09 -1.593471  1.664818  0.012552  0.512366 -0.806493  series_4\n",
       "2018-01-10 -0.451313 -1.593471  1.664818  0.131258  0.564859  series_4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-06    1.403882\n",
       "2018-01-07    0.929296\n",
       "2018-01-08   -0.396890\n",
       "2018-01-09    2.014113\n",
       "2018-01-10    0.693924\n",
       "2018-01-07   -2.077337\n",
       "2018-01-04    1.130121\n",
       "2018-01-05   -1.405627\n",
       "2018-01-06   -0.398193\n",
       "2018-01-07    0.503161\n",
       "2018-01-08   -1.425457\n",
       "2018-01-09    1.729486\n",
       "2018-01-10   -0.082551\n",
       "2018-01-04    0.307832\n",
       "2018-01-05   -1.163562\n",
       "2018-01-06   -1.452256\n",
       "2018-01-07    0.071028\n",
       "2018-01-08    1.157150\n",
       "2018-01-09   -1.503684\n",
       "2018-01-10    0.689935\n",
       "2018-01-04    0.473396\n",
       "2018-01-05    0.801584\n",
       "2018-01-06    0.012552\n",
       "2018-01-07    1.664818\n",
       "2018-01-08   -1.593471\n",
       "2018-01-09   -0.451313\n",
       "2018-01-10   -0.537518\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_train_X_y_single_series(\n",
    "    self,\n",
    "    y: pd.Series,\n",
    "    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Create training matrices from univariate time series and exogenous\n",
    "    variables. This method does not transform the exog variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : pandas Series\n",
    "        Training time series.\n",
    "    exog : pandas Series, pandas DataFrame, default `None`\n",
    "        Exogenous variable/s included as predictor/s. Must have the same\n",
    "        number of observations as `y` and their indexes must be aligned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train_lags : pandas DataFrame\n",
    "        Training values of lags\n",
    "        Shape: (len(y) - self.max_lag, len(self.lags))\n",
    "    X_train_exog : pandas DataFrame\n",
    "        Training values of exogenous variables.\n",
    "        Shape: (len(y) - self.max_lag, len(exog.columns))\n",
    "    y_train : pandas Series\n",
    "        Values (target) of the time series related to each row of `X_train`.\n",
    "        Shape: (len(y) - self.max_lag, )\n",
    "    \n",
    "    \"\"\"\n",
    "    y_name = y.name\n",
    "    check_y(y=y)\n",
    "    y = transform_series(\n",
    "            series            = y,\n",
    "            transformer       = self.transformer_y,\n",
    "            fit               = True,\n",
    "            inverse_transform = False\n",
    "        )\n",
    "    y_values, y_index = preprocess_y(y=y)\n",
    "\n",
    "    if self.differentiation is not None:\n",
    "        y_values = self.differentiator_[y_name].fit_transform(y_values)\n",
    "    \n",
    "    if exog is not None:\n",
    "        if len(exog) != len(y):\n",
    "            raise ValueError(\n",
    "                (f'`exog` must have same number of samples as `y`. '\n",
    "                 f'length `exog`: ({len(exog)}), length `y`: ({len(y)})')\n",
    "            )\n",
    "        check_exog(exog=exog, allow_nan=True)\n",
    "        check_exog_dtypes(exog)\n",
    "        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n",
    "        if not (exog_index == y_index).all():\n",
    "            raise ValueError(\n",
    "                (\"Different index for `y` and `exog`. They must be equal to \"\n",
    "                \"ensure the correct alignment of values.\")\n",
    "            )\n",
    "    \n",
    "    X_train, y_train = self._create_lags(y=y_values)\n",
    "    X_train_col_names = [f\"lag_{i}\" for i in self.lags]\n",
    "    X_train_lags = pd.DataFrame(\n",
    "                        data    = X_train,\n",
    "                        columns = X_train_col_names,\n",
    "                        index   = y_index[self.max_lag: ]\n",
    "                   )\n",
    "\n",
    "    if exog is not None:\n",
    "        # The first `self.max_lag` positions have to be removed from exog\n",
    "        # since they are not in X_train.\n",
    "        X_train_exog = exog.iloc[self.max_lag:, ]\n",
    "    else:\n",
    "        # TODO: test if this is efficient\n",
    "        X_train_exog = pd.DataFrame(\n",
    "                            data    = np.nan,\n",
    "                            columns = ['dummy_exog_col_to_keep_shape'],\n",
    "                            index   = y_index[self.max_lag: ]\n",
    "                        )\n",
    "\n",
    "    y_train = pd.Series(\n",
    "                    data  = y_train,\n",
    "                    index = y_index[self.max_lag: ],\n",
    "                    name  = 'y'\n",
    "                )\n",
    "\n",
    "    if self.differentiation is not None:\n",
    "        y_train = y_train.iloc[self.differentiation: ]\n",
    "        X_train_lags = X_train_lags.iloc[self.differentiation: ]\n",
    "        if X_train_exog is not None:\n",
    "            X_train_exog = X_train_exog.iloc[self.differentiation: ]\n",
    "\n",
    "    assert X_train_lags.index.equals(y_train.index)\n",
    "    assert X_train_exog.index.equals(y_train.index)\n",
    "                    \n",
    "    return X_train_lags, X_train_exog, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'element'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_set = set()\n",
    "my_set.add('element')\n",
    "my_set.add('element')\n",
    "my_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_X_y(\n",
    "        self,\n",
    "        series: Union[pd.DataFrame, dict],\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series, pd.Index, pd.Index]:\n",
    "        \"\"\"\n",
    "        Create training matrices from multiple time series and exogenous\n",
    "        variables.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        series : pandas DataFrame, dict\n",
    "            Training time series.\n",
    "        exog : pandas Series, pandas DataFrame, dict, default `None`\n",
    "            Exogenous variable/s included as predictor/s. Must have the same\n",
    "            number of observations as `series` and their indexes must be aligned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train : pandas DataFrame\n",
    "            Training values (predictors).\n",
    "        y_train : pandas Series\n",
    "            Values (target) of the time series related to each row of `X_train`.\n",
    "            Shape: (len(series) - self.max_lag, )\n",
    "        y_index : pandas Index\n",
    "            Index of `series`.\n",
    "        y_train_index: pandas Index\n",
    "            Index of `y_train`.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(series, (pd.DataFrame, dict)):\n",
    "            raise TypeError(f\"`series` must be a pandas DataFrame or dict. Got {type(series)}.\")\n",
    "        \n",
    "        if not isinstance(exog, (pd.Series, pd.DataFrame, dict, type(None))):\n",
    "            raise TypeError(f\"`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.\")\n",
    "\n",
    "        if isinstance(series, pd.DataFrame):\n",
    "            series_dict = series.to_dict(\"series\")\n",
    "        elif isinstance(series, dict):\n",
    "            series_dict = series\n",
    "\n",
    "        if isinstance(exog, pd.DataFrame):\n",
    "            exog_dict = dict.fromkeys(series_dict.keys(), exog)\n",
    "        elif isinstance(exog, dict):\n",
    "            exog_dict = exog\n",
    "        # TODO: if a series has no exog add none in the exog_dict\n",
    "\n",
    "        series_names = list(series_dict.keys())\n",
    "        if self.transformer_series is None:\n",
    "            self.transformer_series_ = {serie: None for serie in series_names}\n",
    "        elif not isinstance(self.transformer_series, dict):\n",
    "            self.transformer_series_ = {serie: clone(self.transformer_series) \n",
    "                                        for serie in series_names}\n",
    "        else:\n",
    "            self.transformer_series_ = {serie: None for serie in series_names}\n",
    "            # Only elements already present in transformer_series_ are updated\n",
    "            self.transformer_series_.update(\n",
    "                (k, v) for k, v in deepcopy(self.transformer_series).items() \n",
    "                if k in self.transformer_series_\n",
    "            )\n",
    "            series_not_in_transformer_series = set(series.columns) - set(self.transformer_series.keys())\n",
    "            if series_not_in_transformer_series:\n",
    "                warnings.warn(\n",
    "                    (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n",
    "                     f\" No transformation is applied to these series.\"),\n",
    "                     IgnoredArgumentWarning\n",
    "                )  \n",
    "        if self.differentiation is None:\n",
    "            self.differentiator_ = {serie: None for serie in series_names}\n",
    "        else:\n",
    "            self.differentiator_ = {serie: clone(self.self.differentiator) for serie in series_names}\n",
    "\n",
    "\n",
    "        # Remove leading and trailing nans from each series.\n",
    "        for k, v in series_dict.items():\n",
    "            series_dict[k] = v.loc[v.first_valid_index():v.last_valid_index()]\n",
    "\n",
    "        if exog is not None:\n",
    "            # Convert exog to dataframe if it is a series\n",
    "            for k, v in exog_dict.items():\n",
    "                if isinstance(v, pd.Series):\n",
    "                    v = v.to_frame()\n",
    "                exog_dict[k] = v\n",
    "\n",
    "            exog_names = [exog.columns.to_list() for exog in exog_dict.values()]\n",
    "            exog_names = list(set([item for sublist in exog_names for item in sublist]))\n",
    "\n",
    "            # Check that all exog have the same dtypes for common columns\n",
    "            exog_dtype_dict = {col_name: set() for col_name in exog_names}\n",
    "            for exog in exog_dict.values():\n",
    "                for col_name in exog.columns:\n",
    "                    exog_dtype_dict[col_name].add(exog[col_name].dtype)\n",
    "            for col_name, dtypes in exog_dtype_dict.items():\n",
    "                if len(dtypes) > 1:\n",
    "                    raise TypeError(\n",
    "                        (f\"Column {col_name} has different dtypes in different exog \"\n",
    "                         f\"DataFrames or Series.\")\n",
    "                    )\n",
    "                    \n",
    "        # Check that all series have the same index type and frequency  \n",
    "        indexes_dtypes = [series.index.dtype for series in series_dict.values()]\n",
    "        if not len(set(indexes_dtypes)) == 1:\n",
    "            raise TypeError(\"All series must have the same index type.\")\n",
    "        if isinstance(indexes_dtypes[0], pd.DatetimeIndex):\n",
    "            indexes_freq = [series.index.freq for series in series_dict.values()]\n",
    "            if not len(set(indexes_freq)) == 1:\n",
    "                raise ValueError(\"All series must have the same frequency.\")\n",
    "                   \n",
    "        X_train_lags_buffer = []\n",
    "        X_train_exog_buffer = []\n",
    "        y_train_buffer = []\n",
    "\n",
    "        for key in series_dict.keys():\n",
    "            y = series_dict[key]\n",
    "            exog = exog_dict[key]\n",
    "            if exog is not None:\n",
    "                exog = exog.loc[y.index]\n",
    "                exog = exog.reindex(y.index, fill_value=np.nan)\n",
    "                # TODO: add warnings and check that both series and exog have the same index\n",
    "            X_train_lags, X_train_exog, y_train = (\n",
    "                forecaster._create_train_X_y_single_series(y=y, exog=exog)\n",
    "            )\n",
    "            X_train_lags['level'] = key\n",
    "            X_train_lags_buffer.append(X_train_lags)\n",
    "            X_train_exog_buffer.append(X_train_exog)\n",
    "            y_train_buffer.append(y_train)\n",
    "\n",
    "        X_train_lags = pd.concat(X_train_lags_buffer, axis=0)\n",
    "        y_train = pd.concat(y_train_buffer, axis=0)\n",
    "        if exog is not None:\n",
    "            X_train_exog = pd.concat(X_train_exog_buffer, axis=0)\n",
    "            X_train_exog = X_train_exog.drop(columns=['dummy_exog_col_to_keep_shape'])\n",
    "            X_train_exog = transform_dataframe(\n",
    "                                df                = X_train_exog,\n",
    "                                transformer       = self.transformer_exog,\n",
    "                                fit               = True,\n",
    "                                inverse_transform = False\n",
    "                            )\n",
    "            check_exog(exog=X_train_exog, allow_nan=False)\n",
    "            check_exog_dtypes(X_train_exog)\n",
    "            self.exog_dtypes = get_exog_dtypes(exog=X_train_exog)\n",
    "            if not (X_train_exog.index == X_train_lags).all():\n",
    "                raise ValueError(\n",
    "                    (\"Different index for `y` and `exog`. They must be equal \"\n",
    "                    \"to ensure the correct alignment of values.\")\n",
    "                )\n",
    "            X_train = pd.concat([X_train_lags, X_train_exog], axis=1)\n",
    "        else:\n",
    "            X_train = X_train_lags\n",
    "\n",
    "        self.X_train_col_names = X_train.columns.to_list()\n",
    "        y_train_index = y_train.index.to_numpy()\n",
    "\n",
    "\n",
    "        return X_train, y_train, y_index, y_train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 10_000\n",
    "\n",
    "# Create a list with 20 dataframes that have only one column named dummy_exog with n rows and all nan\n",
    "exog_dummy = [pd.DataFrame(data=np.nan, columns=['dummy_exog'], index=pd.date_range('2018-01-01', periods=n)) for i in range(20)]\n",
    "\n",
    "# Create a list with 20 dataframes with 3 columns and n rows and random values\n",
    "exog_1 = [pd.DataFrame(data=np.random.randn(n, 3), columns=['exog_0', 'exog_1', 'exog_2'], index=pd.date_range('2018-01-01', periods=n)) for i in range(20)]\n",
    "exog_2 = [pd.DataFrame(data=np.random.randn(n, 3), columns=['exog_3', 'exog_4', 'exog_5'], index=pd.date_range('2018-01-01', periods=n)) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 ms ± 12 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pd.concat(exog_dummy + exog_1 + exog_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= 10_000\n",
    "\n",
    "# Create a list with 20 dataframes with columns from exog_0 to exog_5 and n rows and nan\n",
    "exog_dummy = [pd.DataFrame(data=np.nan, columns=['exog_0', 'exog_1', 'exog_2', 'exog_3', 'exog_4', 'exog_5'], index=pd.date_range('2018-01-01', periods=n)) for i in range(20)]\n",
    "\n",
    "# create a list with 20 dataframes with columns from exog_0 to exog_2 with random values and columns from exog_3 to exog_5 with nan\n",
    "exog_1 = []\n",
    "for i in range(20):\n",
    "    df = pd.DataFrame(data=np.random.randn(n, 3), columns=['exog_0', 'exog_1', 'exog_2'], index=pd.date_range('2018-01-01', periods=n))\n",
    "    df['exog_3'] = np.nan\n",
    "    df['exog_4'] = np.nan\n",
    "    df['exog_5'] = np.nan\n",
    "    df = df[['exog_0', 'exog_1', 'exog_2', 'exog_3', 'exog_4', 'exog_5']]\n",
    "    exog_1.append(df)\n",
    "\n",
    "# create a list with 20 dataframes with columns from exog_3 to exog_5 with random values and columns from exog_0 to exog_2 with nan\n",
    "exog_2 = []\n",
    "for i in range(20):\n",
    "    df = pd.DataFrame(data=np.random.randn(n, 3), columns=['exog_3', 'exog_4', 'exog_5'], index=pd.date_range('2018-01-01', periods=n))\n",
    "    df['exog_0'] = np.nan\n",
    "    df['exog_1'] = np.nan\n",
    "    df['exog_2'] = np.nan\n",
    "    # sort columns\n",
    "    df = df[['exog_0', 'exog_1', 'exog_2', 'exog_3', 'exog_4', 'exog_5']]\n",
    "    exog_2.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.2 ms ± 46.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pd.concat(exog_dummy + exog_1 + exog_2, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78d62c1713fdacd99ef7c429003c7324b36fbb551fb8b6860a7ea73e9338235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
