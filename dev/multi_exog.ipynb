{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/varios/skforecast'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "str(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Union, Tuple, Optional\n",
    "\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_0</th>\n",
       "      <th>series_1</th>\n",
       "      <th>series_2</th>\n",
       "      <th>series_3</th>\n",
       "      <th>series_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.261850</td>\n",
       "      <td>0.087870</td>\n",
       "      <td>0.088626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.345499</td>\n",
       "      <td>0.230780</td>\n",
       "      <td>0.952178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>1.867116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.873486</td>\n",
       "      <td>-0.305502</td>\n",
       "      <td>0.526883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.159279</td>\n",
       "      <td>0.304625</td>\n",
       "      <td>-1.417519</td>\n",
       "      <td>0.246185</td>\n",
       "      <td>0.688534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>1.484221</td>\n",
       "      <td>-0.005817</td>\n",
       "      <td>-2.142832</td>\n",
       "      <td>-1.651898</td>\n",
       "      <td>0.738103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>-0.688957</td>\n",
       "      <td>-0.141787</td>\n",
       "      <td>-1.449811</td>\n",
       "      <td>-2.236662</td>\n",
       "      <td>-1.016556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.060116</td>\n",
       "      <td>-0.295795</td>\n",
       "      <td>0.786142</td>\n",
       "      <td>1.108431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>-1.218385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003603</td>\n",
       "      <td>-1.835462</td>\n",
       "      <td>-1.498089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>0.363553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.095959</td>\n",
       "      <td>0.255753</td>\n",
       "      <td>-0.215410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>1.851692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.747716</td>\n",
       "      <td>0.387752</td>\n",
       "      <td>0.588002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            series_0  series_1  series_2  series_3  series_4\n",
       "2018-01-01       NaN       NaN -1.261850  0.087870  0.088626\n",
       "2018-01-02       NaN       NaN -0.345499  0.230780  0.952178\n",
       "2018-01-03  1.867116       NaN  1.873486 -0.305502  0.526883\n",
       "2018-01-04  1.159279  0.304625 -1.417519  0.246185  0.688534\n",
       "2018-01-05  1.484221 -0.005817 -2.142832 -1.651898  0.738103\n",
       "2018-01-06 -0.688957 -0.141787 -1.449811 -2.236662 -1.016556\n",
       "2018-01-07  0.061100  0.060116 -0.295795  0.786142  1.108431\n",
       "2018-01-08 -1.218385       NaN -0.003603 -1.835462 -1.498089\n",
       "2018-01-09  0.363553       NaN  1.095959  0.255753 -0.215410\n",
       "2018-01-10  1.851692       NaN -0.747716  0.387752  0.588002"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data simulation\n",
    "# ==============================================================================\n",
    "n = 10\n",
    "series = pd.DataFrame(\n",
    "    np.random.randn(n, 5),\n",
    "    index=pd.date_range(\"2018-01-01\", periods=n),\n",
    "    columns=[f\"series_{i}\" for i in range(5)],\n",
    ")\n",
    "\n",
    "exog = {\n",
    "    f\"series_{i}\": pd.DataFrame(\n",
    "        np.random.randn(n, 2),\n",
    "        index=pd.date_range(\"2018-01-01\", periods=n),\n",
    "        columns=[f\"exog_{j}\" for j in range(2)],\n",
    "    )\n",
    "    for i in range(5)\n",
    "}\n",
    "\n",
    "# exog = pd.DataFrame(\n",
    "#     np.random.randn(n, 2),\n",
    "#     index=pd.date_range(\"2018-01-01\", periods=n),\n",
    "#     columns=[\"exog_0\", \"exog_1\"],\n",
    "# )\n",
    "\n",
    "# Injecting missing values\n",
    "series['series_0'].iloc[0:2] = np.nan\n",
    "series['series_1'].iloc[0:3] = np.nan\n",
    "series['series_1'].iloc[-3:] = np.nan\n",
    "# series['series_3'].iloc[[5, 6]] = np.nan Needs to remove the exception in create_train_X_y when there are missing values in y\n",
    "series\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=LinearRegression(),\n",
    "    lags=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series lengths after removing leading and trailing nans\n",
      "series_0: 8\n",
      "series_1: 4\n",
      "series_2: 10\n",
      "series_3: 10\n",
      "series_4: 10\n",
      "Exog lengths after removing leading and trailing nans\n",
      "series_0: 10\n",
      "series_1: 10\n",
      "series_2: 10\n",
      "series_3: 10\n",
      "series_4: 10\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# ==============================================================================\n",
    "# Store series and exog as dict\n",
    "if isinstance(series, pd.DataFrame):\n",
    "    series_dict = series.to_dict(\"series\")\n",
    "elif isinstance(series, dict):\n",
    "    series_dict = series\n",
    "\n",
    "if isinstance(exog, pd.DataFrame):\n",
    "    exog_dict = dict.fromkeys(series_dict.keys(), exog)\n",
    "elif isinstance(exog, dict):\n",
    "    exog_dict = exog\n",
    "\n",
    "# Remove leading and trailing nans from each series and exog. This is done\n",
    "# so then there is no need to remove leading and trailing nans in each X_train\n",
    "for k, v in series_dict.items():\n",
    "    series_dict[k] = v.loc[v.first_valid_index():v.last_valid_index()]\n",
    "\n",
    "for k, v in exog_dict.items():\n",
    "    exog_dict[k] = v.loc[v.first_valid_index():v.last_valid_index()]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Series lengths after removing leading and trailing nans\")\n",
    "for k, v in series_dict.items():\n",
    "    print(f\"{k}: {len(v)}\")\n",
    "\n",
    "print(\"Exog lengths after removing leading and trailing nans\")\n",
    "for k, v in exog_dict.items():\n",
    "    print(f\"{k}: {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "series_0\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_1\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_2\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_3\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n",
      "series_4\n",
      "Index(['exog_0', 'exog_1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_train_buffer = []\n",
    "y_train_buffer = []\n",
    "\n",
    "for key in series_dict.keys():\n",
    "    y = series_dict[key]\n",
    "    exog = exog_dict[key]\n",
    "    # TODO: All neded check goes here\n",
    "    exog = exog.loc[y.index]\n",
    "    print(y.name)\n",
    "    print(exog.columns)\n",
    "    X_train, y_train = forecaster.create_train_X_y(y=y, exog=exog)\n",
    "    X_train['level'] = key\n",
    "    X_train_buffer.append(X_train)\n",
    "    y_train_buffer.append(y_train)\n",
    "\n",
    "X_train = pd.concat(X_train_buffer, axis=0)\n",
    "y_train = pd.concat(y_train_buffer, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>exog_0</th>\n",
       "      <th>exog_1</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>1.484221</td>\n",
       "      <td>1.159279</td>\n",
       "      <td>1.867116</td>\n",
       "      <td>0.305531</td>\n",
       "      <td>2.765781</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>-0.688957</td>\n",
       "      <td>1.484221</td>\n",
       "      <td>1.159279</td>\n",
       "      <td>-0.536736</td>\n",
       "      <td>2.537970</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.061100</td>\n",
       "      <td>-0.688957</td>\n",
       "      <td>1.484221</td>\n",
       "      <td>-1.245000</td>\n",
       "      <td>0.063485</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-1.218385</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>-0.688957</td>\n",
       "      <td>1.450337</td>\n",
       "      <td>0.921844</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>0.363553</td>\n",
       "      <td>-1.218385</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>-0.109622</td>\n",
       "      <td>1.267858</td>\n",
       "      <td>series_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>-0.141787</td>\n",
       "      <td>-0.005817</td>\n",
       "      <td>0.304625</td>\n",
       "      <td>0.289228</td>\n",
       "      <td>-0.469021</td>\n",
       "      <td>series_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>1.873486</td>\n",
       "      <td>-0.345499</td>\n",
       "      <td>-1.261850</td>\n",
       "      <td>0.299628</td>\n",
       "      <td>1.389228</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>-1.417519</td>\n",
       "      <td>1.873486</td>\n",
       "      <td>-0.345499</td>\n",
       "      <td>-0.293398</td>\n",
       "      <td>0.644642</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>-2.142832</td>\n",
       "      <td>-1.417519</td>\n",
       "      <td>1.873486</td>\n",
       "      <td>1.994853</td>\n",
       "      <td>-1.037329</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>-1.449811</td>\n",
       "      <td>-2.142832</td>\n",
       "      <td>-1.417519</td>\n",
       "      <td>-1.557848</td>\n",
       "      <td>-1.369620</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>-0.295795</td>\n",
       "      <td>-1.449811</td>\n",
       "      <td>-2.142832</td>\n",
       "      <td>-0.670605</td>\n",
       "      <td>-0.640973</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-0.003603</td>\n",
       "      <td>-0.295795</td>\n",
       "      <td>-1.449811</td>\n",
       "      <td>-0.409193</td>\n",
       "      <td>0.096741</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>1.095959</td>\n",
       "      <td>-0.003603</td>\n",
       "      <td>-0.295795</td>\n",
       "      <td>1.354525</td>\n",
       "      <td>2.038312</td>\n",
       "      <td>series_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>-0.305502</td>\n",
       "      <td>0.230780</td>\n",
       "      <td>0.087870</td>\n",
       "      <td>0.944997</td>\n",
       "      <td>-1.236085</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.246185</td>\n",
       "      <td>-0.305502</td>\n",
       "      <td>0.230780</td>\n",
       "      <td>-1.193344</td>\n",
       "      <td>0.387887</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>-1.651898</td>\n",
       "      <td>0.246185</td>\n",
       "      <td>-0.305502</td>\n",
       "      <td>-1.461914</td>\n",
       "      <td>-0.835621</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>-2.236662</td>\n",
       "      <td>-1.651898</td>\n",
       "      <td>0.246185</td>\n",
       "      <td>-0.118662</td>\n",
       "      <td>0.483965</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>0.786142</td>\n",
       "      <td>-2.236662</td>\n",
       "      <td>-1.651898</td>\n",
       "      <td>1.150880</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-1.835462</td>\n",
       "      <td>0.786142</td>\n",
       "      <td>-2.236662</td>\n",
       "      <td>0.505757</td>\n",
       "      <td>-0.183986</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>0.255753</td>\n",
       "      <td>-1.835462</td>\n",
       "      <td>0.786142</td>\n",
       "      <td>0.082649</td>\n",
       "      <td>0.457513</td>\n",
       "      <td>series_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>0.526883</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>0.088626</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.718610</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.688534</td>\n",
       "      <td>0.526883</td>\n",
       "      <td>0.952178</td>\n",
       "      <td>1.312518</td>\n",
       "      <td>0.309126</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-06</th>\n",
       "      <td>0.738103</td>\n",
       "      <td>0.688534</td>\n",
       "      <td>0.526883</td>\n",
       "      <td>-0.398914</td>\n",
       "      <td>-1.080987</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-07</th>\n",
       "      <td>-1.016556</td>\n",
       "      <td>0.738103</td>\n",
       "      <td>0.688534</td>\n",
       "      <td>-0.949255</td>\n",
       "      <td>-0.090064</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>1.108431</td>\n",
       "      <td>-1.016556</td>\n",
       "      <td>0.738103</td>\n",
       "      <td>-1.409975</td>\n",
       "      <td>0.178429</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-1.498089</td>\n",
       "      <td>1.108431</td>\n",
       "      <td>-1.016556</td>\n",
       "      <td>-0.305480</td>\n",
       "      <td>-0.766063</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-10</th>\n",
       "      <td>-0.215410</td>\n",
       "      <td>-1.498089</td>\n",
       "      <td>1.108431</td>\n",
       "      <td>-0.120856</td>\n",
       "      <td>-1.268852</td>\n",
       "      <td>series_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lag_1     lag_2     lag_3    exog_0    exog_1     level\n",
       "2018-01-06  1.484221  1.159279  1.867116  0.305531  2.765781  series_0\n",
       "2018-01-07 -0.688957  1.484221  1.159279 -0.536736  2.537970  series_0\n",
       "2018-01-08  0.061100 -0.688957  1.484221 -1.245000  0.063485  series_0\n",
       "2018-01-09 -1.218385  0.061100 -0.688957  1.450337  0.921844  series_0\n",
       "2018-01-10  0.363553 -1.218385  0.061100 -0.109622  1.267858  series_0\n",
       "2018-01-07 -0.141787 -0.005817  0.304625  0.289228 -0.469021  series_1\n",
       "2018-01-04  1.873486 -0.345499 -1.261850  0.299628  1.389228  series_2\n",
       "2018-01-05 -1.417519  1.873486 -0.345499 -0.293398  0.644642  series_2\n",
       "2018-01-06 -2.142832 -1.417519  1.873486  1.994853 -1.037329  series_2\n",
       "2018-01-07 -1.449811 -2.142832 -1.417519 -1.557848 -1.369620  series_2\n",
       "2018-01-08 -0.295795 -1.449811 -2.142832 -0.670605 -0.640973  series_2\n",
       "2018-01-09 -0.003603 -0.295795 -1.449811 -0.409193  0.096741  series_2\n",
       "2018-01-10  1.095959 -0.003603 -0.295795  1.354525  2.038312  series_2\n",
       "2018-01-04 -0.305502  0.230780  0.087870  0.944997 -1.236085  series_3\n",
       "2018-01-05  0.246185 -0.305502  0.230780 -1.193344  0.387887  series_3\n",
       "2018-01-06 -1.651898  0.246185 -0.305502 -1.461914 -0.835621  series_3\n",
       "2018-01-07 -2.236662 -1.651898  0.246185 -0.118662  0.483965  series_3\n",
       "2018-01-08  0.786142 -2.236662 -1.651898  1.150880  0.002962  series_3\n",
       "2018-01-09 -1.835462  0.786142 -2.236662  0.505757 -0.183986  series_3\n",
       "2018-01-10  0.255753 -1.835462  0.786142  0.082649  0.457513  series_3\n",
       "2018-01-04  0.526883  0.952178  0.088626  0.000063  0.718610  series_4\n",
       "2018-01-05  0.688534  0.526883  0.952178  1.312518  0.309126  series_4\n",
       "2018-01-06  0.738103  0.688534  0.526883 -0.398914 -1.080987  series_4\n",
       "2018-01-07 -1.016556  0.738103  0.688534 -0.949255 -0.090064  series_4\n",
       "2018-01-08  1.108431 -1.016556  0.738103 -1.409975  0.178429  series_4\n",
       "2018-01-09 -1.498089  1.108431 -1.016556 -0.305480 -0.766063  series_4\n",
       "2018-01-10 -0.215410 -1.498089  1.108431 -0.120856 -1.268852  series_4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-01-06   -0.688957\n",
       "2018-01-07    0.061100\n",
       "2018-01-08   -1.218385\n",
       "2018-01-09    0.363553\n",
       "2018-01-10    1.851692\n",
       "2018-01-07    0.060116\n",
       "2018-01-04   -1.417519\n",
       "2018-01-05   -2.142832\n",
       "2018-01-06   -1.449811\n",
       "2018-01-07   -0.295795\n",
       "2018-01-08   -0.003603\n",
       "2018-01-09    1.095959\n",
       "2018-01-10   -0.747716\n",
       "2018-01-04    0.246185\n",
       "2018-01-05   -1.651898\n",
       "2018-01-06   -2.236662\n",
       "2018-01-07    0.786142\n",
       "2018-01-08   -1.835462\n",
       "2018-01-09    0.255753\n",
       "2018-01-10    0.387752\n",
       "2018-01-04    0.688534\n",
       "2018-01-05    0.738103\n",
       "2018-01-06   -1.016556\n",
       "2018-01-07    1.108431\n",
       "2018-01-08   -1.498089\n",
       "2018-01-09   -0.215410\n",
       "2018-01-10    0.588002\n",
       "Name: aaaaa, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_train_X_y_single_series(\n",
    "    self,\n",
    "    y: pd.Series,\n",
    "    exog: Optional[Union[pd.Series, pd.DataFrame]]=None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Create training matrices from univariate time series and exogenous\n",
    "    variables. This method does not transform the exog variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : pandas Series\n",
    "        Training time series.\n",
    "    exog : pandas Series, pandas DataFrame, default `None`\n",
    "        Exogenous variable/s included as predictor/s. Must have the same\n",
    "        number of observations as `y` and their indexes must be aligned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train_lags : pandas DataFrame\n",
    "        Training values of lags\n",
    "        Shape: (len(y) - self.max_lag, len(self.lags))\n",
    "    X_train_exog : pandas DataFrame\n",
    "        Training values of exogenous variables.\n",
    "        Shape: (len(y) - self.max_lag, len(exog.columns))\n",
    "    y_train : pandas Series\n",
    "        Values (target) of the time series related to each row of `X_train`.\n",
    "        Shape: (len(y) - self.max_lag, )\n",
    "    \n",
    "    \"\"\"\n",
    "    y_name = y.name\n",
    "    check_y(y=y)\n",
    "    y = transform_series(\n",
    "            series            = y,\n",
    "            transformer       = self.transformer_y,\n",
    "            fit               = True,\n",
    "            inverse_transform = False\n",
    "        )\n",
    "    y_values, y_index = preprocess_y(y=y)\n",
    "\n",
    "    if self.differentiation is not None:\n",
    "        y_values = self.differentiator_[y_name].fit_transform(y_values)\n",
    "    \n",
    "    if exog is not None:\n",
    "        if len(exog) != len(y):\n",
    "            raise ValueError(\n",
    "                (f'`exog` must have same number of samples as `y`. '\n",
    "                    f'length `exog`: ({len(exog)}), length `y`: ({len(y)})')\n",
    "            )\n",
    "        check_exog(exog=exog, allow_nan=True)\n",
    "        check_exog_dtypes(exog)\n",
    "        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n",
    "        if not (exog_index[:len(y_index)] == y_index).all():\n",
    "            raise ValueError(\n",
    "                (\"Different index for `y` and `exog`. They must be equal to \"\n",
    "                \"ensure the correct alignment of values.\")\n",
    "            )\n",
    "    \n",
    "    X_train, y_train = self._create_lags(y=y_values)\n",
    "    X_train_col_names = [f\"lag_{i}\" for i in self.lags]\n",
    "    X_train_lags = pd.DataFrame(\n",
    "                        data    = X_train,\n",
    "                        columns = X_train_col_names,\n",
    "                        index   = y_index[self.max_lag: ]\n",
    "                   )\n",
    "\n",
    "    X_train_exog = None\n",
    "    if exog is not None:\n",
    "        # The first `self.max_lag` positions have to be removed from exog\n",
    "        # since they are not in X_train.\n",
    "        X_train_exog = exog.iloc[self.max_lag:, ]\n",
    "        X_train_exog.index = exog_index[self.max_lag:]\n",
    "        X_train_exog = X_train_exog.loc[X_train_lags.index, :] # TODO: is this nedeed?\n",
    "\n",
    "    y_train = pd.Series(\n",
    "                    data  = y_train,\n",
    "                    index = y_index[self.max_lag: ],\n",
    "                    name  = 'y'\n",
    "                )\n",
    "\n",
    "    if self.differentiation is not None:\n",
    "        y_train = y_train.iloc[self.differentiation: ]\n",
    "        X_train_lags = X_train_lags.iloc[self.differentiation: ]\n",
    "        if X_train_exog is not None:\n",
    "            X_train_exog = X_train_exog.iloc[self.differentiation: ]\n",
    "                    \n",
    "    return X_train_lags, X_train_exog, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_X_y(\n",
    "        self,\n",
    "        series: Union[pd.DataFrame, dict],\n",
    "        exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None\n",
    "    ) -> Tuple[pd.DataFrame, pd.Series, pd.Index, pd.Index]:\n",
    "        \"\"\"\n",
    "        Create training matrices from multiple time series and exogenous\n",
    "        variables.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        series : pandas DataFrame, dict\n",
    "            Training time series.\n",
    "        exog : pandas Series, pandas DataFrame, dict, default `None`\n",
    "            Exogenous variable/s included as predictor/s. Must have the same\n",
    "            number of observations as `series` and their indexes must be aligned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train : pandas DataFrame\n",
    "            Training values (predictors).\n",
    "        y_train : pandas Series\n",
    "            Values (target) of the time series related to each row of `X_train`.\n",
    "            Shape: (len(series) - self.max_lag, )\n",
    "        y_index : pandas Index\n",
    "            Index of `series`.\n",
    "        y_train_index: pandas Index\n",
    "            Index of `y_train`.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(series, (pd.DataFrame, dict)):\n",
    "            raise TypeError(f\"`series` must be a pandas DataFrame or dict. Got {type(series)}.\")\n",
    "        \n",
    "        if not isinstance(exog, (pd.Series, pd.DataFrame, dict, type(None))):\n",
    "            raise TypeError(f\"`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.\")\n",
    "\n",
    "        if isinstance(series, pd.DataFrame):\n",
    "            series_dict = series.to_dict(\"series\")\n",
    "        elif isinstance(series, dict):\n",
    "            series_dict = series\n",
    "\n",
    "        if isinstance(exog, pd.DataFrame):\n",
    "            exog_dict = dict.fromkeys(series_dict.keys(), exog)\n",
    "        elif isinstance(exog, dict):\n",
    "            exog_dict = exog\n",
    "\n",
    "        series_names = list(series_dict.keys())\n",
    "        if self.transformer_series is None:\n",
    "            self.transformer_series_ = {serie: None for serie in series_names}\n",
    "        elif not isinstance(self.transformer_series, dict):\n",
    "            self.transformer_series_ = {serie: clone(self.transformer_series) \n",
    "                                        for serie in series_names}\n",
    "        else:\n",
    "            self.transformer_series_ = {serie: None for serie in series_names}\n",
    "            # Only elements already present in transformer_series_ are updated\n",
    "            self.transformer_series_.update(\n",
    "                (k, v) for k, v in deepcopy(self.transformer_series).items() \n",
    "                if k in self.transformer_series_\n",
    "            )\n",
    "            series_not_in_transformer_series = set(series.columns) - set(self.transformer_series.keys())\n",
    "            if series_not_in_transformer_series:\n",
    "                warnings.warn(\n",
    "                    (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n",
    "                     f\" No transformation is applied to these series.\"),\n",
    "                     IgnoredArgumentWarning\n",
    "                )  \n",
    "        if self.differentiation is None:\n",
    "            self.differentiator_ = {serie: None for serie in series_names}\n",
    "        else:\n",
    "            self.differentiator_ = {serie: clone(self.self.differentiator) for serie in series_names}\n",
    "\n",
    "\n",
    "        # Remove leading and trailing nans from each series and exog.\n",
    "        for k, v in series_dict.items():\n",
    "            series_dict[k] = v.loc[v.first_valid_index():v.last_valid_index()]\n",
    "        if exog is not None:\n",
    "            for k, v in exog_dict.items():\n",
    "                v = v.loc[v.first_valid_index():v.last_valid_index()]\n",
    "                if isinstance(v, pd.Series):\n",
    "                    v = pd.DataFrame(v)\n",
    "                exog_dict[k]\n",
    "\n",
    "        # TODO: check all series have the same type of index and frequency\n",
    "        # TODO: check that same exogs in diferent series have the same data types (important for later concatenation)\n",
    "        \n",
    "        exog_names = [exog.columns.to_list() for exog in exog_dict.values()]\n",
    "        exog_names = list(set([item for sublist in exog_names for item in sublist]))\n",
    "        X_train_lags_buffer = []\n",
    "        X_train_exog_buffer = []\n",
    "        y_train_buffer = []\n",
    "\n",
    "        for key in series_dict.keys():\n",
    "            y = series_dict[key]\n",
    "            exog = exog_dict[key]\n",
    "            exog = exog.loc[y.index]\n",
    "            exog = exog.reindex(y.index, fill_value=np.nan)\n",
    "            X_train_lags, X_train_exog, y_train = (\n",
    "                forecaster._create_train_X_y_single_series(y=y, exog=exog)\n",
    "            )\n",
    "            X_train_lags['level'] = key\n",
    "            X_train_lags_buffer.append(X_train_lags)\n",
    "            X_train_exog_buffer.append(X_train_exog)\n",
    "            y_train_buffer.append(y_train)\n",
    "\n",
    "        X_train_lags = pd.concat(X_train_lags_buffer, axis=0)\n",
    "        y_train = pd.concat(y_train_buffer, axis=0)\n",
    "        if exog is not None:\n",
    "            X_train_exog = pd.concat(X_train_exog_buffer, axis=0)\n",
    "            X_train_exog = transform_dataframe(\n",
    "                                df                = X_train_exog,\n",
    "                                transformer       = self.transformer_exog,\n",
    "                                fit               = True,\n",
    "                                inverse_transform = False\n",
    "                            )\n",
    "            check_exog(exog=X_train_exog, allow_nan=False)\n",
    "            check_exog_dtypes(X_train_exog)\n",
    "            self.exog_dtypes = get_exog_dtypes(exog=X_train_exog)\n",
    "            if not (X_train_exog.index == X_train_lags).all():\n",
    "                raise ValueError(\n",
    "                    (\"Different index for `y` and `exog`. They must be equal \"\n",
    "                    \"to ensure the correct alignment of values.\")\n",
    "                )\n",
    "            X_train = pd.concat([X_train_lags, X_train_exog], axis=1)\n",
    "        else:\n",
    "            X_train = X_train_lags\n",
    "\n",
    "        self.X_train_col_names = X_train.columns.to_list()\n",
    "        y_train_index = y_train.index.to_numpy()\n",
    "\n",
    "\n",
    "        return X_train, y_train, y_index, y_train_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78d62c1713fdacd99ef7c429003c7324b36fbb551fb8b6860a7ea73e9338235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
