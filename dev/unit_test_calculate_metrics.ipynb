{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, str(Path.cwd().parent))\n",
    "str(Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skforecast.datasets import fetch_dataset\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import bayesian_search_forecaster_multiseries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skforecast.metrics import mean_absolute_scaled_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skforecast.datasets import fetch_dataset\n",
    "from skforecast.model_selection_multiseries.model_selection_multiseries import _calculate_metrics_multiseries\n",
    "from skforecast.metrics import add_y_train_argument\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skforecast.metrics import mean_absolute_scaled_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    data={\n",
    "        \"item_1\": [\n",
    "            8.253175, 22.777826, 27.549099, 25.895533, 21.379238, 21.106643,\n",
    "            20.533871, 20.069327, 20.006161, 21.620184, 21.717691, 21.751748,\n",
    "            21.758617, 20.784194, 18.976196, 20.228468, 26.636444, 29.245869,\n",
    "            24.772249, 24.018768, 22.503533, 20.794986, 23.981037, 28.018830,\n",
    "            28.747482, 23.908368, 21.423930, 24.786455, 24.615778, 27.388275,\n",
    "            25.724191, 22.825491, 23.066582, 23.788066, 23.360304, 23.119966,\n",
    "            21.763739, 23.008517, 22.861086, 22.807790, 23.424717, 22.208947,\n",
    "            19.558775, 20.788390, 23.619240, 25.061150, 27.646380, 25.609772,\n",
    "            22.504042, 20.838095\n",
    "        ],\n",
    "        \"item_2\": [\n",
    "            21.047727, 26.578125, 31.751042, 24.567708, 18.191667, 17.812500,\n",
    "            19.510417, 24.098958, 20.223958, 19.161458, 16.042708, 14.815625,\n",
    "            17.031250, 17.009375, 17.096875, 19.255208, 28.060417, 28.779167,\n",
    "            19.265625, 19.178125, 19.688542, 21.690625, 25.332292, 26.675000,\n",
    "            26.611458, 19.759375, 20.038542, 24.680208, 25.032292, 28.111458,\n",
    "            21.542708, 16.605208, 18.593750, 20.667708, 21.977083, 29.040625,\n",
    "            18.979167, 18.459375, 17.295833, 17.282292, 20.844792, 19.858333,\n",
    "            18.446875, 19.239583, 19.903125, 22.970833, 28.195833, 20.221875,\n",
    "            19.176042, 21.991667\n",
    "        ],\n",
    "        \"item_3\": [\n",
    "            19.429739, 28.009863, 32.078922, 27.252276, 20.357737, 19.879148,\n",
    "            18.043499, 26.287368, 16.315997, 21.772584, 18.729748, 12.552534,\n",
    "            18.996209, 18.534327, 15.418361, 16.304852, 30.076258, 28.886334,\n",
    "            20.286651, 21.367727, 20.248170, 19.799975, 25.931558, 27.698196,\n",
    "            30.725005, 19.573577, 23.310162, 24.959233, 24.399246, 29.094136,\n",
    "            22.639513, 18.372362, 21.256450, 22.430527, 19.575067, 31.767626,\n",
    "            20.086271, 21.380186, 17.553807, 17.369879, 21.829746, 16.208510,\n",
    "            25.067215, 21.863615, 17.887458, 23.005424, 25.013939, 22.142083,\n",
    "            23.673005, 25.238480\n",
    "        ],\n",
    "    },\n",
    "    index=pd.date_range(start=\"2012-01-01\", end=\"2012-02-19\"),\n",
    ")\n",
    "\n",
    "\n",
    "predictions_different_lenght = pd.DataFrame(\n",
    "    data={\n",
    "        \"item_1\": [\n",
    "            25.849411, 24.507137, 23.885447, 23.597504, 23.464140, 23.402371,\n",
    "            23.373762, 23.360511, 23.354374, 23.351532, 23.354278, 23.351487,\n",
    "            23.350195, 23.349596, 23.349319, 23.349190, 23.349131, 23.349103,\n",
    "            23.349090, 23.349084, 23.474207, 23.407034, 23.375922, 23.361512,\n",
    "            23.354837\n",
    "        ],\n",
    "        \"item_2\": [\n",
    "            24.561460, 23.611980, 23.172218, 22.968536, 22.874199, 22.830506,\n",
    "            22.810269, 22.800896, 22.796555, 22.794544, 22.414996, 22.617821,\n",
    "            22.711761, 22.755271, 22.775423, 22.784756, 22.789079, 22.791082,\n",
    "            22.792009, 22.792439, 21.454419, 22.172918, 22.505700, 22.659831,\n",
    "            22.731219\n",
    "        ],\n",
    "        \"item_3\": [\n",
    "            26.168069, 24.057472, 23.079925, 22.627163, 22.417461, 22.320335,\n",
    "            22.275350, 22.254515, 22.244865, 22.240395, 21.003848, 21.665604,\n",
    "            np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,\n",
    "            np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "        ],\n",
    "    },\n",
    "    index=pd.date_range(start=\"2012-01-26\", periods=25)\n",
    ")\n",
    "\n",
    "\n",
    "span_index = span_index = pd.date_range(start=\"2012-01-01\", end=\"2012-02-19\", freq=\"D\")\n",
    "\n",
    "folds = [\n",
    "    [[0, 25], [24, 25], [25, 35], [25, 35], False],\n",
    "    [[0, 25], [34, 35], [35, 45], [35, 45], False],\n",
    "    [[0, 25], [44, 45], [45, 50], [45, 50], False],\n",
    "]\n",
    "\n",
    "levels = [\"item_1\", \"item_2\", \"item_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calculate_metrics_multiseries_output_when_aggregated_metric_and_predictions_have_different_length(\n",
    "    metrics=[mean_absolute_error, mean_absolute_scaled_error]\n",
    "):\n",
    "\n",
    "    metrics = [add_y_train_argument(metric) for metric in metrics]\n",
    "    results = _calculate_metrics_multiseries(\n",
    "        series=data,\n",
    "        predictions=predictions_different_lenght,\n",
    "        folds=folds,\n",
    "        span_index=span_index,\n",
    "        metrics=metrics,\n",
    "        levels=levels,\n",
    "        add_aggregated_metric=True,\n",
    "    )\n",
    "\n",
    "    expected = pd.DataFrame(\n",
    "        data={\n",
    "            \"levels\": [\n",
    "                \"item_1\",\n",
    "                \"item_2\",\n",
    "                \"item_3\",\n",
    "                \"average\",\n",
    "                \"weighted_average\",\n",
    "                \"pooling\",\n",
    "            ],\n",
    "            \"mean_absolute_error\": [\n",
    "                1.477567,\n",
    "                3.480129,\n",
    "                3.173683,\n",
    "                2.710460,\n",
    "                2.613332,\n",
    "                2.613332,\n",
    "            ],\n",
    "            \"mean_absolute_scaled_error\": [\n",
    "                0.610914,\n",
    "                1.170113,\n",
    "                0.707757,\n",
    "                0.829595,\n",
    "                0.855141,\n",
    "                0.793768,\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    display(results)\n",
    "    pd.testing.assert_frame_equal(results, expected)\n",
    "\n",
    "\n",
    "test_calculate_metrics_multiseries_output_when_aggregated_metric_and_predictions_have_different_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.iloc[25:] - predictions).abs().stack().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average([1.477567, 3.480129, 3.173683], weights=[25, 25, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test backtesting_forecaster_multiseries\n",
    "# ==============================================================================\n",
    "import re\n",
    "import pytest\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "from skforecast.exceptions import IgnoredArgumentWarning\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregMultiSeries import ForecasterAutoregMultiSeries\n",
    "from skforecast.ForecasterAutoregMultiSeriesCustom import ForecasterAutoregMultiSeriesCustom\n",
    "from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multivariate\n",
    "from skforecast.model_selection_multiseries.model_selection_multiseries import _bayesian_search_optuna_multiseries\n",
    "\n",
    "\n",
    "# Fixtures\n",
    "from skforecast.model_selection_multiseries.tests.fixtures_model_selection_multiseries import series\n",
    "THIS_DIR = Path(\"/home/ubuntu/varios/skforecast/skforecast/model_selection_multiseries/tests\")\n",
    "series_dict = joblib.load(THIS_DIR/'fixture_sample_multi_series.joblib')\n",
    "exog_dict = joblib.load(THIS_DIR/'fixture_sample_multi_series_exog.joblib')\n",
    "end_train = \"2016-07-31 23:59:00\"\n",
    "series_dict_train = {k: v.loc[:end_train,] for k, v in series_dict.items()}\n",
    "exog_dict_train = {k: v.loc[:end_train,] for k, v in exog_dict.items()}\n",
    "series_dict_test = {k: v.loc[end_train:,] for k, v in series_dict.items()}\n",
    "exog_dict_test = {k: v.loc[end_train:,] for k, v in exog_dict.items()}\n",
    "series_with_nans = series.copy()\n",
    "series_with_nans.iloc[:10, series_with_nans.columns.get_loc('l2')] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 models compared for 2 level(s). Number of iterations: 6.\n"
     ]
    }
   ],
   "source": [
    "def test_output_evaluate_grid_hyperparameters_multiseries_ForecasterAutoregMultiSeries_multiple_metrics_aggregated_with_mocked():\n",
    "    \"\"\"\n",
    "    Test output of _evaluate_grid_hyperparameters_multiseries in ForecasterAutoregMultiSeries \n",
    "    with mocked when multiple metrics (mocked done in Skforecast v0.6.0).\n",
    "    \"\"\"\n",
    "    forecaster = ForecasterAutoregMultiSeries(\n",
    "                     regressor          = Ridge(random_state=123),\n",
    "                     lags               = 2, \n",
    "                     encoding           = 'onehot',\n",
    "                     transformer_series = None\n",
    "                 )\n",
    "\n",
    "    steps = 3\n",
    "    n_validation = 12\n",
    "    lags_grid = [2, 4]\n",
    "    param_grid = [{'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
    "\n",
    "    results = _evaluate_grid_hyperparameters_multiseries(\n",
    "                  forecaster         = forecaster,\n",
    "                  series             = series,\n",
    "                  param_grid         = param_grid,\n",
    "                  steps              = steps,\n",
    "                  metric             = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                  aggregate_metric   = ['weighted_average', 'average', 'pooling'],\n",
    "                  initial_train_size = len(series) - n_validation,\n",
    "                  fixed_train_size   = False,\n",
    "                  levels             = None,\n",
    "                  exog               = None,\n",
    "                  lags_grid          = lags_grid,\n",
    "                  refit              = False,\n",
    "                  return_best        = False,\n",
    "                  verbose            = False,\n",
    "                  show_progress     = False,\n",
    "              )\n",
    "    \n",
    "    expected_results = pd.DataFrame({\n",
    "        \"levels\": {\n",
    "            0: [\"l1\", \"l2\"],\n",
    "            1: [\"l1\", \"l2\"],\n",
    "            2: [\"l1\", \"l2\"],\n",
    "            3: [\"l1\", \"l2\"],\n",
    "            4: [\"l1\", \"l2\"],\n",
    "            5: [\"l1\", \"l2\"],\n",
    "        },\n",
    "        \"lags\": {\n",
    "            0: np.array([1, 2, 3, 4]),\n",
    "            1: np.array([1, 2, 3, 4]),\n",
    "            2: np.array([1, 2, 3, 4]),\n",
    "            3: np.array([1, 2]),\n",
    "            4: np.array([1, 2]),\n",
    "            5: np.array([1, 2]),\n",
    "        },\n",
    "        \"lags_label\": {\n",
    "            0: np.array([1, 2, 3, 4]),\n",
    "            1: np.array([1, 2, 3, 4]),\n",
    "            2: np.array([1, 2, 3, 4]),\n",
    "            3: np.array([1, 2]),\n",
    "            4: np.array([1, 2]),\n",
    "            5: np.array([1, 2]),\n",
    "        },\n",
    "        \"params\": {\n",
    "            0: {\"alpha\": 0.01},\n",
    "            1: {\"alpha\": 0.1},\n",
    "            2: {\"alpha\": 1},\n",
    "            3: {\"alpha\": 1},\n",
    "            4: {\"alpha\": 0.1},\n",
    "            5: {\"alpha\": 0.01},\n",
    "        },\n",
    "        \"mean_absolute_error__weighted_average\": {\n",
    "            0: 0.20968100547390048,\n",
    "            1: 0.20969259864077977,\n",
    "            2: 0.20977945397058564,\n",
    "            3: 0.21077344921320568,\n",
    "            4: 0.21078653208835063,\n",
    "            5: 0.21078779920557153,\n",
    "        },\n",
    "        \"mean_absolute_error__average\": {\n",
    "            0: 0.20968100547390048,\n",
    "            1: 0.20969259864077974,\n",
    "            2: 0.20977945397058564,\n",
    "            3: 0.21077344921320565,\n",
    "            4: 0.21078653208835063,\n",
    "            5: 0.21078779920557153,\n",
    "        },\n",
    "        \"mean_absolute_error__pooling\": {\n",
    "            0: 0.20968100547390045,\n",
    "            1: 0.2096925986407798,\n",
    "            2: 0.20977945397058564,\n",
    "            3: 0.21077344921320565,\n",
    "            4: 0.21078653208835063,\n",
    "            5: 0.21078779920557153,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__weighted_average\": {\n",
    "            0: 0.7969369551529275,\n",
    "            1: 0.7969838748911608,\n",
    "            2: 0.7973389652448446,\n",
    "            3: 0.8009631048212882,\n",
    "            4: 0.8009302953795885,\n",
    "            5: 0.8009249124659391,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__average\": {\n",
    "            0: 0.7969369551529275,\n",
    "            1: 0.7969838748911608,\n",
    "            2: 0.7973389652448445,\n",
    "            3: 0.8009631048212883,\n",
    "            4: 0.8009302953795885,\n",
    "            5: 0.8009249124659391,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__pooling\": {\n",
    "            0: 0.7809734688246502,\n",
    "            1: 0.7810166484905049,\n",
    "            2: 0.7813401480275807,\n",
    "            3: 0.7850423618302551,\n",
    "            4: 0.785091090032226,\n",
    "            5: 0.7850958095104122,\n",
    "        },\n",
    "        \"alpha\": {0: 0.01, 1: 0.1, 2: 1.0, 3: 1.0, 4: 0.1, 5: 0.01},\n",
    "    })\n",
    "\n",
    "    pd.testing.assert_frame_equal(results, expected_results)\n",
    "\n",
    "test_output_evaluate_grid_hyperparameters_multiseries_ForecasterAutoregMultiSeries_multiple_metrics_aggregated_with_mocked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                    regressor          = Ridge(random_state=123),\n",
    "                    lags               = 2, \n",
    "                    encoding           = 'onehot',\n",
    "                    transformer_series = None\n",
    "                )\n",
    "\n",
    "steps = 3\n",
    "n_validation = 12\n",
    "lags_grid = [2, 4]\n",
    "param_grid = [{'alpha': 0.01}, {'alpha': 0.1}, {'alpha': 1}]\n",
    "\n",
    "results = _evaluate_grid_hyperparameters_multiseries(\n",
    "                forecaster         = forecaster,\n",
    "                series             = series,\n",
    "                param_grid         = param_grid,\n",
    "                steps              = steps,\n",
    "                metric             = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                aggregate_metric   = ['weighted_average', 'average', 'pooling'],\n",
    "                initial_train_size = len(series) - n_validation,\n",
    "                fixed_train_size   = False,\n",
    "                levels             = None,\n",
    "                exog               = None,\n",
    "                lags_grid          = lags_grid,\n",
    "                refit              = False,\n",
    "                return_best        = False,\n",
    "                verbose            = False,\n",
    "                show_progress     = False\n",
    "            )\n",
    "\n",
    "results.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "        \"levels\": {\n",
    "            0: [\"l1\", \"l2\"],\n",
    "            1: [\"l1\", \"l2\"],\n",
    "            2: [\"l1\", \"l2\"],\n",
    "            3: [\"l1\", \"l2\"],\n",
    "            4: [\"l1\", \"l2\"],\n",
    "            5: [\"l1\", \"l2\"],\n",
    "        },\n",
    "        \"lags\": {\n",
    "            0: np.array([1, 2, 3, 4]),\n",
    "            1: np.array([1, 2, 3, 4]),\n",
    "            2: np.array([1, 2, 3, 4]),\n",
    "            3: np.array([1, 2]),\n",
    "            4: np.array([1, 2]),\n",
    "            5: np.array([1, 2]),\n",
    "        },\n",
    "        \"lags_label\": {\n",
    "            0: np.array([1, 2, 3, 4]),\n",
    "            1: np.array([1, 2, 3, 4]),\n",
    "            2: np.array([1, 2, 3, 4]),\n",
    "            3: np.array([1, 2]),\n",
    "            4: np.array([1, 2]),\n",
    "            5: np.array([1, 2]),\n",
    "        },\n",
    "        \"params\": {\n",
    "            0: {\"alpha\": 0.01},\n",
    "            1: {\"alpha\": 0.1},\n",
    "            2: {\"alpha\": 1},\n",
    "            3: {\"alpha\": 1},\n",
    "            4: {\"alpha\": 0.1},\n",
    "            5: {\"alpha\": 0.01},\n",
    "        },\n",
    "        \"mean_absolute_error__weighted_average\": {\n",
    "            0: 0.20968100547390048,\n",
    "            1: 0.20969259864077977,\n",
    "            2: 0.20977945397058564,\n",
    "            3: 0.21077344921320568,\n",
    "            4: 0.21078653208835063,\n",
    "            5: 0.21078779920557153,\n",
    "        },\n",
    "        \"mean_absolute_error__average\": {\n",
    "            0: 0.20968100547390048,\n",
    "            1: 0.20969259864077974,\n",
    "            2: 0.20977945397058564,\n",
    "            3: 0.21077344921320565,\n",
    "            4: 0.21078653208835063,\n",
    "            5: 0.21078779920557153,\n",
    "        },\n",
    "        \"mean_absolute_error__pooling\": {\n",
    "            0: 0.20968100547390045,\n",
    "            1: 0.2096925986407798,\n",
    "            2: 0.20977945397058564,\n",
    "            3: 0.21077344921320565,\n",
    "            4: 0.21078653208835063,\n",
    "            5: 0.21078779920557153,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__weighted_average\": {\n",
    "            0: 0.7969369551529275,\n",
    "            1: 0.7969838748911608,\n",
    "            2: 0.7973389652448446,\n",
    "            3: 0.8009631048212882,\n",
    "            4: 0.8009302953795885,\n",
    "            5: 0.8009249124659391,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__average\": {\n",
    "            0: 0.7969369551529275,\n",
    "            1: 0.7969838748911608,\n",
    "            2: 0.7973389652448445,\n",
    "            3: 0.8009631048212883,\n",
    "            4: 0.8009302953795885,\n",
    "            5: 0.8009249124659391,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__pooling\": {\n",
    "            0: 0.7809734688246502,\n",
    "            1: 0.7810166484905049,\n",
    "            2: 0.7813401480275807,\n",
    "            3: 0.7850423618302551,\n",
    "            4: 0.785091090032226,\n",
    "            5: 0.7850958095104122,\n",
    "        },\n",
    "        \"alpha\": {0: 0.01, 1: 0.1, 2: 1.0, 3: 1.0, 4: 0.1, 5: 0.01},\n",
    "    }).equals(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 models compared for 2 level(s). Number of iterations: 6.\n"
     ]
    }
   ],
   "source": [
    "def test_output_grid_search_forecaster_multiseries_ForecasterAutoregMultiSeries_with_mocked():\n",
    "    \"\"\"\n",
    "    Test output of grid_search_forecaster_multiseries in ForecasterAutoregMultiSeries \n",
    "    with mocked (mocked done in Skforecast v0.5.0)\n",
    "    \"\"\"\n",
    "    forecaster = ForecasterAutoregMultiSeries(\n",
    "                     regressor          = Ridge(random_state=123),\n",
    "                     lags               = 2, \n",
    "                     encoding           = 'onehot',\n",
    "                     transformer_series = None\n",
    "                 )\n",
    "\n",
    "    steps = 3\n",
    "    n_validation = 12\n",
    "    lags_grid = [2, 4]\n",
    "    param_grid = {'alpha': [0.01, 0.1, 1]}\n",
    "\n",
    "    results = grid_search_forecaster_multiseries(\n",
    "                  forecaster          = forecaster,\n",
    "                  series              = series,\n",
    "                  param_grid          = param_grid,\n",
    "                  steps               = steps,\n",
    "                  metric              = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                  aggregate_metric   = ['weighted_average', 'average', 'pooling'],\n",
    "                  initial_train_size  = len(series) - n_validation,\n",
    "                  fixed_train_size    = False,\n",
    "                  levels              = None,\n",
    "                  exog                = None,\n",
    "                  lags_grid           = lags_grid,\n",
    "                  refit               = False,\n",
    "                  return_best         = False,\n",
    "                  verbose             = False,\n",
    "                  show_progress       = False,\n",
    "              )\n",
    "    \n",
    "    expected_results = pd.DataFrame({\n",
    "        'levels': {0: ['l1', 'l2'],\n",
    "        1: ['l1', 'l2'],\n",
    "        2: ['l1', 'l2'],\n",
    "        3: ['l1', 'l2'],\n",
    "        4: ['l1', 'l2'],\n",
    "        5: ['l1', 'l2']},\n",
    "        'lags': {0: np.array([1, 2, 3, 4]),\n",
    "        1: np.array([1, 2, 3, 4]),\n",
    "        2: np.array([1, 2, 3, 4]),\n",
    "        3: np.array([1, 2]),\n",
    "        4: np.array([1, 2]),\n",
    "        5: np.array([1, 2])},\n",
    "        'lags_label': {0: np.array([1, 2, 3, 4]),\n",
    "        1: np.array([1, 2, 3, 4]),\n",
    "        2: np.array([1, 2, 3, 4]),\n",
    "        3: np.array([1, 2]),\n",
    "        4: np.array([1, 2]),\n",
    "        5: np.array([1, 2])},\n",
    "        'params': {0: {'alpha': 0.01},\n",
    "        1: {'alpha': 0.1},\n",
    "        2: {'alpha': 1},\n",
    "        3: {'alpha': 1},\n",
    "        4: {'alpha': 0.1},\n",
    "        5: {'alpha': 0.01}},\n",
    "        'mean_absolute_error__weighted_average': {0: 0.20968100547390048,\n",
    "        1: 0.20969259864077977,\n",
    "        2: 0.20977945397058564,\n",
    "        3: 0.21077344921320568,\n",
    "        4: 0.21078653208835063,\n",
    "        5: 0.21078779920557153},\n",
    "        'mean_absolute_error__average': {0: 0.20968100547390048,\n",
    "        1: 0.20969259864077974,\n",
    "        2: 0.20977945397058564,\n",
    "        3: 0.21077344921320565,\n",
    "        4: 0.21078653208835063,\n",
    "        5: 0.21078779920557153},\n",
    "        'mean_absolute_error__pooling': {0: 0.20968100547390045,\n",
    "        1: 0.2096925986407798,\n",
    "        2: 0.20977945397058564,\n",
    "        3: 0.21077344921320565,\n",
    "        4: 0.21078653208835063,\n",
    "        5: 0.21078779920557153},\n",
    "        'mean_absolute_scaled_error__weighted_average': {0: 0.7969369551529275,\n",
    "        1: 0.7969838748911608,\n",
    "        2: 0.7973389652448446,\n",
    "        3: 0.8009631048212882,\n",
    "        4: 0.8009302953795885,\n",
    "        5: 0.8009249124659391},\n",
    "        'mean_absolute_scaled_error__average': {0: 0.7969369551529275,\n",
    "        1: 0.7969838748911608,\n",
    "        2: 0.7973389652448445,\n",
    "        3: 0.8009631048212883,\n",
    "        4: 0.8009302953795885,\n",
    "        5: 0.8009249124659391},\n",
    "        'mean_absolute_scaled_error__pooling': {0: 0.7809734688246502,\n",
    "        1: 0.7810166484905049,\n",
    "        2: 0.7813401480275807,\n",
    "        3: 0.7850423618302551,\n",
    "        4: 0.785091090032226,\n",
    "        5: 0.7850958095104122},\n",
    "        'alpha': {0: 0.01, 1: 0.1, 2: 1.0, 3: 1.0, 4: 0.1, 5: 0.01}\n",
    "    })\n",
    "\n",
    "    pd.testing.assert_frame_equal(results, expected_results)\n",
    "\n",
    "test_output_grid_search_forecaster_multiseries_ForecasterAutoregMultiSeries_with_mocked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/varios/skforecast/skforecast/ForecasterAutoregMultiSeries/ForecasterAutoregMultiSeries.py:383: UserWarning: When using a linear model, it is recommended to use a transformer_series to ensure all series are in the same scale. You can use, for example, a `StandardScaler` from sklearn.preprocessing.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 models compared for 2 level(s). Number of iterations: 6.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>levels</th>\n",
       "      <th>lags</th>\n",
       "      <th>lags_label</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_absolute_error__weighted_average</th>\n",
       "      <th>mean_absolute_error__average</th>\n",
       "      <th>mean_absolute_error__pooling</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>0.209681</td>\n",
       "      <td>0.209681</td>\n",
       "      <td>0.209681</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.209693</td>\n",
       "      <td>0.209693</td>\n",
       "      <td>0.209693</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>0.209779</td>\n",
       "      <td>0.209779</td>\n",
       "      <td>0.209779</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>0.210773</td>\n",
       "      <td>0.210773</td>\n",
       "      <td>0.210773</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.210787</td>\n",
       "      <td>0.210787</td>\n",
       "      <td>0.210787</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>0.210788</td>\n",
       "      <td>0.210788</td>\n",
       "      <td>0.210788</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     levels          lags    lags_label           params  \\\n",
       "0  [l1, l2]  [1, 2, 3, 4]  [1, 2, 3, 4]  {'alpha': 0.01}   \n",
       "1  [l1, l2]  [1, 2, 3, 4]  [1, 2, 3, 4]   {'alpha': 0.1}   \n",
       "2  [l1, l2]  [1, 2, 3, 4]  [1, 2, 3, 4]     {'alpha': 1}   \n",
       "3  [l1, l2]        [1, 2]        [1, 2]     {'alpha': 1}   \n",
       "4  [l1, l2]        [1, 2]        [1, 2]   {'alpha': 0.1}   \n",
       "5  [l1, l2]        [1, 2]        [1, 2]  {'alpha': 0.01}   \n",
       "\n",
       "   mean_absolute_error__weighted_average  mean_absolute_error__average  \\\n",
       "0                               0.209681                      0.209681   \n",
       "1                               0.209693                      0.209693   \n",
       "2                               0.209779                      0.209779   \n",
       "3                               0.210773                      0.210773   \n",
       "4                               0.210787                      0.210787   \n",
       "5                               0.210788                      0.210788   \n",
       "\n",
       "   mean_absolute_error__pooling  alpha  \n",
       "0                      0.209681   0.01  \n",
       "1                      0.209693   0.10  \n",
       "2                      0.209779   1.00  \n",
       "3                      0.210773   1.00  \n",
       "4                      0.210787   0.10  \n",
       "5                      0.210788   0.01  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                    regressor          = Ridge(random_state=123),\n",
    "                    lags               = 2, \n",
    "                    encoding           = 'onehot',\n",
    "                    transformer_series = None\n",
    "                )\n",
    "\n",
    "steps = 3\n",
    "n_validation = 12\n",
    "lags_grid = [2, 4]\n",
    "param_grid = {'alpha': [0.01, 0.1, 1]}\n",
    "\n",
    "results = grid_search_forecaster_multiseries(\n",
    "                forecaster          = forecaster,\n",
    "                series              = series,\n",
    "                param_grid          = param_grid,\n",
    "                steps               = steps,\n",
    "                metric              = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                aggregate_metric    = ['weighted_average', 'average', 'pooling'],\n",
    "                initial_train_size  = len(series) - n_validation,\n",
    "                fixed_train_size    = False,\n",
    "                levels              = None,\n",
    "                exog                = None,\n",
    "                lags_grid           = lags_grid,\n",
    "                refit               = False,\n",
    "                return_best         = False,\n",
    "                verbose             = False,\n",
    "                show_progress       = False,\n",
    "            )\n",
    "\n",
    "results.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_output_bayesian_search_forecaster_multiseries_series_and_exog_dict_multiple_metrics_aggregated_with_mocked():\n",
    "    \"\"\"\n",
    "    Test output of bayesian_search_forecaster_multiseries in ForecasterAutoregMultiSeries\n",
    "    and ForecasterAutoregMultiSeriesCustom when series and exog are dictionaries and \n",
    "    multiple aggregated metrics (mocked done in Skforecast v0.12.0).\n",
    "    \"\"\"\n",
    "\n",
    "    forecaster = ForecasterAutoregMultiSeries(\n",
    "        regressor=LGBMRegressor(\n",
    "            n_estimators=2, random_state=123, verbose=-1, max_depth=2\n",
    "        ),\n",
    "        lags=14,\n",
    "        encoding=\"ordinal\",\n",
    "        dropna_from_series=False,\n",
    "        transformer_series=StandardScaler(),\n",
    "        transformer_exog=StandardScaler(),\n",
    "    )\n",
    "\n",
    "    lags_grid = [[5], [1, 7, 14]]\n",
    "\n",
    "    def search_space(trial):\n",
    "        search_space = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 5),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "            \"lags\": trial.suggest_categorical(\"lags\", lags_grid),\n",
    "        }\n",
    "\n",
    "        return search_space\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "        results_search, _ = bayesian_search_forecaster_multiseries(\n",
    "            forecaster=forecaster,\n",
    "            series=series_dict,\n",
    "            exog=exog_dict,\n",
    "            search_space=search_space,\n",
    "            metric=['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "            aggregate_metric=['weighted_average', 'average', 'pooling'],\n",
    "            initial_train_size=len(series_dict_train[\"id_1000\"]),\n",
    "            steps=24,\n",
    "            refit=False,\n",
    "            n_trials=3,\n",
    "            return_best=False,\n",
    "            show_progress=False,\n",
    "            verbose=False,\n",
    "            suppress_warnings=True,\n",
    "        )\n",
    "\n",
    "    expected = pd.DataFrame({\n",
    "                    \"levels\": {\n",
    "                        0: [\"id_1000\", \"id_1001\", \"id_1002\", \"id_1003\", \"id_1004\"],\n",
    "                        1: [\"id_1000\", \"id_1001\", \"id_1002\", \"id_1003\", \"id_1004\"],\n",
    "                        2: [\"id_1000\", \"id_1001\", \"id_1002\", \"id_1003\", \"id_1004\"],\n",
    "                    },\n",
    "                    \"lags\": {0: np.array([1, 7, 14]), 1: np.array([1, 7, 14]), 2: np.array([5])},\n",
    "                    \"params\": {\n",
    "                        0: {\"n_estimators\": 4, \"max_depth\": 3},\n",
    "                        1: {\"n_estimators\": 3, \"max_depth\": 3},\n",
    "                        2: {\"n_estimators\": 4, \"max_depth\": 3},\n",
    "                    },\n",
    "                    \"mean_absolute_error__weighted_average\": {\n",
    "                        0: 749.8761502029433,\n",
    "                        1: 760.659082077477,\n",
    "                        2: 777.6874712018467,\n",
    "                    },\n",
    "                    \"mean_absolute_error__average\": {\n",
    "                        0: 709.8836514262415,\n",
    "                        1: 721.1848222120482,\n",
    "                        2: 754.3537196425694,\n",
    "                    },\n",
    "                    \"mean_absolute_error__pooling\": {\n",
    "                        0: 709.8836514262414,\n",
    "                        1: 721.1848222120483,\n",
    "                        2: 754.3537196425694,\n",
    "                    },\n",
    "                    \"mean_absolute_scaled_error__weighted_average\": {\n",
    "                        0: 1.7214020008755428,\n",
    "                        1: 1.7480018562024022,\n",
    "                        2: 1.8159623522099073,\n",
    "                    },\n",
    "                    \"mean_absolute_scaled_error__average\": {\n",
    "                        0: 2.0671251354627653,\n",
    "                        1: 2.099920474995049,\n",
    "                        2: 2.1945202856306465,\n",
    "                    },\n",
    "                    \"mean_absolute_scaled_error__pooling\": {\n",
    "                        0: 1.6864677542505797,\n",
    "                        1: 1.7133159005309597,\n",
    "                        2: 1.7921151176255248,\n",
    "                    },\n",
    "                    \"n_estimators\": {0: 4, 1: 3, 2: 4},\n",
    "                    \"max_depth\": {0: 3, 1: 3, 2: 3},\n",
    "                })\n",
    "\n",
    "    pd.testing.assert_frame_equal(expected, results_search)\n",
    "\n",
    "\n",
    "test_output_bayesian_search_forecaster_multiseries_series_and_exog_dict_multiple_metrics_aggregated_with_mocked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>levels</th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_absolute_error__weighted_average</th>\n",
       "      <th>mean_absolute_error__average</th>\n",
       "      <th>mean_absolute_error__pooling</th>\n",
       "      <th>mean_absolute_scaled_error__weighted_average</th>\n",
       "      <th>mean_absolute_scaled_error__average</th>\n",
       "      <th>mean_absolute_scaled_error__pooling</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[id_1000, id_1001, id_1002, id_1003, id_1004]</td>\n",
       "      <td>[1, 7, 14]</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 3}</td>\n",
       "      <td>749.876150</td>\n",
       "      <td>709.883651</td>\n",
       "      <td>709.883651</td>\n",
       "      <td>1.721402</td>\n",
       "      <td>2.067125</td>\n",
       "      <td>1.686468</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[id_1000, id_1001, id_1002, id_1003, id_1004]</td>\n",
       "      <td>[1, 7, 14]</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 3}</td>\n",
       "      <td>760.659082</td>\n",
       "      <td>721.184822</td>\n",
       "      <td>721.184822</td>\n",
       "      <td>1.748002</td>\n",
       "      <td>2.099920</td>\n",
       "      <td>1.713316</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[id_1000, id_1001, id_1002, id_1003, id_1004]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 3}</td>\n",
       "      <td>777.687471</td>\n",
       "      <td>754.353720</td>\n",
       "      <td>754.353720</td>\n",
       "      <td>1.815962</td>\n",
       "      <td>2.194520</td>\n",
       "      <td>1.792115</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          levels        lags  \\\n",
       "0  [id_1000, id_1001, id_1002, id_1003, id_1004]  [1, 7, 14]   \n",
       "1  [id_1000, id_1001, id_1002, id_1003, id_1004]  [1, 7, 14]   \n",
       "2  [id_1000, id_1001, id_1002, id_1003, id_1004]         [5]   \n",
       "\n",
       "                                params  mean_absolute_error__weighted_average  \\\n",
       "0  {'n_estimators': 4, 'max_depth': 3}                             749.876150   \n",
       "1  {'n_estimators': 3, 'max_depth': 3}                             760.659082   \n",
       "2  {'n_estimators': 4, 'max_depth': 3}                             777.687471   \n",
       "\n",
       "   mean_absolute_error__average  mean_absolute_error__pooling  \\\n",
       "0                    709.883651                    709.883651   \n",
       "1                    721.184822                    721.184822   \n",
       "2                    754.353720                    754.353720   \n",
       "\n",
       "   mean_absolute_scaled_error__weighted_average  \\\n",
       "0                                      1.721402   \n",
       "1                                      1.748002   \n",
       "2                                      1.815962   \n",
       "\n",
       "   mean_absolute_scaled_error__average  mean_absolute_scaled_error__pooling  \\\n",
       "0                             2.067125                             1.686468   \n",
       "1                             2.099920                             1.713316   \n",
       "2                             2.194520                             1.792115   \n",
       "\n",
       "   n_estimators  max_depth  \n",
       "0             4          3  \n",
       "1             3          3  \n",
       "2             4          3  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "    regressor=LGBMRegressor(\n",
    "        n_estimators=2, random_state=123, verbose=-1, max_depth=2\n",
    "    ),\n",
    "    lags=14,\n",
    "    encoding=\"ordinal\",\n",
    "    dropna_from_series=False,\n",
    "    transformer_series=StandardScaler(),\n",
    "    transformer_exog=StandardScaler(),\n",
    ")\n",
    "\n",
    "lags_grid = [[5], [1, 7, 14]]\n",
    "\n",
    "def search_space(trial):\n",
    "    search_space = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 5),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n",
    "        \"lags\": trial.suggest_categorical(\"lags\", lags_grid),\n",
    "    }\n",
    "\n",
    "    return search_space\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "    results_search, best_trial = bayesian_search_forecaster_multiseries(\n",
    "        forecaster=forecaster,\n",
    "        series=series_dict,\n",
    "        exog=exog_dict,\n",
    "        search_space=search_space,\n",
    "        metric=['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "        aggregate_metric=['weighted_average', 'average', 'pooling'],\n",
    "        initial_train_size=len(series_dict_train[\"id_1000\"]),\n",
    "        steps=24,\n",
    "        refit=False,\n",
    "        n_trials=3,\n",
    "        return_best=False,\n",
    "        show_progress=False,\n",
    "        verbose=False,\n",
    "        suppress_warnings=True,\n",
    "    )\n",
    "\n",
    "results_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aab4517a0944f2b9e56bf2a8d2956d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_results_output_bayesian_search_optuna_multiseries_ForecasterAutoregMultiSeries_with_multiple_metrics_aggregated():\n",
    "    \"\"\"\n",
    "    Test output of _bayesian_search_optuna_multiseries in ForecasterAutoregMultiSeries\n",
    "    with multiple metrics and aggregated metrics (mocked done in skforecast v0.12.0).\n",
    "    \"\"\"\n",
    "    forecaster = ForecasterAutoregMultiSeries(\n",
    "                     regressor = Ridge(random_state=123),\n",
    "                     lags      = 2,\n",
    "                     encoding  = 'onehot',\n",
    "                     transformer_series = StandardScaler()\n",
    "                 )\n",
    "\n",
    "    steps = 3\n",
    "    n_validation = 12\n",
    "\n",
    "    def search_space(trial):\n",
    "        search_space  = {\n",
    "            'alpha': trial.suggest_float('alpha', 1e-2, 1.0),\n",
    "            'lags' : trial.suggest_categorical('lags', [2, 4])\n",
    "        }\n",
    "        \n",
    "        return search_space\n",
    "\n",
    "    results = _bayesian_search_optuna_multiseries(\n",
    "                  forecaster         = forecaster,\n",
    "                  series             = series,\n",
    "                  steps              = steps,\n",
    "                  search_space       = search_space,\n",
    "                  metric             = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                  aggregate_metric   = ['weighted_average', 'average', 'pooling'],\n",
    "                  refit              = True,\n",
    "                  initial_train_size = len(series) - n_validation,\n",
    "                  fixed_train_size   = True,\n",
    "                  n_trials           = 10,\n",
    "                  random_state       = 123,\n",
    "                  return_best        = False,\n",
    "                  verbose            = False\n",
    "              )[0]\n",
    "    \n",
    "    expected_results = pd.DataFrame({\n",
    "        \"levels\": {\n",
    "            0: [\"l1\", \"l2\"],\n",
    "            1: [\"l1\", \"l2\"],\n",
    "            2: [\"l1\", \"l2\"],\n",
    "            3: [\"l1\", \"l2\"],\n",
    "            4: [\"l1\", \"l2\"],\n",
    "            5: [\"l1\", \"l2\"],\n",
    "            6: [\"l1\", \"l2\"],\n",
    "            7: [\"l1\", \"l2\"],\n",
    "            8: [\"l1\", \"l2\"],\n",
    "            9: [\"l1\", \"l2\"],\n",
    "        },\n",
    "        \"lags\": {\n",
    "            0: np.array([1, 2]),\n",
    "            1: np.array([1, 2]),\n",
    "            2: np.array([1, 2]),\n",
    "            3: np.array([1, 2]),\n",
    "            4: np.array([1, 2]),\n",
    "            5: np.array([1, 2, 3, 4]),\n",
    "            6: np.array([1, 2, 3, 4]),\n",
    "            7: np.array([1, 2, 3, 4]),\n",
    "            8: np.array([1, 2, 3, 4]),\n",
    "            9: np.array([1, 2, 3, 4]),\n",
    "        },\n",
    "        \"params\": {\n",
    "            0: {\"alpha\": 0.5558016213920624},\n",
    "            1: {\"alpha\": 0.6995044937418831},\n",
    "            2: {\"alpha\": 0.7406154516747153},\n",
    "            3: {\"alpha\": 0.8509374761370117},\n",
    "            4: {\"alpha\": 0.9809565564007693},\n",
    "            5: {\"alpha\": 0.23598059857016607},\n",
    "            6: {\"alpha\": 0.398196343012209},\n",
    "            7: {\"alpha\": 0.4441865222328282},\n",
    "            8: {\"alpha\": 0.53623586010342},\n",
    "            9: {\"alpha\": 0.7252189487445193},\n",
    "        },\n",
    "        \"mean_absolute_error__weighted_average\": {\n",
    "            0: 0.21324663796176382,\n",
    "            1: 0.2132571094660072,\n",
    "            2: 0.21326009091608622,\n",
    "            3: 0.21326806055662118,\n",
    "            4: 0.2132773952926551,\n",
    "            5: 0.21476196207156512,\n",
    "            6: 0.21477679099211167,\n",
    "            7: 0.21478095843883202,\n",
    "            8: 0.2147892513261171,\n",
    "            9: 0.21480607764821474,\n",
    "        },\n",
    "        \"mean_absolute_error__average\": {\n",
    "            0: 0.21324663796176382,\n",
    "            1: 0.21325710946600718,\n",
    "            2: 0.21326009091608622,\n",
    "            3: 0.21326806055662115,\n",
    "            4: 0.2132773952926551,\n",
    "            5: 0.21476196207156514,\n",
    "            6: 0.21477679099211167,\n",
    "            7: 0.21478095843883202,\n",
    "            8: 0.21478925132611706,\n",
    "            9: 0.21480607764821472,\n",
    "        },\n",
    "        \"mean_absolute_error__pooling\": {\n",
    "            0: 0.21324663796176382,\n",
    "            1: 0.21325710946600726,\n",
    "            2: 0.21326009091608622,\n",
    "            3: 0.21326806055662115,\n",
    "            4: 0.21327739529265513,\n",
    "            5: 0.21476196207156514,\n",
    "            6: 0.21477679099211167,\n",
    "            7: 0.21478095843883202,\n",
    "            8: 0.21478925132611706,\n",
    "            9: 0.21480607764821472,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__weighted_average\": {\n",
    "            0: 0.7850408674109868,\n",
    "            1: 0.7850769204162228,\n",
    "            2: 0.7850871846200614,\n",
    "            3: 0.7851146198219785,\n",
    "            4: 0.7851467509994321,\n",
    "            5: 0.7897394420953061,\n",
    "            6: 0.7897964331491425,\n",
    "            7: 0.7898124493861658,\n",
    "            8: 0.7898443200957621,\n",
    "            9: 0.7899089846155858,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__average\": {\n",
    "            0: 0.7850408674109867,\n",
    "            1: 0.7850769204162228,\n",
    "            2: 0.7850871846200613,\n",
    "            3: 0.7851146198219786,\n",
    "            4: 0.785146750999432,\n",
    "            5: 0.7897394420953061,\n",
    "            6: 0.7897964331491426,\n",
    "            7: 0.7898124493861657,\n",
    "            8: 0.7898443200957622,\n",
    "            9: 0.7899089846155857,\n",
    "        },\n",
    "        \"mean_absolute_scaled_error__pooling\": {\n",
    "            0: 0.7724806513327171,\n",
    "            1: 0.7725185840968428,\n",
    "            2: 0.7725293843257277,\n",
    "            3: 0.7725582541506879,\n",
    "            4: 0.7725920690001996,\n",
    "            5: 0.7779698752966109,\n",
    "            6: 0.7780235926931046,\n",
    "            7: 0.7780386891653759,\n",
    "            8: 0.7780687299436625,\n",
    "            9: 0.7781296828776818,\n",
    "        },\n",
    "        \"alpha\": {\n",
    "            0: 0.5558016213920624,\n",
    "            1: 0.6995044937418831,\n",
    "            2: 0.7406154516747153,\n",
    "            3: 0.8509374761370117,\n",
    "            4: 0.9809565564007693,\n",
    "            5: 0.23598059857016607,\n",
    "            6: 0.398196343012209,\n",
    "            7: 0.4441865222328282,\n",
    "            8: 0.53623586010342,\n",
    "            9: 0.7252189487445193,\n",
    "        },\n",
    "    })\n",
    "\n",
    "    pd.testing.assert_frame_equal(results, expected_results, check_dtype=False)\n",
    "\n",
    "\n",
    "test_results_output_bayesian_search_optuna_multiseries_ForecasterAutoregMultiSeries_with_multiple_metrics_aggregated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3de407f6ad348639d12e9a703b5cc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>levels</th>\n",
       "      <th>lags</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_absolute_error__weighted_average</th>\n",
       "      <th>mean_absolute_error__average</th>\n",
       "      <th>mean_absolute_error__pooling</th>\n",
       "      <th>mean_absolute_scaled_error__weighted_average</th>\n",
       "      <th>mean_absolute_scaled_error__average</th>\n",
       "      <th>mean_absolute_scaled_error__pooling</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.5558016213920624}</td>\n",
       "      <td>0.213247</td>\n",
       "      <td>0.213247</td>\n",
       "      <td>0.213247</td>\n",
       "      <td>0.785041</td>\n",
       "      <td>0.785041</td>\n",
       "      <td>0.772481</td>\n",
       "      <td>0.555802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.6995044937418831}</td>\n",
       "      <td>0.213257</td>\n",
       "      <td>0.213257</td>\n",
       "      <td>0.213257</td>\n",
       "      <td>0.785077</td>\n",
       "      <td>0.785077</td>\n",
       "      <td>0.772519</td>\n",
       "      <td>0.699504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.7406154516747153}</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.785087</td>\n",
       "      <td>0.785087</td>\n",
       "      <td>0.772529</td>\n",
       "      <td>0.740615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.8509374761370117}</td>\n",
       "      <td>0.213268</td>\n",
       "      <td>0.213268</td>\n",
       "      <td>0.213268</td>\n",
       "      <td>0.785115</td>\n",
       "      <td>0.785115</td>\n",
       "      <td>0.772558</td>\n",
       "      <td>0.850937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>{'alpha': 0.9809565564007693}</td>\n",
       "      <td>0.213277</td>\n",
       "      <td>0.213277</td>\n",
       "      <td>0.213277</td>\n",
       "      <td>0.785147</td>\n",
       "      <td>0.785147</td>\n",
       "      <td>0.772592</td>\n",
       "      <td>0.980957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.23598059857016607}</td>\n",
       "      <td>0.214762</td>\n",
       "      <td>0.214762</td>\n",
       "      <td>0.214762</td>\n",
       "      <td>0.789739</td>\n",
       "      <td>0.789739</td>\n",
       "      <td>0.777970</td>\n",
       "      <td>0.235981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.398196343012209}</td>\n",
       "      <td>0.214777</td>\n",
       "      <td>0.214777</td>\n",
       "      <td>0.214777</td>\n",
       "      <td>0.789796</td>\n",
       "      <td>0.789796</td>\n",
       "      <td>0.778024</td>\n",
       "      <td>0.398196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.4441865222328282}</td>\n",
       "      <td>0.214781</td>\n",
       "      <td>0.214781</td>\n",
       "      <td>0.214781</td>\n",
       "      <td>0.789812</td>\n",
       "      <td>0.789812</td>\n",
       "      <td>0.778039</td>\n",
       "      <td>0.444187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.53623586010342}</td>\n",
       "      <td>0.214789</td>\n",
       "      <td>0.214789</td>\n",
       "      <td>0.214789</td>\n",
       "      <td>0.789844</td>\n",
       "      <td>0.789844</td>\n",
       "      <td>0.778069</td>\n",
       "      <td>0.536236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[l1, l2]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>{'alpha': 0.7252189487445193}</td>\n",
       "      <td>0.214806</td>\n",
       "      <td>0.214806</td>\n",
       "      <td>0.214806</td>\n",
       "      <td>0.789909</td>\n",
       "      <td>0.789909</td>\n",
       "      <td>0.778130</td>\n",
       "      <td>0.725219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     levels          lags                          params  \\\n",
       "0  [l1, l2]        [1, 2]   {'alpha': 0.5558016213920624}   \n",
       "1  [l1, l2]        [1, 2]   {'alpha': 0.6995044937418831}   \n",
       "2  [l1, l2]        [1, 2]   {'alpha': 0.7406154516747153}   \n",
       "3  [l1, l2]        [1, 2]   {'alpha': 0.8509374761370117}   \n",
       "4  [l1, l2]        [1, 2]   {'alpha': 0.9809565564007693}   \n",
       "5  [l1, l2]  [1, 2, 3, 4]  {'alpha': 0.23598059857016607}   \n",
       "6  [l1, l2]  [1, 2, 3, 4]    {'alpha': 0.398196343012209}   \n",
       "7  [l1, l2]  [1, 2, 3, 4]   {'alpha': 0.4441865222328282}   \n",
       "8  [l1, l2]  [1, 2, 3, 4]     {'alpha': 0.53623586010342}   \n",
       "9  [l1, l2]  [1, 2, 3, 4]   {'alpha': 0.7252189487445193}   \n",
       "\n",
       "   mean_absolute_error__weighted_average  mean_absolute_error__average  \\\n",
       "0                               0.213247                      0.213247   \n",
       "1                               0.213257                      0.213257   \n",
       "2                               0.213260                      0.213260   \n",
       "3                               0.213268                      0.213268   \n",
       "4                               0.213277                      0.213277   \n",
       "5                               0.214762                      0.214762   \n",
       "6                               0.214777                      0.214777   \n",
       "7                               0.214781                      0.214781   \n",
       "8                               0.214789                      0.214789   \n",
       "9                               0.214806                      0.214806   \n",
       "\n",
       "   mean_absolute_error__pooling  mean_absolute_scaled_error__weighted_average  \\\n",
       "0                      0.213247                                      0.785041   \n",
       "1                      0.213257                                      0.785077   \n",
       "2                      0.213260                                      0.785087   \n",
       "3                      0.213268                                      0.785115   \n",
       "4                      0.213277                                      0.785147   \n",
       "5                      0.214762                                      0.789739   \n",
       "6                      0.214777                                      0.789796   \n",
       "7                      0.214781                                      0.789812   \n",
       "8                      0.214789                                      0.789844   \n",
       "9                      0.214806                                      0.789909   \n",
       "\n",
       "   mean_absolute_scaled_error__average  mean_absolute_scaled_error__pooling  \\\n",
       "0                             0.785041                             0.772481   \n",
       "1                             0.785077                             0.772519   \n",
       "2                             0.785087                             0.772529   \n",
       "3                             0.785115                             0.772558   \n",
       "4                             0.785147                             0.772592   \n",
       "5                             0.789739                             0.777970   \n",
       "6                             0.789796                             0.778024   \n",
       "7                             0.789812                             0.778039   \n",
       "8                             0.789844                             0.778069   \n",
       "9                             0.789909                             0.778130   \n",
       "\n",
       "      alpha  \n",
       "0  0.555802  \n",
       "1  0.699504  \n",
       "2  0.740615  \n",
       "3  0.850937  \n",
       "4  0.980957  \n",
       "5  0.235981  \n",
       "6  0.398196  \n",
       "7  0.444187  \n",
       "8  0.536236  \n",
       "9  0.725219  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster = ForecasterAutoregMultiSeries(\n",
    "                    regressor = Ridge(random_state=123),\n",
    "                    lags      = 2,\n",
    "                    encoding  = 'onehot',\n",
    "                    transformer_series = StandardScaler()\n",
    "                )\n",
    "\n",
    "steps = 3\n",
    "n_validation = 12\n",
    "\n",
    "def search_space(trial):\n",
    "    search_space  = {\n",
    "        'alpha': trial.suggest_float('alpha', 1e-2, 1.0),\n",
    "        'lags' : trial.suggest_categorical('lags', [2, 4])\n",
    "    }\n",
    "    \n",
    "    return search_space\n",
    "\n",
    "results = _bayesian_search_optuna_multiseries(\n",
    "                forecaster         = forecaster,\n",
    "                series             = series,\n",
    "                steps              = steps,\n",
    "                search_space       = search_space,\n",
    "                metric             = ['mean_absolute_error', 'mean_absolute_scaled_error'],\n",
    "                aggregate_metric   = ['weighted_average', 'average', 'pooling'],\n",
    "                refit              = True,\n",
    "                initial_train_size = len(series) - n_validation,\n",
    "                fixed_train_size   = True,\n",
    "                n_trials           = 10,\n",
    "                random_state       = 123,\n",
    "                return_best        = False,\n",
    "                verbose            = False,\n",
    "                show_progress      = False\n",
    "            )[0]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"levels\": {\n",
    "        0: [\"l1\", \"l2\"],\n",
    "        1: [\"l1\", \"l2\"],\n",
    "        2: [\"l1\", \"l2\"],\n",
    "        3: [\"l1\", \"l2\"],\n",
    "        4: [\"l1\", \"l2\"],\n",
    "        5: [\"l1\", \"l2\"],\n",
    "        6: [\"l1\", \"l2\"],\n",
    "        7: [\"l1\", \"l2\"],\n",
    "        8: [\"l1\", \"l2\"],\n",
    "        9: [\"l1\", \"l2\"],\n",
    "    },\n",
    "    \"lags\": {\n",
    "        0: np.array([1, 2]),\n",
    "        1: np.array([1, 2]),\n",
    "        2: np.array([1, 2]),\n",
    "        3: np.array([1, 2]),\n",
    "        4: np.array([1, 2]),\n",
    "        5: np.array([1, 2, 3, 4]),\n",
    "        6: np.array([1, 2, 3, 4]),\n",
    "        7: np.array([1, 2, 3, 4]),\n",
    "        8: np.array([1, 2, 3, 4]),\n",
    "        9: np.array([1, 2, 3, 4]),\n",
    "    },\n",
    "    \"params\": {\n",
    "        0: {\"alpha\": 0.5558016213920624},\n",
    "        1: {\"alpha\": 0.6995044937418831},\n",
    "        2: {\"alpha\": 0.7406154516747153},\n",
    "        3: {\"alpha\": 0.8509374761370117},\n",
    "        4: {\"alpha\": 0.9809565564007693},\n",
    "        5: {\"alpha\": 0.23598059857016607},\n",
    "        6: {\"alpha\": 0.398196343012209},\n",
    "        7: {\"alpha\": 0.4441865222328282},\n",
    "        8: {\"alpha\": 0.53623586010342},\n",
    "        9: {\"alpha\": 0.7252189487445193},\n",
    "    },\n",
    "    \"mean_absolute_error__weighted_average\": {\n",
    "        0: 0.21324663796176382,\n",
    "        1: 0.2132571094660072,\n",
    "        2: 0.21326009091608622,\n",
    "        3: 0.21326806055662118,\n",
    "        4: 0.2132773952926551,\n",
    "        5: 0.21476196207156512,\n",
    "        6: 0.21477679099211167,\n",
    "        7: 0.21478095843883202,\n",
    "        8: 0.2147892513261171,\n",
    "        9: 0.21480607764821474,\n",
    "    },\n",
    "    \"mean_absolute_error__average\": {\n",
    "        0: 0.21324663796176382,\n",
    "        1: 0.21325710946600718,\n",
    "        2: 0.21326009091608622,\n",
    "        3: 0.21326806055662115,\n",
    "        4: 0.2132773952926551,\n",
    "        5: 0.21476196207156514,\n",
    "        6: 0.21477679099211167,\n",
    "        7: 0.21478095843883202,\n",
    "        8: 0.21478925132611706,\n",
    "        9: 0.21480607764821472,\n",
    "    },\n",
    "    \"mean_absolute_error__pooling\": {\n",
    "        0: 0.21324663796176382,\n",
    "        1: 0.21325710946600726,\n",
    "        2: 0.21326009091608622,\n",
    "        3: 0.21326806055662115,\n",
    "        4: 0.21327739529265513,\n",
    "        5: 0.21476196207156514,\n",
    "        6: 0.21477679099211167,\n",
    "        7: 0.21478095843883202,\n",
    "        8: 0.21478925132611706,\n",
    "        9: 0.21480607764821472,\n",
    "    },\n",
    "    \"mean_absolute_scaled_error__weighted_average\": {\n",
    "        0: 0.7850408674109868,\n",
    "        1: 0.7850769204162228,\n",
    "        2: 0.7850871846200614,\n",
    "        3: 0.7851146198219785,\n",
    "        4: 0.7851467509994321,\n",
    "        5: 0.7897394420953061,\n",
    "        6: 0.7897964331491425,\n",
    "        7: 0.7898124493861658,\n",
    "        8: 0.7898443200957621,\n",
    "        9: 0.7899089846155858,\n",
    "    },\n",
    "    \"mean_absolute_scaled_error__average\": {\n",
    "        0: 0.7850408674109867,\n",
    "        1: 0.7850769204162228,\n",
    "        2: 0.7850871846200613,\n",
    "        3: 0.7851146198219786,\n",
    "        4: 0.785146750999432,\n",
    "        5: 0.7897394420953061,\n",
    "        6: 0.7897964331491426,\n",
    "        7: 0.7898124493861657,\n",
    "        8: 0.7898443200957622,\n",
    "        9: 0.7899089846155857,\n",
    "    },\n",
    "    \"mean_absolute_scaled_error__pooling\": {\n",
    "        0: 0.7724806513327171,\n",
    "        1: 0.7725185840968428,\n",
    "        2: 0.7725293843257277,\n",
    "        3: 0.7725582541506879,\n",
    "        4: 0.7725920690001996,\n",
    "        5: 0.7779698752966109,\n",
    "        6: 0.7780235926931046,\n",
    "        7: 0.7780386891653759,\n",
    "        8: 0.7780687299436625,\n",
    "        9: 0.7781296828776818,\n",
    "    },\n",
    "    \"alpha\": {\n",
    "        0: 0.5558016213920624,\n",
    "        1: 0.6995044937418831,\n",
    "        2: 0.7406154516747153,\n",
    "        3: 0.8509374761370117,\n",
    "        4: 0.9809565564007693,\n",
    "        5: 0.23598059857016607,\n",
    "        6: 0.398196343012209,\n",
    "        7: 0.4441865222328282,\n",
    "        8: 0.53623586010342,\n",
    "        9: 0.7252189487445193,\n",
    "    },\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skforecast_py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78d62c1713fdacd99ef7c429003c7324b36fbb551fb8b6860a7ea73e9338235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
